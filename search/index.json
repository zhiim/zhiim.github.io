[{"content":"Nearest Neighbor Classification 给定一个查询样本 $q$，找到 k 个离 $q$ 最近的样本点。对于回归任务，返回这 k 个点的均值；对于分类任务，返回数量最多的类别或者分布表\n随着超参数 k 的数值增大，kNN 一般会由过拟合变为欠拟合\nExhaustive k-NN 算法 给定一个样本点 $q$，计算所有的 n 个训练样本点到 $q$ 的距离，维护一个距离的小顶堆得到最小的 k 个距离\nk-NN 的训练复杂度为 0, 运行复杂度为 $O(nd + n\\log k)$\nVoronoi Diagrams 设 $X$ 是一个点集，对于 $w \\in X$，它的 Voronoi cell 定义为\n$$Vor(w) = \\{ p \\in \\mathbb{R}^d : \\|p - w\\| \\le \\|p - v\\|, \\forall v \\in X \\}$$$X$ 中 所有的 Voronoi cell 就是 Voronoi diagram\nVoronoi diagram 的空间复杂度为 $O(n^{[d / 2]}$（用边来衡量），当维度 d 增加是空间复杂度会稍微低于这个公式，但是仍然是指数级增长，所以不适合于高维数据。但是在实际应用中复杂度一般都是 $O(n)$\n在运行时，给定一个点 $q$，找出一个点 $w$ 使得 $q \\in Var(w)$，那么我们就可以使用 $w$ 的属性来预测 $q$\n对于二维数据，在训练时需要 $O(n\\log n)$ 的复杂度计算 Voronoi diagram，并使用一个梯形图（trapezoidal map）存储数据；运行时查找 Voronoi cell 的复杂度为 $O(\\log n)$。对于高维数据需要使用 binary space partition tree 来存储，并且复杂度难以衡量\nVoronoi 只是用了一个邻近点来估计 $q$，所以是一种 1-NN 算法，虽然可以推广到 k 阶，但是计算复杂度较大且难以实现\nk-d Trees k-d Tree 使用类似决策树的方式来进行 NN 搜索，但是使用和决策树不同的准则来选择最优划分\n选择宽度最大的特征 $w$ 用于划分，即特征 i that $\\max_{i, j,k}(X_{ji} - X_{ki})$，因为我们不希望一个样本点发散出去的邻接区跨越多个 box 区域，所以在宽度较大的特征上进行切分可以让 box 接近正方形 选择特征的中位数或者中值作为划分边界。使用中位数可以保证树的深度为 $\\log_2 n$，建树复杂度为 $O(nd \\log n)$；如果使用中值可能导致树失衡 每个中间节点都会存储一个训练数据点，所以每次搜索的时候不用下降到叶节点 在运行时，给定一个点 $q$，寻找一个训练样本点 $w$ 满足 $\\|q - w \\| \\le (1 + \\epsilon) \\|q - u\\|$，其中 $u$ 是训练样本中距离 $q$ 最近的距离，也就是我们找到的样本点 $w$ 不能远于最小距离的 $1+\\epsilon$ 倍\n在搜索时，我们需要计算 $q$ 到区域 B 的最小距离 $dist(q, B) = \\min_{z \\in B} \\|q - z\\|$\n搜索过程中需要维护一个小顶堆，用来保存在还没有搜索的 box 的距离。每次搜索的时候优先搜索离 $q$ 最近的 box。如果当前搜索的 box 离 $q$ 的距离的 $1 + \\epsilon$ 倍小于下一个最近 box 则继续，反之停止搜索\n1-NN 算法：\n将根节点的距离设置为 0，推入小顶堆 $Q$ 将当前距离 $r$ 设置为 $\\infty$ 如果 $Q$ 不为空，并且 $(1 + \\epsilon) \\cdot \\text{minkey}(Q) \u003e r$ 将 $Q$ 中节点弹出，它对应的 box 是 $B$ 取出 $B$ 中的训练样本 v，如果 $\\|q - v\\| \u003c r$，那么 $w \\leftarrow v; r \\leftarrow \\|q - v\\|$ 继续查看子节点对应的区域 $B'$ 和 $B''$ 如果 $(1 + \\epsilon) \\cdot dist(q, B') \u003c r$，将 $B'$ 和距离插入 $Q$ 如果 $(1 + \\epsilon) \\cdot dist(q, B'') \u003c r$，将 $B''$ 和距离插入 $Q$ 返回 $w$ ","date":"2025-09-26T20:03:09+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_20_knn/","title":"机器学习 20 kNN"},{"content":"AdaBoost AdaBoost 是一个集成学习方法，它可以降低偏差（Bagging 法可以降低方差）\nAdaBoost 使用不同权重的训练样本训练学习器，如果某个样本被错误分类，那么它的权重将增加；并且对每个学习器使用不同的权重，更加准确的学习器施加更大的权重\n假设我们训练 T 个分类器 $G_1, \\dots, G_T$，训练样本 $X_i$ 在训练 $G_t$ 是的权重取决于 $G_1, \\dots, G_{t-1}$ 错误分类它的次数，此外如果 $X_t$ 被一个非常精准的分类器分类错误，它的权重将会增加的更多，每次 $X_i$ 被正确分类，它的权重也会相应缩小。\n组合学习器（Metalearner）是所有学习器的线性组合，即 $M(z) = \\sum_{t = 1}^T \\beta_t G_t(z)$，虽然每个学习器 $G_t$ 的输出是二元的分类，但是组合学习器的输出是连续的值\n在第 T 步时，我们需要选择一个新的学习器 $G_T$ 和对应的权重 $\\beta_T$，为了实现最优选择，首先选定风险为\n$$Risk = \\frac{1}{n} \\sum_{i = 1}^n L(M(X_i), y_i)$$其中 AdaBoost 的损失函数为\n$$ L(\\hat{\\lambda}, \\lambda) = e^{- \\hat{\\lambda}\\lambda} = \\begin{cases} e^{-\\hat{\\lambda}} \\quad \\lambda = + 1 \\\\ e^{\\hat{\\lambda}} \\quad \\lambda = -1 \\end{cases} $$我们对风险推导可得\n$$ \\begin{align*} n \\cdot Risk \u0026= \\sum_{i = 1}^n L(M(X_i), y_i) = \\sum_{i = 1}^n e^{-y_i M(X_i)} \\\\ \u0026= \\sum_{i = 1}^n \\exp \\left( -y_i\\sum_{i = 1}^T \\beta_t G_t(X_i) \\right) = \\sum_{i = 1}^n \\prod_{t = 1}^T e^{-\\beta_t y_i G_t(X_i)} \\\\ \u0026= \\sum_{i = 1}^n w_i^{(T)} e^{-\\beta_T y_i G_T(X_i)} , \\quad where \\quad w_i^{(T)} = \\prod_{t = 1}^{T - 1} e^{-\\beta_t y_i G_t(X_i)} \\\\ \u0026= e^{-\\beta_T} \\sum_{y_i = G_T(X_i)} w_i^{(T)} + e^{\\beta T} \\sum_{y_i \\ne G_T(X_i)} w_i^{(T)} \\\\ \u0026= e^{-\\beta_T} \\sum_{i = 1}^n w_i^{(T)} + (e^{\\beta_T} - e^{-\\beta_T}) \\sum_{y_i \\ne G_T(X_i)} w_i^{(T)} \\end{align*} $$其中 $w_i^{(T)} = \\prod_{t = 1}^{T - 1} e^{-\\beta_t y_i G_t(X_i)}$ 只与 $G_1, \\dots, G_{T -1}$ 有关，可以看作风险中每个样本的权重，并且该样本在前面几轮中被分错的次数越多，$w_i^{(T)}$ 越大\n同时，我们也可以得到 $w_i^{(T)}$ 的递推公式\n$$ w_i^{(T + 1)} = w_i^{(T)} e^{-\\beta_T y_i G_T(X_i)} = \\begin{cases} w_i^{(T)} e^{-\\beta T} \\quad y_i = G_T(X_i) \\\\ w_i^{(T)} e^{\\beta_T} \\quad y_i \\ne G_T(X_i) \\end{cases} $$在风险的最终形态中，第一项 $e^{-\\beta T} \\sum_{i = 1}^n w_i^{(T)}$ 只与上面的几个分类器有关，所以在第 T 轮想要风险最小，就要最小化第二项中的 $\\sum_{y_i \\ne G_T(X_i)} w_i^{(T)}$，也就是所有被分类错误的样本的权重 $w_i^{(T)}$。所以在每一轮中选择 $G_T$ 的准则就是让分类错误的样本的权重之和最小。虽然决策树一般可以将所有的样本点都分类正确，但是 boosting 中一般只会使用较短、非理想的决策树\n在选择 $\\beta_T$ 时，令 $\\frac{d}{d \\beta_T}Risk = 0$, 即 $$0 = -e^{\\beta_T} \\sum_{i = 1}^n w_ii^{(T)} + (e^{\\beta_T} + e^{-\\beta_T}) \\sum_{y_i \\ne G_T(X_i)} w_i^{(T)}$$ 令 $$err_T = \\frac{\\sum_{y_i \\ne G_T(X_i)}w_i^{(T)}}{\\sum_{i=1}^n w_i^{(T)}}$$ 将 $err_T$ 称为 weighted error rate，可以算出 $$\\beta_T = \\frac{1}{2} \\ln \\left( \\frac{1 - err_T}{err_T} \\right)$$从上式可以看出，如果 $err_T = 0$ 那么 $\\beta_T = \\infty$，如果 $err_T = 1 / 2$ 那么 $\\beta_T = 0$，也就是说如果一个学习器的 error 为 50%, 则它将对组合学习器无贡献\n更具上面的讨论我们就可以得到 AdaBoost 算法：\n初始每个训练样本的权重 $w_i \\leftarrow \\frac{1}{n},\\forall{i} \\in [1, n]$ for 循环 1 至 T 计算加权错误率 $err = \\frac{\\sum_{mis} w_i}{\\sum_{all} w_i}$，以及系数 $\\beta_t = \\frac{1}{2} \\ln \\left( \\frac{1 - err}{err} \\right)$ 更新权重 $w_i \\leftarrow w_i \\cdot \\begin{cases} e^{\\beta_T}, \\quad G_t(X_i) \\ne y_i \\\\ e^{-\\beta_t}, \\quad other \\end{cases} = w_i \\cdot \\begin{cases} \\sqrt{\\frac{1 - err}{err}} \\\\ \\sqrt{\\frac{err}{1 - err}} \\end{cases}$ 返回组合学习器 $h(z) = sign\\left( \\sum_{t = 1}^T \\beta_t G_t(z) \\right)$ 如上图所示，初始时我们为每个训练样本施加相同的权重，然后根据分类器分类正确与错误的情况减小或者增加权重\n为何 boost 往往搭配决策树，并且使用较短的树结构\nBoosting 可以减小偏差，但是不一定会减小方差，使用较短的树结构主要是为了防止过拟合 树的训练一般较快，而在 boosting 法下我们需要训练多个学习器 决策树不需要超参数 AdaBoost 加上短树相当于是一种特征子集选择，无法提高 metalearning 估计能力的特征完全没有使用（err 为 50% 的） 线性决策边界和 boost 的组合并不好 ","date":"2025-09-26T19:59:30+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_19_adaboost/","title":"机器学习 19 AdaBoost"},{"content":"无监督学习 有时候我们的训练数据里面没有标签，但是我们还是希望寻找数据的结构信息\n无监督学习的应用主要分为如下几个大类：\n聚类（clustering）：将数据划分成几个相似的点集 数据降维（dimensionality reduction）：数据一般分布在特征空间的低纬子空间，或者矩阵一般有低阶秩近似 分布学习（density estimation）：使用离散数据拟合一个连续概率分布 主成分分析 Principal Components Analysis 目标：给定一些 $\\mathbb{R}^d$ 中的样本点，寻找 k 个方向，可以捕获大部分特征变化\n主成分分析一般有以下几个目的：\n降低维度可以降低计算复杂度 移除了干扰维度可以防止过拟合，和特征子集选择类似，但是此时选出来的特征是其他特征的线性组合 寻找一个较小的可以表征复杂变化的基 假设 $X$ 是一个 $n \\times d$ 的设计矩阵，并且 $X$ 的均值为 0。假设有一些正交基 ${} v_1, \\dots, v_k {}$，那么 x 可以在正交基下表示为 $\\widetilde{x} = \\sum_{i = 1}^k (x \\cdot v_i) v_i$，其中 $x \\cdot v_i$ 表示主坐标，是 x 在 $v_i$ 方向投影的长度\n在降维的时候我们希望最大程度保留原始信息，在主成分分析中信息被定义为方差，所以最佳的方向就是协方差矩阵 $X^TX$ 的特征向量。\n假设 $X^TX$ 的特征特征值为 $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_d$，对应到特征向量 $v_1, v_2, \\dots, v_k$ 就是主成分。计算每个数据点在主成分下的主坐标，就是实现了对数据的降维\n","date":"2025-09-26T19:57:00+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_18_principal_components_analysis/","title":"机器学习 18 主成分分析"},{"content":"梯度消失与爆炸 当使用 sigmoid 函数时，如果 sigmoid 的输出接近 0 或 1，那么梯度 $s' = s(1 - s) \\approx 0$，此时梯度更新将会特别缓慢，这个问题被称为梯度消失，在深层网络中，反向传播会使得梯度消失加剧\n将激活函数换为 ReLU 将会缓解梯度消失，ReLU 的定义为 $r(\\gamma) = max\\{0, \\gamma\\}$，它的导数为\n$$ r'(\\gamma) = \\begin{cases} 1, \\quad \\gamma \\ge 0, \\\\ 0, \\quad \\gamma \u003c 0 \\end{cases} $$ 在现代神经网络中，ReLU 一般被用于中间层的激活函数，因为 ReLU 的梯度很少全部为 0，更多的情况是部分神经元的梯度为 0\n然而 ReLU 的输出可以无限大，带了梯度爆炸问题，特别是在深层网络中，大输入造成了梯度随着网络深度不断变大\n输出层 ReLU 一般被使用于隐藏层，而在输出层一般为线性单元（回归问题）或者 sigmoid/softmax 单元（分类问题），分别对应了线性回归、logistic 回归和 softmax 回归\n","date":"2025-09-26T19:51:29+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_17_gradient_vanish/","title":"机器学习 17 神经网络 II：梯度消失和爆炸"},{"content":"神经网络 单个的、基础的感知机存在严重局限性，无法解决像 XOR（异或）这样看似简单但非线性的问题。如下图，异或的二维输入和输出表示了三维空间的点，无法被线性分类器分类\n如果添加一个二次特征 $x_1x_2$，那么 XOR 将在 3 维空间线性可分\n当然，我们也有更佳的方法，通过多个单层线性分类器的叠加可以实现类似电路的逻辑连接，此外在每个线性分类器的输出添加一个非线性的函数 $s$ 可以实现类似电路逻辑门。所以通过多个线性连接+非线性函数（一般为 logistic 函数）可以轻易实现异或，其中每个线性线性连接+非线性函数组成了一个神经元\n一层隐藏层的网络 多个神经元组成的三层网络如同\n集合函数 $s(\\gamma) = \\frac{1}{1 + e^{-\\gamma}}$，被称为激活函数。对于一个向量 $u$，定义 $s(u) =\\begin{bmatrix}s(u_1) \\\\ s(u_2) \\\\ \\vdots\\end{bmatrix}$，$s_1(u) = \\begin{bmatrix} s(u_1) \\\\ s(u_2) \\\\ \\vdots \\\\ 1\\end{bmatrix}$ 表示包含偏置的形式\n那么有\n$$ \\begin{align*} h \u0026= s_1(Vx) \\\\ \\hat{y} \u0026= s(Wh) = s(Ws_1(Vx)) \\end{align*} $$神经网络可以有多个输出节点，所以我们可以基于相同的隐状态训练多个分类器，有时候不同的分类器可以使用不同隐藏层神经元，并且多个分类器的结果可以互相补充\n训练 神经网络的训练一般使用随机梯度下降或者批梯度下降\n选择损失函数 $L(\\hat{y}, y)$，例如 $L(\\hat{y},y) = \\|\\hat{y} - y\\|^2$, 寻找 $V$ 和 $W$ 可以最小化成本函数 $J(V,W) = \\frac{1}{n} \\sum_{i = 1}^n L(\\hat{y}(X_i), Y_i)$\n神经网络的成本函数一般不是凸函数，存在非常多的局部最小值（local minima），一般可以通过增加更多的隐藏节点缓解这个问题\n神经网络的节点具有对称性，不同神经元之间结构一致，如果所有的神经元都初始化为相同点权重，那么所有的输出和梯度都一致，所有的神经元在参数更新中都会保持一致，所以一般使用随机权重初始化神经元\n神经元的权重不能初始化为过大或者过小，否则会处于 logistic 函数的饱和区，梯度过小\n梯度计算 我们可以使用链式法则求每个节点的梯度\n对于公式 $f=((a + b) c)^2$，可以将其拆解成链式传播的形式，然后从 $f$ 开始逐次求解每个节点的梯度\n对于多元数据，同样可以使用多元微分 $$\\frac{\\partial}{\\partial \\alpha} L(y_1(\\alpha), y_2(\\alpha)) = \\frac{\\partial L}{\\partial y_1} \\frac{\\partial y_1}{\\partial \\alpha} + \\frac{\\partial L}{\\partial y_2} \\frac{\\partial y_2}{\\partial \\alpha} = \\nabla_y L\\cdot \\frac{\\partial}{\\partial \\alpha} y$$ 同样我们也可以通过链式法则求成本函数，梯度计算是从后往前进行的，被称为反向传播算法\n","date":"2025-09-26T19:41:36+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_16_neural_network/","title":"机器学习 16 神经网络 I：神经网络与梯度计算"},{"content":"决策树的变种 决策树回归 决策树回归就是通过一系列是否问题，创建了一个分段常数函数（Piecewise Constant Regression Fn），在函数的每一段，决策树的预测是一个固定的常数\n上图左边是一个对特征维度为 2 的样本进行决策回归的过程，数据顺着路径到达某一个叶子节点得到最终的预测值 中间的图表示决策树将整个特征空间划分成多个区块，每个区块对应一个叶子节点 右图表示对应的分段常数函数，每个特征空间中的区块都对应了一个常数的叶子节点预测值 每一个叶子节点的预测值，由落入这个叶子节点的所有样本的值的平均决定，即\n$$\\mu_S = \\frac{1}{|S|}\\sum_{i \\in S}y_i$$其中 $S$ 表示落入某个节点的样本索引的集合\n决策树的目标是使分裂后的子区域纯度尽可能高。对于分类数，目标是让子区域的类别尽可能一致；对于回归树，我们希望让子区域的样本真实值 $y$ 尽可能接近\n所以我们可以使用子区域内样本的方差作为 cost\n$$J(S) = Var(\\{ y_i: i\\in S \\}) = \\frac{1}{|S|} \\sum_{i \\in S} (y_i - \\mu_S)^2$$所以每次选择可以使得划分出的两个子集方差的加权平均（根据样本点树加权）最小的划分点\n早停 在构造决策树的时候，没必要让每个叶节点到达纯净，可以实现加速以及防止过拟合\n对于上图两个重合非常大的概率分布，得到一个叶子节点的纯净决策树一定会造成过拟合\n此外叶子节点不那么纯净，往往能包含更加丰富的信息。例如，对于上图的决策树划分，我们可以通过统计每个子区域内不同类别的点数，得到某种类别被分到该叶子节点的概率。对于分类树，直接去概率最大的类别最为叶子节点输出的类别；对于回归树，使用平均值作为叶子节点的预测值\n决策树可以使用的早停策略包括：\n继续划分不再降低熵或者误差（有可能一直划分到纯净节点） 大多数（例如大于 90%）节点类别相同 节点包含的样本点过少（例如小于 10） 子区域过小 树的深度太大 使用验证集检验 剪枝 如果树太大，可以通过移除一些划分来提高在验证集的表现\n剪枝往往比早停更加有效，因为有时候某次划分带来的提升不明显，但是接下来可能出现提升明显的划分\n在剪枝时，如果每次去除一个分割后重新从头计算验证集指标会造成非常大的开销。相反，我们应该在构建深度树的时候记录下每个节点中包含了哪些样本，这样剪枝的时候只需将子节点的样本合并到父节点\n多变量划分 一个标准的决策树一次只对一个特征进行划分（如左图），对于 SVM 可以轻易区分的斜线，决策树需要学习一个非常复杂的规则\n如果想要使用多个特征进行划分，可以在每个节点使用例如 SVM、逻辑回归和高斯判别分析，这些分类器通过决策树的层级结构可以学习到非线性的决策边界\n集成学习 Ensemble Learning 决策树快、简单并且可以解释性强，但是方差较大。例如，如果将两个数据集分成两份，分别训练两个决策树，这两个决策树可能差异非常大，特别是如果根节点选择的划分特征不同的时候\n我们可以通过集合多个弱估计器，来得到一个强估计器，如果一个弱估计器的方差较大，那么将多个弱估计器组合去平均将会大大降低方差\n为了得到一个强估计器，可以使用不同的方法取平均：\n不同的学习算法 使用不同数据集训练的同一算法 bagging 法：使用一个数据集的不同随机子采样训练同一算法 随机森林：使用一个数据集的不同随机子采样训练随机决策树 平均可以很好的降低方差，但是无法降低偏差，所以我们需要在平均之前得到一些小偏差的估计器（即使存在一定程度的过拟合也没问题）\nBagging Bagging 是一种使用一个数据集训练多个不同学习器的方法，它对大多数算法都有效（kNN 除外）\n给定一个 $n$ 点的训练样本集，从里面又放回地随机选取 $n'$ 个样本，这意味着某些样本被多次采样。如果 $n' = n$，那么大概有 63.2%的样本被采样\nBagging 法存在一个问题：某些样本被采样了 $j$ 次，所以他们在训练数据中占有更大的比重\n随机森林 仅仅使用 Bagging 法加上决策树仍然不够，因为可能不同的树模型非常相似。例如，某个特征非常强，那么不同的树都会从它开始划分\n为了得到不同的决策树，对于每个节点，随机选取 $m$ 个特征，从这 $m$ 个特征中一个可以最佳划分的特征。其中 $m$ 是超参数，$m$ 越小越随机，不同树之间越不相关，偏差更大\n","date":"2025-09-26T19:32:13+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/","title":"机器学习 15 决策树 II：决策树扩展"},{"content":"决策树是一种用于分类和回归的非线性方法\n决策树有两种节点状态：\n内部节点：对特征值进行测试（通常是一个），并决定走向哪个分支 叶节点：确定最终的类别 决策树对类别特征和数值特征都较为有效，并且可解释性强，上图是决策树的工作模式。如右图所示，决策树将 x-空间划分成了长方形区块，不过不同程度的分支组合，决策树的决策边界可以任意复杂\n设 $X$ 是一个 $n\\times d$ 设计矩阵，$y\\in \\mathbb{R}^n$ 是标签，$S \\in \\{ 1, 2, \\dots, n\\}$ 是采样点的索引\n决策树的构造\n$$ \\begin{aligned} \u0026\\text{Build Decision Tree} \\\\ \u0026\\text{Procedure } GrowTree(S) \\\\ \u0026\\quad \\text{if }(y_i = C \\text{ for all } i \\in S) \\\\ \u0026\\quad \\quad \\text{return new } Leaf(C) \\\\ \u0026\\quad \\text{else} \\\\ \u0026\\quad \\quad \\text{choose best splitting feature } j \\text{ and splitting value } \\beta \\\\ \u0026\\quad \\quad S_l = \\{i \\in S: X_{ij} \u003c \\beta\\} \\\\ \u0026\\quad \\quad S_r = \\{i \\in S: X_{ij} \\ge \\beta \\} \\\\ \u0026\\quad \\quad \\text{return new } node(j, \\beta, GrowTree(S_l), GrowTree(S_r) \\\\ \u0026\\quad \\text{end if} \\\\ \u0026\\text{end procedure} \\\\ \\end{aligned} $$其中的关键是如何选择最优的划分\n尝试所有可能的划分 对于一个集合 $S$，定义它的成本为 $J(S)$ （用来衡量 S 的混乱程度，目标是划分后，子集尽可能纯洁） 选择一个划分可以最小化 $J(S_l) + J(S_r)$ （或加权平均 $\\frac{|S_l|J(S_l) + |S_r|J(S_r)}{|S_l| + |S_r|}$） 现在我们只需要确定如何计算成本 $J (S)$\n方案一：假设 $S$ 中类别最多的类别是 $C$，那么定义 $J (S)$ 为 $S$ 中不属于类别 $C$ 的样本个数\n但是对于上图这种划分，最后都能得到 $J(S_l) + J(S_r) = 10$ 的结果，但是左边的更好（划分后类别更纯净）， $\\frac{|S_l|J(S_l) + |S_r|J(S_r)}{|S_l| + |S_r|}$ 甚至会优先选择右边\n方案二：衡量熵\n假设 $Y$ 是一个随机类别变量，$P (Y = C) = p_C$，那么 $Y$ 属于 $C$ 的惊讶度（degree of suprise）是 $-\\log_2p_C$（如果一个非常可能发生的事发生了，我们并不惊讶；如果不太可能发生的事发生了，我们非常惊讶，它的信息量更大）\n那么集合 $S$ 的熵为\n$$H(S) = - \\sum p_C \\log_2 p_C, \\quad p_C = \\frac{|\\{ i \\in S: y_i = C\\}|}{|S|}$$熵代表了在均匀分布下随机采样的子集 $S$ 中，为了正确识别它的类别，我们需要传输的信息量\n上图分别是两个类别和三个类别的条件下，熵随着类别概率变化的曲线，可以看出如果一个集合中类别约纯净，它的熵越小\n在每次划分时，我们可以计算划分后的平均熵\n$$H_{after} = \\frac{|S_l|H(S_l) + |S_r|H(S_r)}{|S_l| + |S_r|}$$那么我们选择划分的准则就是最大化信息增益 $H(S) - H_{after}$，也就是最小化 $H_{after}$\n信息增益将会永远大于等于 0，当某一个子集为空或者所有子集内部类别分布完全一致时，信息增益为 0\n左图是使用熵作为成本函数的划分，可以看到，熵随 $p_C$ 的变化曲线是严格凸的，父节点的熵永远大于两个自节点熵的加权平均；左图是使用 $J(S)$ 作为成本函数的曲线，在很多情况下划分前后的成本相同，无法衡量划分好坏\n对于二元特征 $x_i$，直接划分成 $x_i = 0$ 和 $x_i = 1$ 两类 如果 $x_i$ 有三个或者以上的离散值，可以二元分裂，也可以多元分裂 如果 $x_i$ 是一个数值，需要在每两个不同值间尝试分裂点\n为了加速计算，我们首先对数值进行排序（如下图），每两个不同值之间都存在一个分裂点（竖线），使用不同的分裂点只会将一个样本从右侧移动到左侧，所以只需初始一次排序，后续记录不同类别的数量变化，这样从左到右改变分裂点时只需要 $O (1)$ 的时间重新计算熵 决策树的计算复杂度：\n在测试阶段。从根节点到叶节点，最多走完树的深度，复杂度 $\\le O(\\log n)$ 在训练阶段。 对于每个节点。如果是二元特征，每个节点需要依次尝试利用 $d$ 个特征划分，复杂度为 $O (d)$；如果是数值特征，每个特征如果有 $n'$ 个分裂点，那么复杂度为 $O (n'd)$ 对于整个树。每个样本点参与 $O (depth)$ 个节点（从上至下），每个样本点在每个节点的计算复杂度为 $O(d)$（$n'd$ 平均到 $n'$ 个点），总共有 $n$ 个样本点，所以训练复杂度 $\\le O(nd \\cdot depth)$ ","date":"2025-09-26T17:42:31+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_14_decision_tree/","title":"机器学习 14 决策树 I：决策树基础"},{"content":"岭回归 Ridge Regression 岭回归即是最小二乘线性规划加上 $l_2$ 正则化，可以描述为：\n寻找一个 $w$，最小化 $J(w) = \\|Xw - y\\|^2 + \\lambda \\|w'\\|^2$\n其中，$w'$ 是使用 0 替代了 $w$ 中对应常数项权重的 $\\alpha$ （不用对常数项进行惩罚）\n加入 $l_2$ 范数惩罚 $\\lambda \\|w'\\|^2$ 会鼓励参数学习向着 $\\|w'\\|$ 较小的方向进行，原因如下：\n保证了代价函数是一个正定二次型，从而有唯一最优解1 通过降低方差防止过拟合\n如果权重过大，细微的数据动荡将会造成较大的变化 上图中红色的表示 $\\|Xw - y\\|^2$ 的等值线，蓝色的表示 $\\|w\\|$ 的等值线，红色等值线第一次和蓝色等值线相交的点就是岭回归的解\n可以通过使代价函数的梯度为 0 得到最优 $w$，即求解线性方程组\n$$(X^TX + \\lambda I')w = X^T y$$其中 $I'$ 是一个特殊的单位矩阵，它的右下角元素被设为 0（不对常数项偏置惩罚）\n岭回归的方差为 $Var(z^T(X^TX + \\lambda I')^{-1} X^T e)$，可以看出 $\\lambda$ 增大时，方差减小，但是偏差会增大\n岭回归的贝叶斯解释 假设$w'$的先验概率满足 $w' \\sim \\mathcal{N}(0, \\varsigma^2)$，则概率密度函数为 $f(w') \\propto e^{- \\|w'\\|^2 / (2 \\varsigma^2)}$\n可以计算出后验概率为\n$$f_{W|X,Y}(w) = \\frac{f_{Y| X, W}(y)f(w')}{f_{Y|X}(y)}$$后验概率的对数形式2为\n$$ \\begin{align*} \\ln f_{W|X,Y}(w) \u0026= \\ln f_{Y| X, W}(y) + \\ln f(w') - \\text{const} \\\\ \u0026 = -\\text{const} \\|Xw - y\\|^2 - \\text{const} \\|w'\\|^2 -\\text{const} \\end{align*} $$最大化后验概率，即最小化 $\\|Xw - y\\|^2 + \\|w'\\|^2$\n最大似然估计：寻找参数 $w$，使得在该参数下，当前观测样本出现的概率最大\n最大后验概率估计：寻找参数 $w$，使得在观察到当前样本的前提下，参数的后验概率最大\nRef: https://zhuanlan.zhihu.com/p/506449599\n特征子集选择 添加特征会增加方差，但是不一定会降低偏差，通过去除难以预测的特征，可以降低偏差\n最直接的方法是，我们可是使用 $d$ 个特征的 $2^d - 1$ 个特征子集，训练不同的模型，然后通过交叉检验选择最优的模型。但是计算开销过大\n前向逐步选择：从 0 特征开始，每次加入一个最优的特征，直到验证集的误差开始升高。最优特征的选择，可以通过依次遍历不同的特征，并通过交叉检验获得。只需要训练 $O (d^2)$ 个模型，但是可能会错过组合在一起最优的多个特征\n后向逐步选择：从 d 个特征开始，逐渐移除特征，每次选择可以使验证集性能下降最小的特征\nLASSO 最小二乘线性回归 + $l_1$ 惩罚平均损失\nLASSO 和岭回归一样，都是在最小二乘回归的基础上正则项，但是 LASSO 的 $l_1$ 会使某些维度的权重接近 0\n寻找 $w$ 可以最小化 $\\|Xw-y\\|^2 + \\lambda \\|w'\\|_1$，其中 $\\|w'\\|_1 = \\sum_{i = 1}^d|w_i|$（不对常数项 $\\alpha$ 惩罚）\n岭回归的正则项 $\\|w'\\|^2$ 是一个超球面，而 $\\|w'\\|_1$ 对应的是一个正多面体\n上图中，红色的椭圆形表示 $\\|Xw - y\\|^2$，其中最小二乘法的解为中心的红点。左边蓝色的四边形表示 $l_1$ 正则项，如果 $\\lambda$ 较大，对应的四边形约小，两个图形的第一次相交于竖轴，此时特征 $w_1$ 对应的权重为 0，这种方法可以用于去除无用特征\n$\\|Xw - y\\|^2$ 中的二次型是 $w^T(X^TX)w$，$\\lambda \\|w'\\|^2$ 中的二次型是 $\\lambda w^T I w$，相当于加入一个一定为正的对角矩阵\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n由于 $y = Xw + e$，其中噪声 $e$ 符合高斯分布，所以 $y$ 符合均值为 $Xw$ 的高斯分布，即 $f_{Y| X, W}(y) \\propto \\exp(-(y - Xw)^2) / 2 \\sigma^2$，所以 ${} \\ln f_{Y| X, W}(y) =-\\text{const} \\|Xw - y\\|^2 {}$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-09-26T17:15:20+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_13_regularization/","title":"机器学习 13 回归 IV：正则化"},{"content":"统计解释 一种典型的模型假设：\n采样点来自一个未知的概率分布，即 $X_i \\sim D$ 标签 y 的值是一个未知的非随机函数 $g$ 的结果加上随机噪声，即\n$\\forall X_i, \\quad y_i = g(X_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim D'$，其中 $D'$ 的均值为 0 回归问题的目标：找到一个映射 $h$ 来近似 $g$\n理想的途径是，选择 $h(x) = E_Y[Y|X=x] = g(x) + E[\\varepsilon] = g(x)$\n从最大似然法推导最小平方代价函数 假设 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$，那么 $y_i \\sim \\mathcal{N}(g(X_i), \\sigma^2)$，那么 $y_i$ 的概率密度函数的对数形式为\n$$\\ln f(y_i) = -\\frac{(y_i - g(X_i))^2}{2\\sigma^2} - C$$其中 $C$ 表示一个常量\n于是对数似然为\n$$l(g;X,y) = \\ln (f(y_1)f(y_2) \\cdots f(y_n)) = -\\frac{1}{2\\sigma^2}\\sum(y_i - g(X_i))^2 - C'$$从上式可以看出，最大化似然 $l (g; X, y)$，等价于选择一个函数 $g$，可以最小化 $\\sum(y_i - g(X_i))^2$\n经验风险 假设 $h$ 的风险是在联合概率分布 $(X, Y)$ 上的损失的期望，即 $R(h) = E[L]$\n如果我们知道 $X$ 的分布，我们可以通过生成模型估计 $X$ 和 $Y$ 的联合概率分布。但是实际分布通常未知，只能假设所有的样本点都满足实际概率分布，然后最小化经验风险\n$$\\hat{R}(h) = \\frac{1}{n} \\sum_{i = 1}^n L(h(X_i), y_i)$$理论上，当样本数量 $n\\rightarrow \\infty$ 时，经验风险将会收敛到实际风险。通过最小化 $\\hat{R}(h)$ 来选择 $h$ 的过程被称为经验风险最小化\n从最大似然推导 logistic 损失 设样本点 $X_i$ 属于类别 C 的实际概率为 $y_i$，预测概率为 $h (X_i)$\n假设总共有 $\\beta$ 个和 $X_i$ 相同的数据点，那么将有 $y_i \\beta$ 个属于类别 $C$，$(1-y_i)\\beta$ 个不属于 $C$\n那么似然为\n$$\\prod_{i = 1} ^n h(X_i)^{y_i \\beta}(1 - h(X_i))^{(1 - y_i)\\beta}$$对数似然为\n$$ \\begin{align*} l(h) \u0026 = \\ln \\mathcal{L}(h) \\\\ \u0026 = \\beta \\sum_i \\underbrace{\\left( y_i \\ln h(X_i) + (1 - y_i) \\ln (1 - h(X_i)) \\right)}_{\\text{logistic loss}} \\end{align*} $$最大化似然，等价于最小化 logistic 损失之和\n偏差-方差分解 偏差：由于 $h$ 无法完美拟合 $g$ 导致的误差\n方差：由于拟合数据中的随机噪声造成的误差\n由于训练数据 $X$ 和 $y$ 来自一个联合概率分布，我们用这些数据来选择一个权重，所以最终选择的拟合函数 $h$ 也来自一个概率分布\n对风险进行分解（$\\gamma$ 和 $h(z)$ 互相独立）\n$$ \\begin{align*} R(h) \u0026= E[L(h(z), \\gamma)] \\\\ \u0026 = E[(h(z) - \\gamma)^2] \\\\ \u0026 = E[h^2(z)] + E[\\gamma^2] - 2 E[\\gamma h(z)] \\\\ \u0026 = (Var(h(z)) + E[h(z)]^2) + (Var(\\gamma) + E[\\gamma]^2) - 2 E[\\gamma]E[h(z)] \\\\ \u0026 = \\underbrace{(E[h(z)] - E[\\gamma])^2}_{偏差的平方} + \\underbrace{Var(h(z))}_{方差} + \\underbrace{Var(\\varepsilon)}_{噪声} \\end{align*} $$上式被称为风险函数的偏差-方差分解\n偏差-方差分解 上图中，左侧的 sine 曲线时实际函数 $g$，黑色直线是 50 条使用最小二乘线性回归拟合的结果 $h$，模型的风险可以分成右侧偏差、方差和噪声三个部分\n基于偏差-方差分解，我们可以得到如下结论：\n欠拟合意味着偏差过大 大多数情况下，过拟合是由于方差过大 训练误差一般反映偏差但是体现不出方差，但是测试误差可以体现两者 对于大多数分布来说，当训练数量量 $n\\rightarrow \\infty$ 时，方差趋近于 0 如果 $h$ 可以很好地拟合 $g$，那么对于大多数分布而言，当训练数量量 $n\\rightarrow \\infty$ 时，偏差趋近于 0 如果 $h$ 不能很好地拟合 $g$，那么对于大多数样本点，偏差较大 添加一个好的特征可以降低偏差，但是添加一个坏的特征很少会增加偏差 添加特征通常会增加偏差 噪声误差无法去除 在测试集中，噪声只影响 $Var (\\varepsilon)$；在训练集中，噪声同时影响偏差和方差 左图中黑色线条为真实的函数，所有点在真实的分布上添加噪声采样而得，不同颜色的曲线分别是使用不同阶数拟合数据得到的曲线。中间灰色的曲线表示训练误差随模型阶数变化的曲线，红色的是测试误差随训练阶数变化的曲线。右边红色曲线是测试集上的平方误差（包含了偏差和方差）随结束变化的曲线，绿色的表示偏差，红色的表示方差\n","date":"2025-09-26T17:05:20+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_12_statistical_justifications_for_regression/","title":"机器学习 12 回归 III：回归的统计解释"},{"content":"最小二乘多项式回归 我们可以使用一个非线性的多项式特征 $\\Phi(X_i)$ 来替代 $X_i$，例如 $$\\Phi(X_i) = \\begin{bmatrix}X_{i1}^2 \u0026 X_{i1}X_{i2} \u0026 X_{i2}^2 \u0026 X_{i1} \u0026 X_{i2} \u0026 1\\end{bmatrix}$$一旦我们创建了多项式特征向量，接下来的流程将和线性回归或 logistic 回归一致\n不同次数的拟合效果 如果多项式的次数过大，非常容易过拟合\n加权最小二乘回归 如果某些样本点比其他的样本更加可信，或者有些样本点我们希望更好地拟合，那么我们可以对这些点施加更高的权重，而对异常点施加更低的权重\n为每个样本点设定一个权重 $\\omega_i$，$n$ 个权重组成一个对角矩阵 $\\Omega$。于是加权的条件下问题描述变成：\n寻找一个 $w$ 使得可以最小化 $(Xw - y)^T \\Omega (Xw - y) = \\sum_{i = 1}^n \\omega_i (X_i \\cdot w - y_i)^2$\n同样，我们可以通过计算梯度为 0 的点求解 $w$，即 $X^T\\Omega X w = X^T \\Omega y$\n牛顿法 牛顿法是一种迭代寻找函数 $J(w)$ 的最优解的方法（$J(w)$ 必须是光滑的），通常比梯度下降更快\n牛顿法的迭代过程 牛顿法的核心是通过二次函数（抛物线）来局部逼近最优解，每次都移动到抛物线的顶点（上图中棕色曲线是用于近似的二次曲线，从左到右棕色曲线的顶点逐渐逼近最优点）：\n给定一个点 $v$，使用二次函数在 $v$ 附近近似 $J(w)$ 跳转到二次函数的顶点 重复上述步骤直到到达停止条件 在每一次迭代中寻找顶点的方式如下：\n以 $v$ 为基准，在 $w$ 处梯度的泰勒展开为\n$$\\nabla J(w) = \\nabla J(v) + (\\nabla^2 J(v)) (w - v) + O(\\| w - v \\|^2)$$其中 $\\nabla^2 J(v)$ 是Hessian 矩阵，通过计算梯度为 0 的点得到顶点\n$$w \\approx v - (\\nabla^2 J(v))^{-1} \\nabla J(v)$$上式中存在计算矩阵的逆的过程，直接计算矩阵的逆通常计算量非常大并且不稳定，一般将其转换为一个线性方程组求解，即 $(\\nabla^2 J(w))e = - \\nabla j(w)$\n牛顿法的算法流程可以表述如下：\n$$ \\begin{align*} \u0026 \\text{pick starting point } w \\\\ \u0026 \\text{repeat until convergence} \\\\ \u0026 \\quad e \\leftarrow \\text{ solution to liner system } (\\nabla^2 J(w))e = - \\nabla j(w) \\\\ \u0026 \\quad w \\leftarrow w + e \\end{align*} $$牛顿法无法区分最小值点、最大值点和鞍点，所以初始点需要足够接近目标点\n如果目标函数是一个二次函数，牛顿法只需要一步就可以找到精确解，$J$ 越接近二次函数，牛顿法越快收敛\n对于某些优化问题，牛顿法相比梯度下降可以更快收敛。首先，牛顿法会寻找到达最小值的正确步长，而不是任意距离；其次，牛顿法会尝试寻找最优的下降方向，而不是梯度最陡的方向\n然而，牛顿法也有一些缺点。牛顿法需要计算 Hessian 矩阵，在维度较高的时候计算开销非常大，所以神经网络一般不使用牛顿法。此外，牛顿法要求目标函数必须光滑，例如感知机的风险函数就无法使用牛顿法\n使用牛顿法优化 Logistic 回归 10 回归I：最小二乘回归和逻辑回归中已经计算了 $\\nabla J(w) = - X^T (y - s)$ ，可以进一步计算牛顿法需要的二阶梯度（Hessian 矩阵） $$\\nabla_w^2 J(w) = \\sum_{i = 1} ^ n s_i (1 - s_i) X_i X_i^T = X^T \\Omega X$$ 其中\n$$ \\Omega = \\begin{bmatrix} s_1(1 - s_1) \u0026 0 \u0026 \\dots \u0026 0 \\\\ 0 \u0026 s_2(1 - s_2) \u0026 \u0026 0 \\\\ \\vdots \u0026 \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\dots \u0026 s_n(1 - s_n) \\end{bmatrix} $$$s_i \\in (0,1)$ （logistic 函数的输出），所以 $\\Omega$ 对于任意 $w$ 是正定的\n$\\Rightarrow$ Hessian 矩阵 $\\nabla_w^2 J(w) =X^T \\Omega X$ 是半正定的\n$\\Rightarrow$ 函数 $J$ 是凸函数 （二阶导数大于等于零，Hessian 矩阵半正定）\n由于 logistic 函数的代价函数 $J (w)$ 是凸的，所以如果可以收敛的话，牛顿法可以找到全局最优点\nlogistic 回归的牛顿法优化可以表示成：\n$$ \\begin{align*} \u0026 w \\leftarrow 0 \\\\ \u0026 \\text{repeat until convergence} \\\\ \u0026 \\quad e \\leftarrow \\text{solution to normal equations} (X^T\\Omega X) e = X^T (y - s) \\\\ \u0026 \\quad w \\leftarrow w + e \\end{align*} $$牛顿法优化 logistic 回归是，$e$ 受远离决策边界且被分类错误的点影响最大（$y_i - s_i$ 最大），受远离决策边界且被分类正确的点影响最小（$y_i - s_i$ 最小）\n如果数据集非常大，为了加快训练，开始可以失效所以样本点的子集（初期的更新比较粗略），随着迭代进行逐渐增大子集的大小（后期需要精细的参数更新）\nLDA 和 logistic 回归对比 LDA 的优点：\n对于可以很好分离的类别，LDA 很稳定，而 logistic 回归不稳定 对于大于 2 个类别的情况，LDA 仍然简洁，而 logistic 回归需要改为 softmax 回归 LDA 一般对于样本符合高斯分布的情况表现更好，特别是样本量 $n$ 较小的情况 logistic 回归的优点：\nlogistic 回归更加强调决策边界 logistic 回归对于距离边界不同距离的点施加的权重不同，而 LDA 会一视同仁。所以 logistic 回归能够更好地降低训练损失，但是对异常数据更敏感 logistic 回归对非高斯数据的鲁棒性更好 ROC 曲线 ROC曲线 ROC 曲线展示了在不同的判别门限设置下，假阳性和真阳性的比例变化的曲线，ROC 曲线可以通过在不同的判别门限下测试得到\nx 轴表示假阳性（负类别别判别成正类别）的比例 y 轴表示真阳性的比例 曲线到顶部的垂直距离表示假阴性的比例 曲线到右边的水平距离表示真阴性的比例 右上角的点表示永远被判别为正例，左下角的点表示永远被判别为负例，对角线表示一个随机分类器\n通过计算 ROC 曲面下的面积可以衡量一个分类器的好坏，面积为 1 表示分类器永远可以正确分类，面积为 0.5 表示一个随机分类器\n","date":"2025-09-26T16:50:48+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/","title":"机器学习 11 回归 II：牛顿法和ROC曲线"},{"content":"回归问题 分类问题：给定样本点 $x$，预测类别 回归问题：给定样本点 $x$，预测数值 QDA和LDA不仅仅估计了一个分类器，它们也可以给出估计正确的概率，所以QDA和LDA是对概率的回归\n回归问题的拟合可以分成两个部分：\n选择一个回归函数 $h(x;w)$，其中参数为 $w$ 选择一个优化的代价函数（通常基于损失函数定义）1 一些回归函数：\n（1） 线性：$h(x;w,\\alpha)=w\\cdot x + \\alpha$\n（2） 多项式型\n（3） logistic2：$h(x;w,\\alpha)=s(w\\cdot x + \\alpha)$，其中 $s$ 是logistic函数 $s(\\gamma)=\\frac{1}{1+e^{-\\gamma}}$\n一些损失函数：\n（A） 平方误差：$L(\\hat{y}, y) = (\\hat{y} - y)^2$\n（B） 绝对值误差：$L(\\hat{y}, y) = |\\hat{y} - y|$\n（C） logistic损失（交叉熵）：$L(\\hat{y}, y) = -y \\ln \\hat{y} - (1 - y) \\ln (1 - \\hat{y})$\n一些代价函数：\n（a） 平均：$J(h) = \\frac{1}{n} \\sum_{i = 1}^n L(h(X_i), y_i)$\n（b） 最大：$J(h) = \\max_{i=1}^n L(h(X_i), y_i)$\n（c） 加权：$J(h) = \\sum_{i = 1}^n w_i L(h(X_i), y_i)$\n（d） $l_2$ 正则化：$J + \\lambda \\|w\\|_2^2$\n（e） $l_1$ 正则化：$J + \\lambda \\|w\\|_1$\n上述不同的回归和损失与代价函数的选择，可以组成不同的算法：\n最小二乘回归（Least-squares linear regression）：(1) + (A) + (a)，成本函数是二次的，可以通过微积分直接求解 岭回归（Ridge regression）：(1) + (A) + (a) + (d)，成本函数是二次的，可以通过微积分直接求解，$l_2$ 范数限制权重不过大防止过拟合 Lasso：(1) + (A) + (a) + (e)，二次规划问题，$l_1$ 正则化倾向于某些权重变为0，适合特征较多时的特征选择 逻辑回归（logistic regression）：(3) + (C) + (a)，成本函数是凸函数，可以通过梯度下降法求解 最小绝对偏差（least absolute deviations）：(1) + (B) + (a)，线性规划，绝对误差对离群点不那么敏感 最小二乘回归 线性回归函数 + 平方损失 + 平均代价\n问题描述：寻找$w$，$\\alpha$使得$\\sum_{i = 1}^n (X_i \\cdot w + \\alpha - y_i)^2$最小。如果将偏置$\\alpha$包含在权重$w$中，使$w$为一个$d+1$维的向量，那么上述优化问题可以表述成最小化$RSS(w) = \\| X w - y \\|^2$\n$RSS(w)$为凸函数，在导数为$0$达到最小值，即$2X^TXw - 2X^T y = 0$，解得$w = (X^T X)^{-1} X^T y$\n上式包含一个假设，即$X^T X$是非奇异的（如果某些样本点共线或者共面），反之解不唯一或不存在\n可以看出$((X^T X)^{-1} X^T)X=I$，所以$X^+ = (X^T X)^{-1} X^T$被称为矩阵$X$的伪逆\n将$w$的解代入可得$y=X(X^T X)^{-1} X^T y = Hy$，其中矩阵$H$被称为帽子矩阵（hat matrix）\n优点：容易计算，有唯一、稳定的解 缺点：对异常值敏感，因为误差被平方；如果$X^T X$是奇异的则无法适用 逻辑回归 logistic 函数 + logistic 损失 + 平均代价\n拟合概率，通常被用于分类问题，标签$y_i$可以为任意概率值，但是通常为$0$或$1$\nQDA和LDA是生成模型，而逻辑回归时判别模型\n问题描述：寻找$w$使其可以最小化代价函数\n$$ J = \\sum_{i = 1} ^n L(s(X_i \\cdot w), y_i) = -\\sum_{i = 1}^n \\left( y_i \\ln s(X_i \\cdot w) + (1 - y_i) \\ln(1 - s(X_i \\cdot w)) \\right) $$ 可以看出，在预测概率$\\hat{y}$和真实概率相同的条件下，logistics损失达到最小值\n逻辑回归的代价函数$J(w)$是凸的，可以通过梯度下降法求解\nlogistic函数的导数为\n$$ s'(\\gamma) = \\frac{\\mathrm{d}}{\\mathrm{d}\\gamma} \\frac{1}{1 + e^{-\\gamma}} = \\frac{e^{-\\gamma}}{(1 + e^{-\\gamma})^2} = s(\\gamma)(1 - s(\\gamma)) $$$J(w)$管于$w$的梯度为\n$$ \\begin{aligned} \\nabla_w J \u0026= - \\sum \\left( \\frac{y_i}{s_i} \\nabla s_i - \\frac{1 - y_i}{1 - s_i} \\nabla s_i \\right) \\\\ \u0026= - \\sum \\left( \\frac{y_i}{s_i} - \\frac{1 - y_i}{1 - s_i} \\right) s_i (1 - s_i) X_i \\\\ \u0026= - \\sum(y_i - s_i) X_i \\\\ \u0026= -X^T (y - s) \\end{aligned} $$其中$s = \\begin{bmatrix} s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_n \\end{bmatrix}$是$n$个估计值组成的向量\n于是梯度下降的准则为\n$$ w \\leftarrow w + \\varepsilon X^T (y - s) $$对于线性可分数据，逻辑回归的梯度下降法虽然能保证找到一个完美的分类边界，并且这个边界最终会是理论上最优的最大间隔，但其权重会趋于无穷大，且收敛速度极慢\n损失函数衡量的是单个训练样本的预测值与真实值之间的误差；代价函数衡量的是整个数据集的误差 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nlogistic的形式和LDA类似，所以我们也可以不使用高斯分布拟合数据，而是直接通过logistic函数去拟合概率\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-06-22T20:29:21+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_10_regression/","title":"机器学习 10 回归 I：最小二乘回归和逻辑回归"},{"content":"我们已经定义了多维正态分布\n$$ f(x)=n(q(x)),\\quad n(q) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}}e^{-q/2},\\quad q(x)=(x-\\mu)^T \\Sigma^{-1} (x-\\mu) $$其中有三个重要的矩阵：\n$\\Sigma = V\\Gamma V^T$ 是协方差矩阵，其中它的特征值是沿着对应的特征向量方向的方差，即 $Var(v_n^TX)=\\lambda_n$\n$\\Sigma^{1/2}=V\\Gamma^{1/2}V^T$ 将球面映射到椭球面，它的特征值是椭球面的半轴长\n$\\Sigma^{-1}=V\\Gamma^{-1}V^T$ 是精度矩阵，它的二次型决定了曲面的形状\n各向异性高斯的参数的最大似然参数估计 给定训练样本 $X_1, \\dots, X_n$ 和类别 $y_1, \\dots, y_n$，我们期望拟合一个最优的高斯分布，使得每个样本点都能归属到对应的类别\n首先估计出先验概率 $\\hat{\\pi}_C = a / n$，均值 $\\hat{\\mu}_C = \\frac{1}{n} \\sum_{i = 1}^n X_i$\n对于 QDA，可以估计出条件协方差\n$$\\hat{\\Sigma}_C = \\frac{1}{n_C} \\sum_{i:y_i = C} (X_i - \\hat{\\mu}_C)(X_i - \\hat{\\mu}_C)^T$$其中 $\\hat{\\Sigma}_C$ 是一个半正定矩阵，如果它有零特征值，那么不存在逆矩阵 $\\hat{\\Sigma}_C^{-1}$，并且行列式 $| \\hat{\\Sigma}_C|=0$，QDA 将会失效，可以通过降维解决\n对于 LDA，所有类别的协方差相同\n$$ \\hat{\\Sigma} = \\frac{1}{n} \\sum_{C} \\sum_{i:y_i = C} (X_i - \\hat{\\mu}_C)(X_i - \\hat{\\mu}_C)^T $$QDA 选定类别 $C$ 使得 $P(Y=C|X=x)$ 最大，其实就是选定 $C$ 最大化二次判别函数\n$$ \\begin{align*} Q_C(x) \u0026= \\ln \\left( (\\sqrt{2\\pi})^d f_{X|Y=C}(x) \\pi_C \\right) \\\\ \u0026= -\\frac{1}{2} (x - \\mu_C)^T \\Sigma_C^{-1} (x - \\mu_C) - \\frac{1}{2} \\ln |\\Sigma_C | + \\ln \\pi_C \\end{align*} $$在多类别的条件下，只需要选择最大的判别函数对应的类别\n上图中，在两个类别的场景下 左图是两个类别的$C$和$D$的条件概率分布 中间的决策函数 $Q_C(x) - Q_D(x)$ 是一个二次函数，所以决策边界是一个二次曲面\n右图中是后验概率$P(Y=C | X= x) = s(Q_C(x) - Q_D(x))$\n如果我们准确已知真实的参数$\\pi_C$，$\\mu_C$和$\\Sigma_C$，我们可以直接得到贝叶斯分类器和贝叶斯最优决策边界；如果我们只能从数据中估计$\\hat{\\pi}_C$，$\\hat{\\mu}_C$和$\\hat{\\Sigma}_C$，这个过程就是QDA算法，QDA分类器只能逼近贝叶斯分类器\nLDA 如果所有类别的协方差$\\Sigma$相同，决策函数中的二次项互相抵消，决策边界是一个线性超平面\n$$ Q_C(x) - Q_D(x) =\\underbrace{ (\\mu_C - \\mu_D)^T \\Sigma^{-1} x }_{w^T x} \\underbrace{ - \\frac{\\mu_C^T \\Sigma^{-1} x - \\mu_D^T \\Sigma^{-1} \\mu_D}{2} + \\ln\\pi_C -\\ln\\pi_D}_{+\\alpha} $$其中决策边界为$w^T x + \\alpha = 0$ 后验概率为$P(Y = C | X = x) = s(w^T x + \\alpha)$\n多类别LDA的情况：选择使线性判别函数最大的类别$C$，判别函数的定义为\n$$ \\mu_C^T \\Sigma^{-1} x - \\frac{\\mu_C^T \\Sigma^{-1} \\mu_C}{2} + \\ln \\pi_C $$ QDA和LDA的对比 在两个类别的条件下，虽然我们估计了高斯分布地较多参数，但是在决策函数$Q_C(x) - Q_D(x)$中 LDA有$d+1$个参数（包括$w$的$d$个参数和$\\alpha$的1个参数），所以LDA更加容易欠拟合 QDA有$\\frac{d(d+3)}{2}+1$个参数1，所以QDA更加容易过拟合 在上图中，贝叶斯决策边界是紫色的虚线，QDA决策边界是绿色的实线，LDA的决策边界是黑色的点线。当贝叶斯决策边界是线性的时，LDA可以得到更加稳定的拟合，而QDA可能过拟合；当贝叶斯决策边界是曲线时，QDA可能得到更好的拟合效果\n改变先验概率$\\pi$或者损失，等价于对决策函数施加额外的常数偏置2\n在二分类的情况下，选择$p$的决策阈值，等价于选择$\\pi_C=1-p，\\pi_D=p$；或者选定非对称损失，假阳性损失为$p$，假阴性损失为$1-p$\n一些额外的术语 假设$X$是一个$n\\times d$的设计矩阵（design matrix），它的每一行是一个$d$维的样本点$X_i^T$\n对$X$进行中心化：从$X$的每一行减去$\\hat{\\mu}^T$，得到$\\dot{X}$，其中$\\hat{\\mu}^T$是X的所有行的平均，所以$\\dot{X}$的所有行的均值为0 此时可以计算出所有样本点的协方差$\\text{Var}(R)=\\frac{1}{n}\\dot{X}^T\\dot{X}$ 对$\\dot{X}$去相关：对$\\dot{X}$进行旋转，即$Z=\\dot{X}V$，其中$\\text{Var}(R)=V\\Lambda V^T$ 上式通过旋转将样本点转换到了特征向量对应的坐标系3，此时特征向量就是坐标轴，所以$\\text{Var}(Z)=\\Lambda$，数据点在不同坐标轴方向的相关性为0 对$\\dot{X}$球化：$W=\\dot{X} \\text{Var}(R)^{-1/2}$4 对$X$白化：中心化+球化，$X \\rightarrow \\dot{X} \\rightarrow W$ 白化被应用到很多机器学习算法，例如支持向量机和神经网络。某些特征的数值可能远大于其他特征，例如它们的测量单位不同。SVM 对由数值大的特征的惩罚，会比对数值小的特征更重\nQDA的决策函数$Q_C(x) - Q_D(x)=x^T \\underbrace{(\\Sigma_D^{-1} - \\Sigma_C^{-1})}_{d(d-1) / 2 \\text{ params}}x + \\underbrace{(\\mu_C^T \\Sigma_C^{-1} - \\mu_D^T \\Sigma_D^{-1})}_{d \\text{ params}} x + \\underbrace{\\dots}_{1 \\text{ params}}$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n贝叶斯决策规则：$\\text{if } L(-1,1)P(Y=1|X=x) \u003e L(1, -1)P(Y=-1|X=x) \\text{ then } r^*(x) = 1$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n矩阵$V$是一个由特征向量组成的正交矩阵，用它乘原数据，相当于将原坐标变换到这些特征向量对应的新坐标系\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n$\\Sigma^{1/2}$将球面映射到椭球面\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-06-21T20:02:09+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/","title":"机器学习 09 高斯判别分析 IV：各向异性高斯"},{"content":"特征向量 给定一个方阵 $A$，如果存在一个向量 $v \\neq 0$ 和常量 $\\lambda$，满足 $Av = \\lambda v$，那么 $v$ 是 $A$ 的一个特征向量，$\\lambda$ 是与其对应的 $A$ 的特征值\n特征向量乘以 $A$ 后，仍然指向相同的或者相反的方向\n定理：如果 $v$ 是 $A$ 的特征向量，并且其对应的特征值为 $\\lambda$，那么 $v$ 也是 $A^k$ 的特征向量，对应的特征值为 $\\lambda^k$\n谱定理（Spectral Theorem）：每个 $n \\times n$ 维的实对称矩阵有 $n$ 个实数特征值和 $n$ 个特征向量，并且不同特征值的特征向量之间正交\n使用特定的特征向量构建矩阵 选定 n 个互相正交的单位向量 $v_1,\\dots,v_n$，那么矩阵 $V=\\begin{bmatrix}v_1 \u0026 v_2 \u0026 \\cdots \u0026 v_n\\end{bmatrix}$ 满足 $V^TV=VV^TI$，$V$ 是一个正交矩阵\n正交矩阵乘一个向量，不会改变向量的长度；正交矩阵同乘两个向量，不会改变两个向量之间的夹角\n选定特征值构成对角矩阵\n$$ \\Lambda = \\begin{bmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\ 0 \u0026 \\lambda_2 \u0026 \u0026 0 \\\\ \\vdots \u0026 \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\cdots \u0026 \\lambda_n \\end{bmatrix} $$根据特征向量的定义，有：$AV=V\\Lambda$\n两侧同乘 $V^T$ 可得 $$A=V\\Lambda V^T = \\sum_{i=1}^n \\lambda_iv_iv_i^T$$ 上式即为矩阵的特征分解\n如果已知一个对称概率密度矩阵 $\\Sigma$，我们可以通过以下方式得到它的平方根 $A=\\Sigma^{1/2}$：\n计算 $\\Sigma$ 的特征向量和特征值 计算 $\\Sigma$ 的特征值的平方根 使用 $\\Sigma$ 的特征向量和特征值的平方根构造矩阵 $A$ 二次型 Quadratic Form 矩阵 $M$ 的二次型为 $x^T M x$\n假设我们已知一个二次型 $q_s(z)=z^TIz =\\|z\\|^2$ ，它的等值线为圆形如上图左边\n我们有一个变化矩阵\n$$ A = V\\Lambda V^T = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{bmatrix} \\begin{bmatrix} 2 \u0026 0 \\\\ 0 \u0026 -\\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\end{bmatrix} $$通过变换 $z = Ax$ 将 $z-$ 空间中的圆形等高线变为上图右边 $x-$ 空间中的椭圆等高线 $q_e(x)$。在等高线 $q_e(x)$ 中，$(1,1)$ 被放大 2 倍，$(1, -1)$ 方向被收缩 -1/2 倍\n可以求出等高线的表达式 $$q_e(x) = q_s(A^{-1}x) = \\|A^{-1}x\\|^2=x^TA^{-2}x$$ 可知二次方程 $x^TA^{-2}x$ 的等高线是一个由 $A$ 的特征向量和特征值决定的椭圆\n同理可得，$\\{x:x^TA^{-2}x=1\\}$ 是一个椭球体，它的轴线是 $v_1,v_2,\\dots,v_n$，半轴径是 $\\lambda_1,\\lambda_2,\\dots,\\lambda_n$（$\\|Av_n\\|=\\lambda_n$）\n所以我们可以知道，矩阵 $M$ 的二次型 $x^TMx$ 的等高面是一个由 $M^{-1/2}$ 的特征向量和特征值决定的椭球面\n特殊情况：如果 $M$ 是对角矩阵，那么特征向量就是坐标轴，及椭球面的轴线和坐标轴平行\n已知一个对称矩阵 $M$，那么\n如果 $w^TMw\u003e0$ 对于所有的 $w\\neq0$ 都成立，那么 $M$ 是正定矩阵（positive definite matrix），所有特征值都是正数； 如果 $w^TMw\\ge0$ 对于所有的 $w\\neq0$ 都成立，那么 $M$ 是半正定矩阵（positive semi-definite matrix），所有特征值都是非负数； 如果 $w^TMw\\le0$ 对于所有的 $w\\neq0$ 都成立，那么 $M$ 是半负定矩阵（negative semi-definite matrix），所有特征值都是非正数； 如果既不是半正定矩阵又不是半负定矩阵，则 $M$ 是不定矩阵（indefinite matrix） 可逆矩阵$\\Leftrightarrow$ 行列式不为 0 $\\Leftrightarrow$ 没有 0 特征值\n对于正定矩阵，它在所有方向上束缚住了曲面，形成了一个封闭的椭球面；而半正定矩阵在某些方向松开，使得曲面可以在这些方向无限延生，形成了柱面；如果不是正定矩阵，则为双曲面\n各向异性高斯 Anisotropic Gaussians 对于一个多维高斯分布 $X\\sim \\mathcal{N}(\\mu,\\Sigma)$，$X$ 和 $\\mu$ 是一个 $d$ 维向量，如果在不同的方向有不同的方差，概率密度函数可以表示为\n$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}}\\exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$$其中 $\\Sigma$ 是一个半正定矩阵，被称为协方差矩阵；$\\Sigma^{-1}$ 也是半正定矩阵，被称为精度矩阵（precision matrix）\n如果把概率密度函数写成 $f(x)=n(q(x))$，其中 $q(x)=(x-\\mu)^T\\Sigma^{-1}(x-\\mu)$\n$q(x)$ 是一个中心在 $\\mu$ 的柱面，由精度矩阵 $\\Sigma^{-1}$ 决定；$n(\\cdot)$ 是一个单调凸函数，不会改变等值面的形状（只改变数值，不改变性质和位置，如下图）\n对于高斯随机过程的概率密度函数来说，对应的等值面和 $q(x)=(x-\\mu)^T\\Sigma^{-1}(x-\\mu)$ 相同，只是经过了数值缩放。它在均值 $\\mu$ 处到达最大值，在远离 $\\mu$ 处逐渐趋近于零\n$q(x)$ 也等于 $\\Sigma^{-1 / 2}x$ 到 $\\Sigma^{-1 / 2}\\mu$ 的距离的平方，即 $$d(x, \\mu) = \\|\\Sigma^{-1 / 2}x - \\Sigma^{-1 / 2}\\mu\\| = \\sqrt{(x- \\mu)^T \\Sigma^{-1} (x - \\mu)} = \\sqrt{q(x)}$$协方差 假设 $R, S$ 是随机变量，可以是列向量或者常量，那么\n协方差 $Cov(R, S) = E[(R- E[R])(S - E[S])^T]=E[RS^T] - \\mu_R \\mu_S^T$\n方差 $Var(R) = Cov(R, R)$\n如果 $R$ 是一个向量，那么 $R$ 的协方差矩阵为\n$$ Var(R) = \\begin{bmatrix} Var(R_1) \u0026 Cov(R_1, R_2) \u0026 \\cdots \u0026 Cov(R_1, R_d) \\\\ Cov(R_2, R_1) \u0026 Var(R_2) \u0026 \u0026 Cov(R_2, R_d) \\\\ \\vdots \u0026 \u0026 \\ddots \u0026 \\vdots \\\\ Cov(R_d, R_1) \u0026 Cov(R_d, R_2) \u0026 \\cdots \u0026 Var(R_d) \\end{bmatrix} $$对于一个符合高斯分布的变量 $R \\sim \\mathcal{N}(\\mu, \\Sigma)$，有 $Var(R) = \\Sigma$\n如果两个随机变量 $R_i, R_j$ 独立，那么 $Cov(R_i, R_j) = 0$ （独立 $\\Rightarrow$ 不相关） 如果 $Cov(R_i, R_j) = 0$，并且他们一起满足多维高斯分布，那么他们独立（高斯分布限制了只能存在线性关系+不相关 $\\Rightarrow$ 独立） 如果 $Var(R)=\\Sigma$ 是一个对角矩阵，并且满足多维高斯分布（向量中的每个元素互相独立）\n$\\Leftrightarrow f(R) = f(R_1)f(R_2)\\cdots f(R_d)$\n$\\Rightarrow$ 椭球面是轴对齐的，并且 $\\Sigma$ 的对角元素等于半径的平方\n","date":"2025-06-18T20:45:13+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/","title":"机器学习 08 高斯判别分析 III：特征向量和多维正态分布"},{"content":"高斯判别分析 Gaussian Discriminant Analysis 基本假设：每个类别服从高斯分布\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2): f(x) = \\frac{1}{(\\sqrt{2 \\pi} \\sigma)^d}\\exp{\\left( -\\frac{\\|x - \\mu\\|^2}{2\\sigma^2} \\right)}$$其中 $x,\\mu$ 是向量，$\\sigma$ 是标量，$d$ 表示维度。（注：这里使用的正态分布公式是一个简化版本，它假设数据在所有的特征维度上的方差相同，并且不考虑不同特征之间的相关性）\n对于每个类别 $C$，假设我们知道了均值 $\\mu_C$ 和协方差 $\\sigma_C^2$，那么我们就可以根据上述高斯分布公式，将均值和方差代入确定概率密度函数 $f_{X|Y=C}(x)$。同时假设前验概率 $\\pi_C=P(Y = C)$ 已知\n在给定观测 $x$ 的条件下，贝叶斯决策准则 $r^*(x)$ 根据最大化 $f_{X|Y=C}(x)$ 来判决类别 $C$\n两个类别的概率密度函数 等价于最大化\n$$Q_C(x) = \\ln\\left( (\\sqrt{2\\pi})^d f_{X|Y=C}(x) \\pi_C \\right)=-\\frac{\\|x - \\mu_C\\|^2}{2\\sigma_C^2}-d\\ln\\sigma_C + \\ln\\pi_C$$$Q_C(x)$ 是一个关于 $x$ 的二次函数\n二次判别分析 Quadratic Discriminant Analysis (QDA) 假设只有两个类别 $C$ 和 $D$，那么贝叶斯分类器为\n$$ r^*(x) = \\begin{cases} C \u0026 \\text{if } Q_C(x) - Q_D(x) \u003e 0 \\\\ D \u0026 otherwise \\end{cases} $$决策函数为 $Q_C(x) - Q_D(x)$，贝叶斯决策边界为 $\\{ x: Q_C(x) - Q_D(x) = 0 \\}$。如果样本点是 $1$ 维的，那么贝叶斯决策边界可能有 $1$ 个或者 $2$ 个点（二次方程的解）；如果样本点是 $d$ 维的，那么贝叶斯决策边界是一个二次曲面。\n除了根据贝叶斯规则判别样本点的类别外，也可以得到判别正确的概率，即 $$P(Y = C | X = x) = \\frac{f_{X | Y = C}\\pi_C}{f_{X | Y = C}\\pi_C + f_{X | Y = D}\\pi_D}$$ 上式也可以进一步写成 $$P(Y = C | X = x) = \\frac{e^{Q_C(x)}}{e^{Q_C(x)} + e^{Q_D(x)}} = s(Q_C(x) - Q_D(x))$$ 其中 $s(\\gamma)=\\frac{1}{1 + e^{-\\gamma}}$ 是 Logistic 函数也即是 Sigmoid 函数\nLogistic函数 Logistic 函数满足 $s(0) = 0.5$，也就是说当 $Q_C(x) = Q_D(x)$ 时，判别成两种类型的概率均等\n多类别QDA判决边界 多类别 QDA 将特征空间划分为多个区域。在二维或更高维度中，通常会形成多个决策边界，这些边界在连接点处相互邻接\n线性判决分析 Linear Discriminant Analysis (LDA) LDA 时 QDA 的一个变种，它有线性的决策边界，相较于 QDA 不容易过拟合\n基本假设：所有的高斯分布有相同的方差 $\\sigma^2$\n此时贝叶斯分类器是一个线性分类器\n$$Q_C(x) - Q_D(x) = \\underbrace{\\frac{(\\mu_C - \\mu_D) \\cdot x}{\\sigma^2}}_{w \\cdot x} \\underbrace{- \\frac{\\|\\mu_C\\|^2 - \\|\\mu_D\\|^2}{2\\sigma^2} + \\ln\\pi_C - \\ln\\pi_D} _{+ \\alpha}$$ 决策边界为 $w \\cdot x + \\alpha = 0$ 前验概率为 $P(Y=C | X= x) = s(w \\cdot x + \\alpha)$ 特殊情况：如果 $\\pi_C = \\pi_D = 0.5$，那么决策边界变为 $(\\mu_C - \\mu_D) \\cdot x - (\\mu_C - \\mu_D) \\cdot \\left( \\frac{\\mu_C + \\mu_D}{2} \\right) = 0$，此时变成了质心法\n多类别 LDA：选择类别 C，最大化线性决策函数 $\\frac{\\mu_C \\cdot x}{\\sigma^2} - \\frac{\\|\\mu_C\\|^2}{2\\sigma^2} + \\ln\\pi_C$\n多类别LDA决策边界 概率分布的参数估计 最大似然估计：通过选择使似然函数 $\\mathcal{L}$ 最大化的参数来得到统计模型的参数估计值\n估计前验概率 $\\pi_C$ 一个不均匀硬币，有概率 $p$ 得到正面，$1 - p$ 的概率得到反面。在 $n$ 次试验中，有 $a$ 次为正面，则似然函数为 $$\\mathcal{L} = C_n^a p^a (1 - p)^{n - a}$$ 通过最大似然法，使 $\\mathcal{L}$ 最大，$\\frac{\\partial \\mathcal{L}}{\\partial p} = 0$ ，得到 $p = \\frac{a}{n}$\n类似的，如果共有 $n$ 个训练样本点，其中 $a$ 个属于类别 C，则前验概率 $\\hat{\\pi}_C = a / n$\n估计均值 $\\mu_C$ 和方差 $\\sigma_C^2$ 根据 $n$ 个样本点 $X_1,X_2,\\dots,X_n$ 求出最拟合的高斯分布\n似然函数为 $$\\mathcal{L}(\\mu, \\sigma;X_1,X_2,\\dots,X_n) = f(X_1)f(X_2)\\cdots f(X_n)$$ 为了方便计算，最大化对数似然函数\n$$ \\begin{align*} l(\\mu, \\sigma;X_1,\\dots,X_n) \u0026= \\ln{f(X_1)} + \\dots \\ln{f(X_n)} \\\\ \u0026= \\sum_{i = 1}^n \\left( -\\frac{\\| X_i - \\mu\\|^2}{2 \\sigma^2} - d\\ln{\\sqrt{2\\pi} -d \\ln{\\sigma}} \\right) \\end{align*} $$可得\n$$ \\begin{align*} \u0026 \\nabla_ul = \\sum_{i = 1}^n \\frac{X_i - \\mu}{\\sigma^2} = 0 \\quad \\Rightarrow \\quad \\hat{\\mu} = \\frac{1}{n} \\sum_{i = 1}^nX_i \\\\ \u0026 \\frac{\\partial l}{\\partial \\sigma} = \\sum_{i = 1}^n \\frac{\\|X_i - \\mu\\|^2 - d\\sigma^2}{\\sigma^2} = 0 \\quad \\Rightarrow \\quad \\hat{\\sigma}^2 = \\frac{1}{dn} \\sum_{i = 1}^n \\|X_i - \\hat{\\mu} \\|^2 \\end{align*} $$对于 QDA，分别使用类别内的数据点每个类别 C 的均值和方差，使用所有数据点估计前验概率 $\\hat{\\pi}_C = \\frac{n_C}{\\sum_D n_D}$\n对于 LDA，使用同样的方法估计每个类别的均值以及前验概率，然后使用所有数据估计方差 $$\\hat{\\sigma}^2 = \\frac{1}{dn} \\sum_C \\sum_{\\{i:y_i = c\\}} \\|X_i - \\hat{\\mu}_C\\|^2$$","date":"2025-05-23T23:21:38+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/","title":"机器学习 07 高斯判别分析 II：各向同性高斯"},{"content":"决策理论亦风险最小化 如果多个属于不同类别的样本点可能处于同一个位置（例如训练数据中，升高为 1.70m 的样本，可能男性和女性同样存在），此时我们没法通过一个分类器直接将样本划分为某一种类别，所以我们需要一个概率分类器，输出当前样本为某种类别的概率\n贝叶斯定理：\n$$P(Y| X) = \\frac{P(X| Y) P(Y)}{P(X)}$$其中 $P(Y | X)$ 表示观测到 $X$ 的条件下事件 $Y$ 发生的后验概率，$ P(Y)$ 表示事件发生的先验概率。\n例如，假设人群中 $10\\%$ 的人患有癌症，$90\\%$ 的人没有癌症，不同职业的人患有癌症的概率分布为\njob $(X)$ miner $(X = 0)$ farmer $(X = 1)$ other ${} (X = 2)$ cancer ${} (Y = 1)$ $20\\%$ $50\\%$ $30\\%$ no cancer $(Y = -1)$ $1\\%$ $10\\%$ $89\\%$ 我们可以计算出，农民没有癌症的概率为\n$$P(Y = -1 | X = 0) = \\frac{P(X = 0 | Y = -1) P (Y = -1)}{P(X = 0)}=\\frac{0.1 \\times 0.9}{0.1 \\times 0.5 + 0.9 \\times 0.1} = \\frac{9}{14}$$在我们的概率分类器中，可能不同的错误判决类型带来的影响不同。例如将癌症判决为假阴性（本来有癌症但被判决为没有癌症）和假阳性（本来没有被判决为有）的后果差别非常大，前者会造成癌症被忽视，后者只会带来额外的检查开销。此时，我们可以通过损失函数衡量不同错误的代价\n$$ L(\\hat{y}, y) = \\begin{cases} 1 \u0026 \\text{if } \\hat{y} = 1, y = -1 \\\\ 5 \u0026 \\text{if } \\hat{y} = -1, y = 1 \\\\ 0 \u0026 \\text{if } \\hat{y} = y \\end{cases} $$其中 $\\hat{y}$ 表示预测的类别，$y$ 表示真实的类别， 假阴性在损失函数中被施加了更多的权重，上述损失函数被称为是非对称的\n相反的，一个对称的0-1 损失函数可以表示为\n$$ L(\\hat{y}, y) = \\begin{cases} 1 \u0026 \\text{if } \\hat{y} \\neq y \\\\ 0 \u0026 \\text{if } \\hat{y} = y \\end{cases} $$定义 $r: \\mathbb{R}^d \\rightarrow \\pm 1$ 是一个决策规则，也被称为分类器，它表示一个将特征向量 $x$ 映射到 1 或 -1 的函数\n$r$ 的风险指在所有的可能的数据点 $(x, y)$ 上的期望损失\n$$ \\begin{align*} R(r) \u0026= E[L(r(X), Y)] \\\\ \u0026= \\sum_x \\left( L(r(x), 1)P(Y = 1 | X = x) + L(r(x), -1)P(Y = -1 | X = x) \\right) P(X = x) \\\\ \u0026= P(Y = 1) \\sum_{x} L(r(x), 1) P(X = x | Y = 1) +\\\\ \u0026 \\quad P(Y = -1) \\sum_x L(r(x), -1) P(X = x | Y = -1) \\end{align*} $$贝叶斯决策规则（贝叶斯分类器）就是可以最小化风险函数 $R(r)$ 的函数 $r^*$，当 $L(1, 1) = L(-1, -1) = 0$，即预测正确损失函数为 0 的情况下\n$$ r^* (x) = \\begin{cases} 1 \u0026 \\text{if } L(-1, 1)P(Y = 1 | X = x) \u003e L(1, -1)P(Y = -1 | X = x) \\\\ -1 \u0026 \\text{otherwise} \\end{cases} $$当损失函数 $L$ 是对称的时，$r^*$ 就是选择后验概率最大的类别\n如果我们可以知道真实的概率分布，可以直接根据上式构建一个理想的概率分类器 $r^*$。但是真实场景中，我们只能通过一些统计手段估计概率分布\n获得使风险函数最小的 $r^*$ 的过程被称为风险最小化\n连续概率分布 假设 $X$ 有一个连续的概率密度函数，则\n概率密度函数 $X \\in [x_1, x_2]$ 的概率为 $\\int_{x_1}^{x_2} f(x) dx$ 概率密度函数包含的整个区域面积为 $\\int_{-\\infty}^{\\infty} f(x)dx = 1$ $g(X)$ 的期望为 $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f(x) dx$ $X$ 的均值为 $\\mu = E[X] = \\int_{-\\infty}^{\\infty}x f(x)dx$，方差为 $\\sigma^2 = E[(X - \\mu)^2] = E[X^2] - \\mu^2$ 假设我们使用 0-1 损失函数，在连续概率分布的情况下，此时贝叶斯决策规则为\n$$ r^* (x) = \\begin{cases} 1 \u0026 \\text{if } f_{X | Y = 1}(x)P(Y = 1) \u003e f_{X | Y = -1}(x)P(Y = -1) \\\\ -1 \u0026 \\text{otherwise} \\end{cases} $$上式表示，在观测到数据 $x$ 时，如果它有更多的可能为类别 1 则被判别为类别 1，反之不为类别 1\n我们可以画出 $f_{X | Y = 1}(x)P(Y = 1)$ 和 $f_{X | Y = -1}(x)P(Y = -1)$ 的概率密度曲线\n贝叶斯最优决策边界 贝叶斯决策规则对应了一个贝叶斯最优决策边界，即 $f_{X | Y = 1}(x)P(Y = 1) = f_{X | Y = -1}(x)P(Y = -1)$，它是两个概率密度函数的交点。在贝叶斯最优边界左边的点 $x$ 会被判别为类别 1，右边的点 $x$ 会被判别为类别 -1\n贝叶斯最优决策边界是两个后验概率密度函数相交的边界，在这个边界上两者的概率密度相等，所以贝叶斯最优决策边界为 $\\{ x: P(Y = 1 | X = x) = 0.5 \\}$\n在连续概率分布下，风险函数可以写成\n$$ \\begin{align*} R(r) \u0026= E[L(r(X), Y)] \\\\ \u0026= P(Y = 1) \\int L(r(x), 1) f_{X | Y = 1}(x) dx + \\\\ \u0026 \\quad P(Y = -1) \\int L(r(x), -1) f_{X | Y = -1}(x) dx \\end{align*} $$在 $r$ 为贝叶斯决策规则 $r^*$ 的条件下，贝叶斯风险就是两个概率密度函数重合部分的面积 $$R(r^*) = \\int \\min_{y = \\pm 1} L(-y, y) f_{X |Y=y}(x) P(Y = y) dx$$构建分类器的三种方法 现在我们可以总结出三种构建分类器的方法：\n生成模型 Generative models（例如LDA） 假设样本点来自特定的概率分布，并且不同类别的分布不同 猜测概率分布的形式 对每个类别，给定了概率分布的形式 $f_{X | Y = C}(x)$， 使得概率分布的参数拟合类别 C 的点 对每个类别 C 估计 $P(Y = C)$ 根据贝叶斯定理计算 $P(Y | C)$ 根据后验概率的大小，得出最终的类别，即使得 $f_{X|Y=C}(x)P(Y = C)$ 的 C 判别模型 Discriminative models（例如 logistic 回归） 直接对 $P(Y|X)$ 建模 寻找决策边界（例如 SVM） 直接对 $r(x)$ 建模 相较于方法 3，方法 1 和 2 都可以直接给出估计正确的概率。相较于方法 2，方法 1 一般在有异常值和少量训练样本下更加稳定，但是现实问题中很难准确估计概率分布\n如果数据服从高斯分布或者一个特定的概率分布，一般使用生成模型效果更稳定\n","date":"2025-05-23T14:43:55+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_06_decision_theory/","title":"机器学习 06 高斯判别分析 I：决策理论"},{"content":"机器学习问题的抽象 机器学习问题可以抽象成数据、模型、优化问题、优化算法四个层级\n数据\n数据是否有标签？\n是：标签是类别（分类）还是数值（回归） 否：相似性（聚类）还是定位（降维） 模型\n决策函数选择：线性、多项式、逻辑、神经网络，\u0026hellip; 最近邻，决策树 特征选择 低容量 vs 高容量考虑（影响过拟合、欠拟合、推断） 最优化问题\n变量、目标函数、约束条件\n最优化算法\n梯度下降、单纯形、SVD\n最优化问题 无约束问题 目标：寻找一个 $w$ 最小化一个连续的目标函数 $f(w)$\n如果 $f$ 的导数也是连续的则称 $f$ 是光滑的\n全局最小值：对于任意 $v$，有 $f(w) \\le f(v)$\n局部最小值：一个 $w$ 附近的区间内，对于任意 $v$，有 $f(w)\\le f(v)$\n全局最小值和局部最小值 凸函数：函数 $f$ 是凸的，如果对于每个 $x, y \\in \\mathbb{R}^d$，连接 $(x, f(x))$ 和 $(y, f(y))$ 的切线在函数 $f$ 之上。\n也可以表示为：对于任意 $x, y \\in \\mathbb{R}^d$，以及 $\\beta \\in [0, 1]$，有 $f(x + \\beta(y - x)) \\le f(x) + \\beta (f(y) - f(x))$\n凸函数的和也是凸函数。感知机的风险函数是凸损失函数的和，所以它也是凸函数。\n一个连续函数如果在封闭的凸定义域内，则\n没有最小值（趋近于 $-\\infty$） 只有一个最小值 一个连通的最小值集合，且值相等（一个线段或者平面） 对于后两种情况，我们可以通过梯度下降法寻找最小值\n很多时候我们没法找到一个凸目标函数，所以梯度下降只能找到一个局部最小值而不是全局最小值。但是在很多情形下，例如在训练神经网络时，随机梯度下降仍然是一个比较好的选择\n线性规划 线性目标函数 + 线性约束函数\n目标：寻找 $w$ 最大化（或者最小化）$c \\cdot w$，并满足 $Aw \\le b$\n其中 $A$ 是一个 $n \\times d$ 维度的矩阵，$b \\in \\mathbb{R}^n$。所以上述不等式包含了 $n$ 个线性约束\n$$A_i \\cdot w \\le b_i, \\quad i \\in [1, n]$$ 线性规划 满足所有约束条件的 $w$ 的集合是一个凸多面体，被称为可行域$F$。优化过程是在 $F$ 中寻找一个沿方向 $c$ 最远的点\n凸多面体：对于任意 $p,q \\in P$，连接 $p,q$ 的线段完全在 $P$ 内\n在线性规划中，最优点通常位于边界或者顶点上，所以某些等式形式的约束被称为积极约束，积极约束对应的对偶变量通常非零。\n在支持向量机中，在对偶问题中如果 $a_i \u003e 0$，则原始问题中 $y_i (w^T x_i + b) = 1 - \\xi_i$，所以支持向量在活跃约束上\n线性规划最著名的算法是单纯性算法（simplex algorithm）\n二次规划 二次凸目标函数 + 线性不等式约束\n目标：寻找 $w$ 最小化 $f(w) = w^T Q w + c^T w$，并满足 $Aw \\le b$\n其中 $Q$ 是一个对称半正定矩阵\n如果 $Q$ 是一个正定矩阵，目标函数是严格凸的，那么二次规划只有一个唯一的全局最优解（如果有解的话）。如果 $Q$ 是一个半正定矩阵，目标函数是凸的，但不一定是严格凸的，可能有多个目标函数值相同的全局最优解。如果 $Q$ 是非正定的，目标函数是非凸的，可能存在多个局部最优解，求解全局最优解通常是 NP-hard\n","date":"2025-05-23T14:33:10+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_05_machine_learning_abstractions_and_numerical_optimization/","title":"机器学习 05 机器学习抽象和数值优化"},{"content":"软间隔支持向量机 硬间隔 SVM 存在两个问题：\n如果数据不是线性可分的，则算法将会失效； 对极端值（outliers）敏感，例如右图添加了一个极端值，严重改变了决策边界。 异常值造成了决策边界的偏移 软间隔支持向量机：通过引入松弛变量，允许一些样本点违背最小间隔的约束，此时约束条件可以变为\n$$y_i (X_i \\cdot w + \\alpha) \\ge 1 - \\xi_i$$其中 $\\xi_i$ 为引入的松弛变量，满足 $\\xi_i \\ge 0$。只有当样本点违背最小间隔约束时，松弛变量 $\\xi_i$ 不为 $0$。\n松弛变量存在的情况 此时，仍然定义间隔为 $1 / \\|w\\|$，为了防止松弛变量的滥用，我们在目标函数中添加一个损失项对其进行约束\n$$ \\begin{align*} \u0026 \\text{Find } w, \\alpha, \\xi_i \\text{ that minimize } \\|w\\|^2 + C\\sum_{i = 1}^n\\xi_i \\\\ \u0026 \\begin{aligned} \\text{subject to } \u0026 \\quad y_i(X_i \\cdot w + \\alpha) \\ge 1 - \\xi_i \u0026 \\text{for all } i \\in [1, n] \\\\ \u0026 \\quad \\xi_i \\ge 0 \u0026 \\text{for all } i \\in [1, n] \\end{aligned} \\end{align*} $$这是一个 $d + n + 1$ 维空间的、具有 $2n$ 个约束项的二次规划问题。其中 $C \u003e 0$ 是一个正则化超参数（regularization hyperparameter）。\n较小C 较大C 目标 最大化间隔 ${} 1 / \\|w\\|$ 保持大多数松弛变量为零或很小 风险 欠拟合（误分类许多训练数据） 过拟合（训练效果好，测试效果差） 异常值 不太敏感 非常敏感 边界（非线性时） 更“平坦” 更曲折 不同C值的决策边界，右下方C更大 特征与非线性 非线性决策边界：通过创建非线性特征将样本点提升到高维空间，那么高维的线性分类器等价于低维的非线性分类器\n样例 1：抛物线提升映射（The parabolic lifting map） 定义一个非线性映射，将 $d$ 维空间的点 $x$ 提升为 $d + 1$ 维空间的抛物面\n$$ \\begin{align*} \u0026 \\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d + 1} \\\\ \u0026 \\Phi(x) = \\begin{bmatrix} x \\\\ \\|x\\|^2 \\end{bmatrix} \\end{align*} $$ 点被提升到抛物面 上图中样本点由二维空间的点，经过非线性映射被提升到三维空间的抛物面。此时二维平面的球形决策边界，等价于三维平面的线性决策边界。\n定理：$\\Phi(X_1),\\Phi(X_2),\\dots,\\Phi(X_n)$ 线性可分 $\\leftrightarrow$ $X_1,X_2,\\dots,X_n$ 可以被超球面分离\n证明：考虑 $\\mathbb{R}^d$ 中的超球面，它的球心为 $c$，半径为 $\\rho$。$x$ 在超球面内当且仅当\n$$ \\begin{align*} \u0026 \\|x-c\\|^2 \u003c \\rho^2 \\\\ \u0026 \\|x\\|^2 - 2c \\cdot x + \\|c\\|^2 \u003c \\rho^2 \\\\ \u0026 \\begin{bmatrix} -2c^T \u0026 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ \\|x\\|^2 \\end{bmatrix} \u003c \\rho^2 - \\|c\\|^2 \\end{align*} $$其中 $\\begin{bmatrix}-2c^T \u0026 1\\end{bmatrix}$ 是 $\\mathbb{R}^{d + 1}$ 中的法向量，而 $\\Phi(x)$ 表示 $\\mathbb{R}^{d + 1}$ 中的一个点，所以 $\\mathbb{R}^{d}$ 中的超球面内等价于 $\\mathbb{R}^{d + 1}$ 中的超平面以下。\n样例 2：椭球体/双曲面/抛物面决策边界 不同的二次曲面 对于式 $$A x_1^2 + B x_2^2 + C x_3^2 + D x_1x_2 + E x_2x_3 + F x_3x_1 + G x_1 + H x_2 + I x_3 + \\alpha = 0$$ 可以表示上图中的任意一个二次曲面。\n所以，我们可以定义非线性映射\n$$ \\begin{align*} \u0026 \\Phi(x) = \\begin{bmatrix} x_1^2 \u0026 x_2^2 \u0026 x_3^2 \u0026 x_1x_2 \u0026 x_2x_3 \u0026 x_3x_1 \u0026 x_1 \u0026 x_2 \u0026 x_3 \\end{bmatrix}^T \\\\ \u0026 \\begin{bmatrix} A \u0026 B \u0026 C \u0026 D \u0026 E \u0026 F \u0026 G \u0026 H \u0026 I \\end{bmatrix} \\cdot \\Phi(x) + \\alpha \\end{align*} $$此时，决策函数可以为任意二次多项式，决策超曲面可以为任意二次曲面。$\\Phi-$ 空间的线性决策边界等价于 $x-$ 空间的二次曲面分类边界。\n样例 3：$p$ 次多项式作为决策函数 拥有不同次方的决策函数的硬支持向量机：1，2，5 增加决策函数的最高次\n线性不可分的数据可能由于最够高的非线性变为线性可分 高维提供了更高的自由度，可能找到更大的间隔，可能提升决策边界的鲁棒性 核技巧 支持向量机的学习算法 在训练软间隔支持向量机时，我们可以使用如下学习算法（基于拉格朗日对偶性，推过过程参考李航-《统计学习方法》）：\n输入：训练数据集 $T={(x_1,y_1),(x_2, y_2),\\dots,(x_n,y_n)}$，其中 $x_i \\in \\mathbb{R}^{d}, y_i \\in \\{-1,1\\}$\n输出：决策超平面和决策函数\n（1）选择一个正则化超参数 $C \u003e 0$, 求解以下凸二次规划问题（软间隔支持向量机的原始问题的对偶问题）：\n$$ \\begin{align*} \u0026 \\min_{\\alpha} \\frac{1}{2} \\sum_{i = 1}^n\\sum_{j = 1}^n\\alpha_i\\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i = 1}^n \\alpha_i \\\\ \u0026\\begin{aligned} \\text{s.t. } \u0026 \\sum_{i = 1}^n \\alpha_i y_i = 0 \\\\ \u0026 0 \\le \\alpha_i \\le C, \\quad i = 1,2,\\dots,n \\end{aligned} \\end{align*} $$求解最优解 $\\alpha^* = (\\alpha_1^*, \\alpha_2^*, \\dots, \\alpha_n^*)$\n（2）计算 $w^* = \\sum_{i = 1}^n \\alpha_i^* y_i x_i$\n选择一个下标 $j$，满足 $0\u003c \\alpha_j^* \u003c C$，计算\n$$b^* = y_j - \\sum_{i = 1}^n y_i \\alpha_i^*(x_i \\cdot x_j)$$（3）求得决策超球面 $$w^* \\cdot x + b^* = 0$$ 以及决策函数\n$$f(x) = w^* \\cdot x + b^* = \\sum_{i = 1}^n \\alpha_i^* y_i(x \\cdot x_i) + b^*$$在上述算法中，$w^*$ 和 $b^*$ 只依赖于 $\\alpha_i^* \u003e 0$ 的样本点 $x_i$，称这些样本点为支持向量。\n可以看出在支持向量机学习算法中，无论优化的目标函数还是决策函数都只涉及两个向量之间的点积，例如 $x_i \\cdot x_j$ 和 $x \\cdot x_i$。\n核函数 我们已经知道，低维空间的非线性决策曲面可以等价为高维空间的线性决策平面。\n但是如果特征的维度 $d$ 非常高，此时使用非线性映射 $\\Phi(x)$ 得到的高次多项式特征维度非常大，$\\Phi(x_i) \\cdot \\Phi(x_j)$ 的计算复杂度非常大。\n于是，我们希望找到一个低维空间核函数，等价于高维空间的点积运算\n定义：设 $\\mathcal{X}$ 是一个低维输入空间，$\\mathcal{H}$ 是一个高维特征空间，如果存在一个映射 $$\\Phi(x): \\mathcal{X} \\rightarrow \\mathcal{H}$$ 使得对所有的 $x, z \\in \\mathcal{X}$，函数 $K(x, z)$ 满足 $$K(x, z) = \\Phi(x) \\cdot \\Phi(z)$$ 则称 $K(x, z)$ 为核函数。\n如果我们不显示指定一个非线性映射 $\\Phi(x)$，而是使用一个特定的核函数 $K(x, z)$ 替代高维空间的点积 $\\Phi(x_i) \\cdot \\Phi(x_j)$ ，那么支持向量机的优化目标可以写成\n$$ \\begin{align*} \u0026 \\min_{\\alpha} \\frac{1}{2} \\sum_{i = 1}^n\\sum_{j = 1}^n\\alpha_i\\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i = 1}^n \\alpha_i \\\\ \u0026\\begin{aligned} \\text{s.t. } \u0026 \\sum_{i = 1}^n \\alpha_i y_i = 0 \\\\ \u0026 0 \\le \\alpha_i \\le C, \\quad i = 1,2,\\dots,n \\end{aligned} \\end{align*} $$此时我们可以在核函数 $K(x, z)$ 对应的高维特征空间 $\\mathcal{H}$ 中寻找一个线性决策超平面，但是参数学习是在低维输入空间 $\\mathcal{X}$ 进行的，这称为支持向量机的核技巧。\n","date":"2025-05-21T12:41:59+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_04_support_vector_machine/","title":"机器学习 04 支持向量机"},{"content":"感知机算法 在 机器学习 02 线性分类器和感知机——感知机中，我们将在 $x-$ 空间寻找分类超平面的问题转换为在 $w-$ 空间寻找一个最优点 $w$ 的问题。\n$x$-space $w$-space hyperplane: $\\{ x : w \\cdot x = 0 \\}$ point: $w$ point: $x$ hyperplane: $\\{ z : w \\cdot z = 0 \\}$ 在 $x-$ 空间的超平面被转换为在 $w-$ 空间的一个点，这个点是超平面的法线 在 $x-$ 空间的样本点被转换为在 $w-$ 空间的超平面，这个超平面的法线为样本点 如果我们需要找到的 $w$ 满足：当 $x$ 为类别 $C$ 时，$x \\cdot w \\ge 0$，那么\n在 $x-$ 空间，存在一个超平面，分离了不同类别的点。所有属于类别 $C$ 的点与 $w$ 在超平面的同侧；所有不属于类别 $C$ 的点与 $w$ 在超平面的不同侧。 在 $w-$ 空间，每一个点 $X_i$ 对应了 $w-$ 空间一个超平面 $H_i$。如果 ${} X_i \\in \\text{class C}$，那么 $w$ 与 $X_i$ 在 $H_i$ 的同侧；反之，在不同侧。 在左图中，$x-$ 空间内寻找一个超平面，可以将蓝色的样本点 $C$ （类别为 $C$ 的点）和红色的样本点 $X$ （类别不为 $C$ 的点）正确分类，$w$ 是这个超平面的法线。并且 $w$ 和样本点 $C$ 处于同侧。 在右图中，每一个样本点在 $w-$ 空间对应一个超平面，$w$ 需要处于红色超平面的反侧，处于蓝色超平面的正侧，即图中阴影区。 现在我们需要在 $w-$ 空间中找出最优的 $w$，使其最小化 $R(w)$ 。\n优化算法：对风险函数 $R(w)$ 梯度下降\n首先初始化 $w$ 的值，然后计算 $R(w)$ 关于 $w$ 的梯度（即 $R(w)$ 增长最快的方向），然后在相反的方向移动一步。\n使用 机器学习 02 线性分类器和感知机——感知机 中定义的风险函数\n$$R(w) = \\frac{1}{n} \\sum_{i = 1}^n L(X_i \\cdot w, y_i) = \\frac{1}{n} \\sum_{i = 1}^n -y_iX_i \\cdot w$$计算其梯度为\n$$ \\nabla R(w) = \\begin{bmatrix} \\frac{\\partial R}{\\partial w_1} \\\\ \\frac{\\partial R}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial R}{\\partial w_d} \\end{bmatrix} = \\nabla \\sum_{i \\in V}-y_i X_i \\cdot w = -\\sum_{i\\in V}y_i X_i $$其中 $V$ 表示所有满足 $y_i X_i \\cdot w \u003c 0$ 的下标 $i$ 的集合。\n对 $R(w)$ 的梯度下降可以表示为\n$$ \\begin{align*} \u0026 w \\leftarrow \\text{任意非零初始化} \\\\ \u0026 \\text{while } R(w) \u003e 0 \\\\ \u0026 \\quad \\quad V \\leftarrow \\text{所有满足 } y_i X_i \\cdot w \u003c 0 \\text{ 的下标 } i \\text{ 的集合} \\\\ \u0026 \\quad \\quad w \\leftarrow w + \\varepsilon \\sum_{i \\in V} y_i X_i \\\\ \u0026 \\text{return } w \\end{align*} $$其中 $\\varepsilon$ 是学习率。\n梯度下降的每一步复杂度为 $O(nd)$，我们可以通过随机梯度下降提升算法性能。\n随机梯度下降的核心思想是在每一步的梯度下降中，随机选择一个满足条件的下标 $i$，并使用损失函数 $L(X_i \\cdot w, y_i)$ 的梯度，该方法被称为感知机算法，其每一步的计算复杂度为 $O(d)$。\n$$ \\begin{align*} \u0026 \\text{while } \\text{找到某一个 } i \\text{ 满足 } y_i X_i \\cdot w \u003c 0 \\\\ \u0026 \\quad \\quad w \\leftarrow x + \\varepsilon y_i X_i \\\\ \u0026 \\text{return } w \\end{align*} $$虽然随机梯度下降法非常高效，但是并不是所有梯度下降法可以解决的问题都可以使用随机梯度下降。 如果数据线性可分，感知机算法一定可以找到全局最优，随机梯度法一定可行。\n如果我们需要找到不经过原点的超平面，此时只需要额外增加一个维度用于表示 $\\alpha$\n$$ f(x) = w \\cdot x + \\alpha = \\begin{bmatrix} w_1 \u0026 w_2 \u0026 \\alpha \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ 1 \\end{bmatrix} $$此时我们的每个样本数据点存在于实数空间 $\\mathbb{R}^{d + 1}$，其中最后一个维度的特征 $x_{d+1} = 1$。仍然可以在 $d + 1$ 维空间中使用感知机算法找到解。\n感知机收敛理论：如果数据是线性可分的，感知机算法可以在最多 $O(r^2/\\gamma^2)$ 的迭代中找到一个可以将所有数据正确分类的分类器。其中 $r=\\max{\\|X_i\\|}$ 是数据的半径，$\\gamma$ 是最大间隔。\n最大边界分类器 一个线性分类器的间隔是超平面到最近的一个训练样本点的距离。如果我们可以找到一个超平面，它的间隔最大，那么这个分类器最优（决策边界离所有的点距离最远，最不容易分类错误）。\n如图，一个满足上述条件的超平面可以表示为 $w \\cdot x + \\alpha = 0$，那么平行于这个超平面的、经过最近的样本点的超平面可以表示为 $|w \\cdot x + \\alpha| = 1$（$1$ 只是为了方便讨论，也可以为 $w \\cdot x + \\alpha = k$，对 $k$ 归一化后也为 $1$）。\n由于所有的样本点都在这两个最近超平面之外，所有样本点满足 $$w \\cdot X_i + \\alpha \\ge 1 \\quad \\text{for } i \\in [1, n]$$ 由于标签 $y_i \\in \\{1, -1\\}$，约束条件可以表示成 $$y_i(w \\cdot X_i + \\alpha) \\ge 1 \\quad \\text{for } i \\in [1, n]$$已知，如果 $\\|w\\| = 1$，那么 $X_i$ 到超平面的有符号距离为 $w \\cdot X_i + \\alpha$。所以我们对 $w$ 归一化，可得 $$\\frac{1}{\\|w\\|}|w \\cdot X_i + \\alpha| \\ge \\frac{1}{\\|w\\|}$$ 即间隔为 $\\frac{1}{\\|w\\|}$，为了使间隔最大化，需要最小化 $\\|w\\|$。\n于是可以求解优化问题\n$$ \\begin{align*} \u0026 \\text{找到 } w, \\alpha \\text{ 最小化 } \\|w\\|^2 \\\\ \u0026 \\text{同时满足 } y_i (X_i \\cdot w + \\alpha) \\ge 1 \\quad \\text{for all } i \\in [1, n] \\end{align*} $$上述问题被称为在 $d + 1$ 维空间中，有 $n$ 个约束的的二次规划（quadratic program）。如果所有样本点线性可分，它有唯一解，反之无解。\n最小化 $\\|w\\|^2$ 而不是 $\\|w\\|$ 的原因：$\\|w\\|$ 在 0 处不平滑。\n上述方法可以得到一个最大间隔分类器，即硬间隔支持向量机（hard-margin support vector machine， SVM）。\n上图是 $(w_1, w_2, \\alpha)$ 的三维空间。类别 $C$ 的样本点为法线的超平面为绿色，非类别 $C$ 的样本点为法线的超平面为红色。图中为 3 个样本点的情况。\n为了满足约束条件 $y_i (X_i \\cdot w + \\alpha) \\ge 1$，$w, \\alpha$ 存在于绿色超平面之上，红色超平面之下 为了满足 $\\|w\\|^2$ 最小，$w, \\alpha$ 应该最接近 $\\alpha$ 轴 和感知机算法类似，只有所有数据点线性可分时，硬边界 SVM 可行。\n","date":"2025-05-19T16:29:23+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/","title":"机器学习 03 感知机 II：感知机学习和最大间隔分类器"},{"content":"分类器 假设我们有 $n$ 个观测样例（bofservations），每个观测样例有 $d$ 个特征（features），有些样例属于类别 C，有些不是\n不同的决策边界 决策边界：分类器选择的可以将样例分成不同类别的边界\n过拟合：决策边界拟合得太好，以至于无法用于未来样本的分类\n决策函数：将一个点映射到一个值，例如\n$$ \\begin{matrix} f(x) \u003e 0 \u0026 \u0026 \\text{if } x \\in \\text{class } C; \\\\ f(x) \\le 0 \u0026 \u0026 \\text{if } x \\notin \\text{class } C; \\end{matrix} $$则分类边界为 $\\{x\\in \\mathbb{R}^{d}: f(x)=0\\}$\n分类边界 Linear Classifier Math 内积：$x\\cdot y = x_1y_1+x_2y_2+\\dots + x_dy_d$，也可以写成 $x^Ty$\n欧拉范数：$\\|x\\|=\\sqrt{x\\cdot x}=\\sqrt{x_1^2 + x_2^2 + \\dots + x_d^2}$。可以表示向量 $x$ 的欧拉长度 (Euclidean length)。\n归一化向量 $x$： ${x}/{\\|x\\|}$\n给定一个线性决策函数 $f(x) = w \\cdot x + \\alpha$，决策边界为 $H = \\{x: w \\cdot x = -\\alpha\\}$。集合 $H$ 被称为超平面（hyperplane）。\n超平面是 $d-1$ 维，并且分割了一个 $d$ 维空间 它是无限且平展的 $w$ 和超平面 $H$ 上的任何直线正交。所以 $w$ 被称为 $H$ 的法线（normal vector）。如果 $w$ 是一个单位向量，那么 $f(x) = w \\cdot x + \\alpha$ 是 $x$ 到 $H$ 的有符号距离（singed distance）。此外，$H$ 到原点的距离为 $\\alpha$。\n平面的法线 如果存在一个超平面可以将所有的训练样本点分类，则它们线性可分。\n一个简单的分类器 质心法（Centroid method）：首先分别计算属于类别 $C$ 的所有点的平均 $\\mu_C$，以及所有不属于 $C$ 的平均 $\\mu_X$。则决策函数可以表示为 $$f(x) = (\\mu_C - \\mu_X) \\cdot x - (\\mu_C - \\mu_X) \\cdot \\frac{\\mu_C + \\mu_X}{2}$$质心法非常简洁，但是也只能处理简单的情况。例如下图中的数据点无法使用质心法正确分类。\n质心法失效的情况 感知机 假设有 $n$ 个采样点数据 $X_1, X_2,\\dots,X_n$。对于每个采样点，对应的标签为\n$$ y_i = \\begin{cases} \\begin{matrix} 1 \u0026 \\text{if } X_i \\in \\text{class C} \\\\ -1 \u0026 \\text{if } X_i \\notin \\text{class C} \\end{matrix} \\end{cases} $$为了简化讨论，这里只考虑经过原点的决策边界。于是我们寻找决策边界的目标可以转换成寻找一个权重 $w$，使其满足\n$$ \\begin{matrix} X_i \\cdot w \\ge 0 \u0026 \u0026 \\text{if } y_i = 1 \\\\ X_i \\cdot w \\le 0 \u0026 \u0026 \\text{if } y_i = -1 \\end{matrix} $$等价为 $$y_iX_i\\cdot w \\ge 0$$ 上式即为约束（constraint）。\n为了找到最优的 $w$，我们定义一个风险函数$R$，如果约束被违背，则风险函数为正值，然后通过最小化 $R$ 选择最优 $w$。\n首先定义损失函数\n$$ L(\\hat{y}, y_i) = \\begin{cases} \\begin{matrix} 0 \u0026 \u0026 \\text{if } y_i\\hat{y} \\ge 0 \\\\ -y_i \\hat{y} \u0026 \u0026 \\text{otherwise} \\end{matrix} \\end{cases} $$其中 $\\hat{y}$ 是分类器的预测值。\n于是，我们可以定义风险函数（即优化问题的目标函数）为 $$R(w) = \\frac{1}{n} \\sum_{i = 1}^n L(X_i \\cdot w, y_i) = \\frac{1}{n} \\sum_{i = 1}^n -y_iX_i \\cdot w$$ 所以寻找超平面的问题变为一个最优化问题：寻找 $w$ ，使得 $R(w)$ 最小。\n","date":"2025-05-18T16:32:02+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/","title":"机器学习 02 感知机 I：线性分类器和感知机"},{"content":"机器学习系列是我学习 CS 189/289A Introduction to Machine Learning Spring 2025 时记录的学习笔记，也可以当作是对课程笔记的翻译，经供参考。\n分类问题 收集具有标签的训练样本 预测新样本的类别 两种不同分类器的决策边界 训练、验证和测试 我们拥有一些设置好标签的数据 从数据中取出一部分作为验证集（通常 20），其余作为训练集 训练一个或者多个分类器：训练多种不同算法，或者具有不同超参数的同一算法 使用验证集选出具有最低验证误差的分类器 使用新的数据测试算法的效果 训练误差：训练集中分类错误的比例\n验证误差：验证集中分类错误的比例\n测试误差：测试集中分类错误的比例\n大多数机器学习算法的一些超参数改变会造成欠拟合或者过拟合\n[k邻接算法中超参数k的改变带来的影响] 过拟合：当验证/测试误差恶化时，原因是分类器对异常值或其他虚假模式变得过于敏感。 欠拟合：当验证/测试误差恶化时，原因是分类器不够灵活，无法拟合模式。 异常值：具有非典型标签的点（例如，富裕的借款人仍然违约）。增加过拟合的风险。 ","date":"2025-05-18T15:40:22+08:00","image":"https://zhiim.github.io/p/ml_01_introduction/ml_header.webp","permalink":"https://zhiim.github.io/p/ml_01_introduction/","title":"机器学习 01 引言"},{"content":"项目地址 DOA_py: DOA etimation algorithms implemented in Python for ULA, UCA and broadband/wideband DOA estimation\n安装 使用 pip 或者从源码安装\n1 2 3 4 5 6 # pip 安装 pip install doa_py # 从源码安装 git clone https://github.com/zhiim/doa_py.git cd doa_py pip install . 使用案例（均匀线阵） 1 import numpy as np Import Signal 和 Array signal.py定义了一些常用的信号类型，信号对应的 Class 都继承自Signal基类, 用于产生多个入射信号\narray.py定义了一些常用的阵列结构，所有的阵列对应的 Class 都继承自Array基类，Array会使用Signal产生的信号以及入射角度，根据阵列接受信号的数学模型生成阵列接受信号的仿真数据\n阵列接收信号的数学模型（窄带信号）: $\\bf{X} = \\bf{A} \\bf{S}$\n其中： $\\bf{X} = \\begin{bmatrix} x_1(t) \u0026 x_2(t) \u0026 \\dots \u0026 x_M(t) \\end{bmatrix}^T$ 是一个$M\\times 1$维的向量，表示阵列接收到的信号\n$\\bf{A} = \\begin{bmatrix} \\bf{a}(\\theta_1) \u0026 \\bf{a}(\\theta_2) \u0026 \\dots \u0026 \\bf{a}(\\theta_N) \\end{bmatrix}$ 是一个$M \\times N$维的矩阵，表示阵列的流型矩阵\n$\\bf{a}(\\theta_n) =\\begin{bmatrix} e^{-j \\omega_0 \\tau_{1n}} \u0026 e^{-j \\omega_0 \\tau_{2n}} \\dots \u0026 e^{-j \\omega_0 \\tau_{Mn}} \\end{bmatrix}^T$是与入射角度对应的导向矢量\n$\\bf{X} = \\begin{bmatrix} s_1(t) \u0026 s_2(t) \u0026 \\dots \u0026 s_N(t) \\end{bmatrix}^T$ 是一个$N\\times 1$维的向量，表示入射的信号\n1 2 from doa_py.arrays import UniformLinearArray from doa_py.signals import ComplexStochasticSignal 设定仿真参数 设定与阵列结构以及信号参数、入射方向有关的仿真参数，产生采样后的阵列接受信号\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 信号参数 num_snapshots = 300 signal_fre = 2e7 fc = 5e7 snr = 0 # 阵列参数 num_antennas = 8 antenna_spacing = 0.5 * (3e8 / signal_fre) # 阵元间距半波长 # 入射角度 angle_incidence = np.array([0, 30, -60]) num_signal = len(angle_incidence) 生成仿真信号 创建Array和Signal的实例，并生成阵列接受信号的仿真数据\n1 2 3 4 5 6 7 8 9 10 # 创建信号实例 signal = ComplexStochasticSignal(fc=fc) # 创建阵列实例 array = UniformLinearArray(m=num_antennas, dd=antenna_spacing) # 使用信号和阵列生成仿真数据 received_data = array.received_signal( signal=signal, snr=snr, nsamples=1000, angle_incidence=angle_incidence, unit=\u0026#34;deg\u0026#34; ) 估计入射角 algorithm中定义了几种经典的 DOA 估计算法，有些算法会输出空间谱，有些算法会直接输出估计的入射角的值\n首先 import 两个和绘图函数，分别用于展示这两种算法的估计结果\n1 from doa_py.plot import plot_estimated_value, plot_spatial_spectrum MUSIC 算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 search_grids = np.arange(-90, 90, 1) from doa_py.algorithm.music import music music_spectrum = music( received_data=received_data, num_signal=num_signal, array=array, signal_fre=signal_fre, angle_grids=search_grids, unit=\u0026#34;deg\u0026#34;, ) # 绘制空间谱 plot_spatial_spectrum( spectrum=music_spectrum, angle_grids=search_grids, ground_truth=angle_incidence, num_signal=num_signal, ) Root-MUSIC 算法 1 2 3 4 5 6 7 8 9 10 from doa_py.algorithm.music import root_music rmusic_estimates = root_music( received_data=received_data, num_signal=num_signal, array=array, signal_fre=signal_fre, unit=\u0026#34;deg\u0026#34;, ) plot_estimated_value(estimates=rmusic_estimates, ground_truth=angle_incidence) ESPRIT 算法 1 2 3 4 5 6 7 8 9 10 from doa_py.algorithm.esprit import esprit esprit_estimates = esprit( received_data=received_data, num_signal=num_signal, array=array, signal_fre=signal_fre, ) plot_estimated_value(estimates=esprit_estimates, ground_truth=angle_incidence) OMP 算法 1 2 3 4 5 6 7 8 9 10 11 12 from doa_py.algorithm.sparse import omp omp_estimates = omp( received_data=received_data, num_signal=num_signal, array=array, signal_fre=signal_fre, angle_grids=search_grids, unit=\u0026#34;deg\u0026#34;, ) plot_estimated_value(estimates=omp_estimates, ground_truth=angle_incidence) 更多 查看项目仓库，了解更多的功能和使用方法\n","date":"2024-07-07T10:59:51+08:00","permalink":"https://zhiim.github.io/p/python_package_doa_estimation/","title":"用于 DOA 估计的 Python 包 DOA_py"},{"content":"最近Github向GitHub Student Developer Pack添加了无限使用Github Copilot的权益，大大提升了学生包的价值，学生认证后即可免费体验这款强大的AI编程助手。Github Copilot的实际体验让人不得不感慨AI的强大，Copilot自动补全的代码大多数情况下可以直接使用，大大提升了编程效率；Copilot Chat也可以对各种编程问题提供非常有用的建议。\n除此之外，由于Github Copilot的底层使用了GPT，我们也可以通过Copilot免费使用GPT-4。要实现这样的功能只需要使用开源项目copilot-gpt4-service获取GPT API，然后就可以通过GPT API搭配GPT前端来访问GPT-4，此外还可以使用GPT API部署AI学术助手gpt academic。\ncopilot-gpt4-service：通过Github Copilot Plugin Token调用GPT API。 chatbox：GPT前端，搭配GPT API可以搭建自己的ChatGPT，并和GPT-4对话。 gpt_academic：使用GPT API的AI学术助手，支持英文论文润色，一键翻译论文等功能。 获取GPT API 从Copilot获取GPT API只需要两步\n运行copilot-gpt4-service得到一个API URL用来替代OpenAI的API URL 获取Github Copilot Plugin Token代替OpenAI的API Key 编译可执行文件 copilot-gpt4-service使用Go语言编写，官方未提供编译好的可执行文件，所以只能自己编译，需要下载并安装Go语言编译器，然后将源码下载到本地编译。\n将copilot-gpt4-service克隆到本地\n1 git clone https://github.com/aaamoon/copilot-gpt4-service.git 编译源码，得到一个名为main的可执行文件\n1 2 3 4 # 进入项目目录 cd copilot-gpt4-service # 编译 go build main.go 在终端中执行可执行文件，此时程序会打印出程序运行的端口以及可用的API地址\n1 ./main 选择本地IP对应的API的地址http://127.0.0.1:8080，在浏览器中打开，如果出现下方提示表示程序运行正常\nVery important: please do not make this service public, for personal use only, otherwise the account or Copilot will be banned. 非常重要：请不要将此服务公开，仅供个人使用，否则账户或 Copilot 将被封禁。\n获取Github Copilot Plugin Token 打开新的终端（如果关闭原本终端，正在运行的copilot-gpt4-service也会被关闭），运行Python脚本获取Github Copilot Plugin Token。\n1 2 3 4 # 安装依赖库 pip install -r requests # 运行脚本 python shells/get_copilot_token.py 根据终端输出的提示，在浏览器中打开Github验证页面，输入终端打印的认证码获取Github Copilot Plugin Token。\n使用GPT前端 Chatbox是一款跨平台的GPT前端，可以使用GPT API搭建自己的ChatGPT，下载安装并填入GPT API的URL和Github Copilot Plugin Token即可使用。\n下载并打开chatbox，进行如下设置：\nAI模型提供方选择OpenAI API OpenAI API 密钥填入上一步获取的Github Copilot Plugin Token API 域名填入http://127.0.0.1:8080 模型选择gpt-4或者GPT的其他任意版本 为了验证API返回的结构是否来自GPT-4，可以提问鲁迅为什么暴打周树人，GPT-3.5会胡说八道，GPT-4会给出正确答案。\n搭建GPT Academic 将项目克隆到本地，并进入项目目录\n1 2 git clone https://github.com/binary-husky/gpt_academic.git cd gpt_academic 安装依赖库\n1 pip install -r requirements.txt 修改config.py中的下面几项\n1 2 3 4 5 6 7 API_KEY = \u0026#34;ghu_xxxxx\u0026#34; # Github Copilot Plugin Token API_URL_REDIRECT = {\u0026#34;https://api.openai.com/v1/chat/completions\u0026#34;: \u0026#34;http://127.0.0.1:8080/v1/chat/completions\u0026#34;} CUSTOM_API_KEY_PATTERN = \u0026#34;ghu_[a-zA-Z0-9]{36}\u0026#34; WEB_PORT = 1234 # 固定端口，也可以为其他值 运行程序，程序会自动在浏览器打开GPT Academic的页面，同样可以通过提问验证API配置是否正常\n1 python main.py ","date":"2024-02-12T15:19:19+08:00","permalink":"https://zhiim.github.io/p/get-access-to-gpt4-with-github-copilot/","title":"通过Github Copilot获取GPT-4 API"},{"content":"环境准备 本文部署的模型是使用TensorFlow2训练的ResNet网络，用于干扰信号识别，原本的模型文件将权重保存在ResNet.h5中。\n从模型文件中读取权重，并使用测试数据进行推理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import ResNet as ResNet import numpy as np # 干扰类型 class_names = [\u0026#39;CSNJ\u0026#39;, \u0026#39;CW\u0026#39;, \u0026#39;LFM\u0026#39;, \u0026#39;MTJ\u0026#39;, \u0026#39;PBNJ\u0026#39;, \u0026#39;PPNJ\u0026#39;] # 权重文件的存储路径 model_path = \u0026#34;ResNet.h5\u0026#34; data_dimension = (256, 256, 1) model = ResNet.ResNet50(data_dimension) # 导入网络模型 model.load_weights(model_path) # 读取测试数据 test_img = np.load(\u0026#39;test_img.npy\u0026#39;) # 转换输入数据的维度 test_img = np.array(test_img) # (256, 256) test_img = np.expand_dims(test_img, axis=2) # (256, 256, 1) test_img = np.expand_dims(test_img, 0) # (1, 256, 256, 1) test_output = model.predict(test_img) # 模型输出的干扰类型 pred = class_names[test_output.argmax()] print(pred) Atlas 200 DK的环境部署采用“开发环境与运行环境分设”的方案，开发环境使用Windows下Linux子系统中的Ubuntu 22.04.3 LTS，CANN版本为6.0.RC1.alpha005。\n参考官方教程开发环境与运行环境分设完成环境配置。\ncann-toolkit 配置 在Ubuntu中安装完Ascend-cann-toolkit后，运行模型转换工具atc时可能会出现报错\n1 error while loading shared libraries: libascend_hal.so: cannot open shared object file: No such file or directory 这是由于无法找到库文件libascend_hal.so，此时可以进入Ascend-cann-toolkit的安装路径，寻找libascend_hal.so\n1 2 3 xu@DESKTOP-9B4N33I:~$ cd Ascend/ascend-toolkit/latest/ xu@DESKTOP-9B4N33I:~/Ascend/ascend-toolkit/latest$ find . -name libascend_hal.so ./x86_64-linux/devlib/libascend_hal.so 将libascend_hal.so复制到任意路径，并将该路径添加到Ascend-cann-toolkit环境变量中的LD_LIBRARY_PATH。\n例如将libascend_hal.so复制到~/Ascend/missing_lib\n1 2 xu@DESKTOP-9B4N33I:~$ mkdir ~/Ascend/missing_lib xu@DESKTOP-9B4N33I:~$ cp ~/Ascend/ascend-toolkit/latestx86_64-linux/devlib/libascend_hal.so ~/Ascend/missing_lib 在~/.bashrc中更改环境变量\n1 export LD_LIBRARY_PATH=${ASCEND_TOOLKIT_HOME}/lib64:${ASCEND_TOOLKIT_HOME}/lib64/plugin/opskernel:${ASCEND_TOOLKIT_HOME}/lib64/plugin/nnengine:/home/xu/Ascend/missing_lib:$LD_LIBRARY_PATH 在终端输入source ~/.bashrc并重启终端，此时atc可以正常运行。\n模型转换 由于TensorFlow2不再支持导出模型为FrozenGraphDef格式，而atc转换TensorFlow模型时只能使用FrozenGraphDef格式，所以本文采用的转换流程是，先将TensorFlow模型导出为SavedModel格式，再将模型转换为ONNX格式，最后使用atc将ONNX模型转换为.om格式。\n将TensorFlow模型导出为SavedModel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import tensorflow as tf import ResNet as ResNet import os model_path = \u0026#39;ResNet.h5\u0026#39; # 权重文件路径 data_dimension = (256, 256, 1) model = ResNet.ResNet50(data_dimension) # 读取网络模型 model.load_weights(model_path) # export model to savedmodel mobilenet_save_path = os.path.join(\u0026#34;./model\u0026#34;) tf.saved_model.save(model, mobilenet_save_path) 将SavedModel转换为ONNX模型 使用tf2onnx将SavedModel转换为.onnx格式的模型。\n1 python -m tf2onnx.convert --saved-model tensorflow-model-path --output model.onnx 由于atc不支持ONNX的高版本算子，转换时tf2onnx的\u0026ndash;opset 参数值需使用默认值15。\n导出的ONNX模型可以使用Netron查看网络结构。使用ONNX模型完成推理以验证模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import onnxruntime as onnxrt import numpy as np class_names = [\u0026#39;CSNJ\u0026#39;, \u0026#39;CW\u0026#39;, \u0026#39;LFM\u0026#39;, \u0026#39;MTJ\u0026#39;, \u0026#39;PBNJ\u0026#39;, \u0026#39;PPNJ\u0026#39;] # load model model = onnxrt.InferenceSession(\u0026#34;resnet.onnx\u0026#34;, providers=[\u0026#39;AzureExecutionProvider\u0026#39;, \u0026#39;CPUExecutionProvider\u0026#39;]) print(\u0026#34;loading model success!\u0026#34;) # input data test_img = np.load(\u0026#39;test_img.npy\u0026#39;) test_img = np.array(test_img, dtype=np.float32) # (256, 256) test_img = np.expand_dims(test_img, axis=2) # (256, 256, 1) test_img = np.expand_dims(test_img, 0) # (1, 256, 256, 1) # input sf_input = {model.get_inputs()[0].name:test_img} # output output = model.run(None, sf_input) print(\u0026#34;get output of spatial filter success!\u0026#34;) output = np.array(output) pred = class_names[output.argmax()] print(pred) 将ONNX模型转换为om格式 domain_version报错 在Ubuntu终端中运行atc模型转换工具\n1 atc --model=resnet.onnx --framework=5 --output=resnet --soc_version=Ascend310 可能会出现报错\n1 E16005: The model has [2] [--domain_version] fields, but only one is allowed. 这是由于在将模型转换为ONNX是产生了两个(domain, version)（domain_version的概念参考ONNX Concepts）。\n获取ONNX模型中的(domain,version)\n1 2 3 4 import onnx model = onnx.load(\u0026#34;resnet.onnx\u0026#34;) print(model.opset_import) 得到输出\n1 2 3 4 5 [domain: \u0026#34;\u0026#34; version: 15 , domain: \u0026#34;ai.onnx.ml\u0026#34; version: 2 ] 此时只需去除多余的(domain,version)，保留一个即可\n1 2 3 4 5 6 7 8 9 10 11 import onnx model = onnx.load(\u0026#34;resnet.onnx\u0026#34;) # atc 模型转换只支持一个domain_version if len(model.opset_import) \u0026gt; 1: # 删除多余的domain_version model.opset_import.pop() # 将删除domain_version后的模型保存 onnx.save(model, \u0026#34;./resnet.onnx\u0026#34;) 模型输入维度报错 再次运行atc，出现报错\n1 E10001: Value [-1] for parameter [input_2] is invalid. Reason: maybe you should set input_shape to specify its shape. 使用Netron查看网络结构，发现input_2是输入节点，Netron中显示的该节点信息为\n1 2 name: input_2 tensor: float32[unk__618,256,256,1] 输入数据为四维张量，每一个维度分别表示N（数量）H（高）W（宽）C（通道数），例如前面模型推理时使用的数据维度为(1, 256, 256, 1)表示1个高和宽都为256，通道数为1的图像。\n在导出的ONNX模型中维度N的数值为指定，需要在模型转换时使用--input-shape参数指定输入节点的维度\n1 atc --model=resnet.onnx --framework=5 --output=resnet --input-shape=\u0026#34;input_2:1,256,256,1\u0026#34; --soc_version=Ascend310 在Atlas 200 DK中进行模型推理 模型推理时使用的是pyACL接口，并使用了第三库acllite。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import numpy as np import sys import os # 将acllite路径添加到python环境变量 path = os.path.dirname(os.path.abspath(__file__)) sys.path.append(os.path.join(path, \u0026#34;..\u0026#34;)) sys.path.append(os.path.join(path, \u0026#34;../../common\u0026#34;)) sys.path.append(os.path.join(path, \u0026#34;../../common/acllite\u0026#34;)) from acllite_model import AclLiteModel from acllite_resource import AclLiteResource MODEL_PATH = \u0026#34;../model/resnet.om\u0026#34; # init acl_resource = AclLiteResource() acl_resource.init() # load model model = AclLiteModel(model_path=MODEL_PATH) test_img = np.load(\u0026#34;../data/test_img.npy\u0026#34;) test_img = np.array(test_img) # (256, 256) test_img = np.expand_dims(test_img, axis=2) # (256, 256, 1) test_img = np.expand_dims(test_img, 0) # (1, 256, 256, 1) test_img = test_img.astype(np.float32) # get output of model output = model.execute(test_img) output = np.squeeze(output) print(output) class_names = [\u0026#39;CSNJ\u0026#39;, \u0026#39;CW\u0026#39;, \u0026#39;LFM\u0026#39;, \u0026#39;MTJ\u0026#39;, \u0026#39;PBNJ\u0026#39;, \u0026#39;PPNJ\u0026#39;] pred = class_names[output.argmax()] print(pred) ","date":"2023-09-26T12:08:08+08:00","permalink":"https://zhiim.github.io/p/deploy-deep-learning-model-on-atlas/","title":"在Atlas 200 DK中部署深度学习模型"},{"content":"Btrfs 更据官方文档的说明\nBTRFS is a modern copy on write (COW) filesystem for Linux aimed at implementing advanced features while also focusing on fault tolerance, repair and easy administration.\nBtrfs拥有较多传统文件系统如Ext3/4不具有的高级功能，为我们日常文件系统使用和备份带来了便利。\n写时复制（Copy-on-Write，CoW） Btrfs默认采用CoW机制，将文件指针存放在metadata文件中。在对文件进行更改时，Btrfs会将新的数据写在新的空闲数据块中，数据写入完成后，更新metadata增加将文件指向新的内存块的指针。\n子卷（subvolume） 一个子卷可以看作是Btrfs中的一个文件夹，但是子卷也可以像分区一样被挂载。例如在subvolume-test中创建一个子卷subvolume，就相当于创建了一个对应的文件夹。\n1 2 3 4 5 6 7 8 ❯ mkdir subvolume-test \u0026amp;\u0026amp; cd subvolume-test ❯ sudo btrfs subvolume create subvolume # 创建子卷 Create subvolume \u0026#39;./subvolume\u0026#39; ❯ ls -l 总计 0 drwxr-xr-x 1 root root 0 7月10日 17:19 subvolume # 当前目录出现对应的文件夹 ❯ sudo btrfs subvolume list -o . # 查看当前目录下存在的子卷 ID 263 gen 6134 top level 257 path @home/xu/subvolume-test/subvolume 如果把子卷挂载到某一目录中，那么该目录内的内容就和该子卷相同了。\n1 2 3 4 5 6 7 8 9 10 ❯ mkdir mount-test # 创建挂载文件夹 ❯ sudo mount -o subvol=@home/xu/subvolume-test/subvolume /dev/nvme0n1p6 mount-test ❯ sudo touch subvolume/test # 在子卷内新建文件 ❯ tree . . ├── mount-test │ └── test # 挂载文件夹内容和子卷相同 └── subvolume └── test 3 directories, 2 files Btrfs文件系统中默认会存在一个ID为5的根子卷，用户创建的子卷都是根子卷下的嵌套子卷。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ❯ mkdir rootsub ❯ sudo mount -o subvolid=5 /dev/nvme0n1p6 rootsub # 挂载根子卷 ❯ ls rootsub @ @home # 根子卷下包含的子卷 ❯ df 文件系统 1K的块 已用 可用 已用% 挂载点 dev 3929540 0 3929540 0% /dev run 3941308 1480 3939828 1% /run /dev/nvme0n1p6 251133952 38761288 210874440 16% / tmpfs 3941308 157696 3783612 5% /dev/shm /dev/nvme0n1p6 251133952 38761288 210874440 16% /home tmpfs 3941312 19428 3921884 1% /tmp /dev/nvme0n1p8 523248 6172 517076 2% /efi tmpfs 788260 76 788184 1% /run/user/1000 onedriver 1131413504 200908800 930504704 18% /home/xu/OneDrive /dev/nvme0n1p6 251133952 38761288 210874440 16% /home/xu/rootsub 可以看到我的根子卷下创建了@子卷和@home子卷分别挂载在/和/home文件夹下。每一个子卷都可以达到文件系统的总容量，使用子卷挂载/和/home就免除了某一个分区空间不足的烦恼。\n快照（snapshot） 快照可以看作是一个子卷，对一个子卷创建快照就是创建了一个新的子卷，这个新的子卷内容和原本子卷完全相同，但是快照不会包含子卷内的嵌套子卷。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ❯ mkdir snapshot-test \u0026amp;\u0026amp; cd snapshot-test ❯ sudo btrfs subvolume create demo # 创建子卷demo Create subvolume \u0026#39;./demo\u0026#39; ❯ sudo touch demo/a demo/b ❯ sudo btrfs subvolume snapshot demo demo-snapshot # 创建子卷的快照 Create a snapshot of \u0026#39;demo\u0026#39; in \u0026#39;./demo-snapshot\u0026#39; ❯ tree . ├── demo │ ├── a │ └── b └── demo-snapshot # 快照和子卷具有相同的内容 ├── a └── b 3 directories, 4 files 得益于CoW的特性，Btrfs创建快照时并不会将原子卷中的内容复制到快照子卷中，而是在快照中创建了指向原子卷内容的指针。如果对子卷内的文件进行更改，原本内容在数据块中不会改变，只是增加了指向新数据块的指针。\n使用快照备份系统 得益于Btrfs的快照特性，我们可以快速方便地备份和回滚系统，而且系统快照只会占用很小的储存空间，在备份方案上，我选择使用Snapper创建和管理系统快照。\nSnapper自动备份 首先安装Snapper\n1 sudo pacman -S snapper 为挂载在/目录下的子卷创建命名为root的快照配置文件\n1 sudo snapper -c root create-config / 此时Snapper会在/etc/snapper/configs生成配置文件，并且在根目录/挂载的子卷即@下创建.snapshots子卷，所有快照都会存储在.snapshots子卷下\n1 2 3 4 5 6 ❯ sudo btrfs subvolume list / ID 256 gen 6803 top level 5 path @ ID 257 gen 6803 top level 5 path @home ID 258 gen 17 top level 256 path var/lib/portables ID 259 gen 18 top level 256 path var/lib/machines ID 266 gen 6800 top level 256 path .snapshots # 挂载在@子卷下 Snapper可以自动创建和清除快照，默认会每小时创建一个快照，每天清理一次快照，在清理时会保存10个每小时快照、10个每日快照、10个每月快照、10个每年快照，但是对于日常使用更改较频繁的子卷我们不需要这么频繁的快照。\n修改/lib/systemd/system/snapper-timeline.timer，设置每6小时创建一次快照\n1 2 3 4 5 6 7 8 9 [Unit] Description=Timeline of Snapper Snapshots Documentation=man:snapper(8) man:snapper-configs(5) [Timer] OnCalendar=00/3:00 # 每3小时创建一次快照 [Install] WantedBy=timers.target 修改配置文件/etc/snapper/configs/root中的变量，改变清理是保留的快照数量\n1 2 3 4 5 6 TIMELINE_MIN_AGE=\u0026#34;1800\u0026#34; TIMELINE_LIMIT_HOURLY=\u0026#34;3\u0026#34; # 保留3个小时快照 TIMELINE_LIMIT_DAILY=\u0026#34;7\u0026#34; # 保留7个每日快照 TIMELINE_LIMIT_WEEKLY=\u0026#34;0\u0026#34; TIMELINE_LIMIT_MONTHLY=\u0026#34;0\u0026#34; TIMELINE_LIMIT_YEARLY=\u0026#34;0\u0026#34; 开启Snapper自动快照和自动清理\n1 2 sudo systemctl enable --now snapper-timeline.timer sudo systemctl enable --now snapper-cleanup.timer 改变快照存储位置 Snapper默认把快照存储在@子卷内的.snapshots分卷下，如果@子卷损坏.snapshots也可能受影响，所以更好的子卷结构是把快照存储在@snapshots子卷下，然后把@snapshots挂载在根子卷下，这样@和@snapshots互不影响。\n删除默认的.snapshots子卷\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ❯ sudo btrfs subvolume list / ID 256 gen 7144 top level 5 path @ ID 257 gen 7144 top level 5 path @home ID 258 gen 17 top level 256 path var/lib/portables ID 259 gen 18 top level 256 path var/lib/machines ID 266 gen 7115 top level 256 path .snapshots ID 267 gen 6963 top level 266 path .snapshots/1/snapshot ID 272 gen 6995 top level 266 path .snapshots/2/snapshot # 如果已经有快照，需要先删除.snapshots里面的快照子卷 ❯ sudo btrfs subvolume delete .snapshots/1/snapshot Delete subvolume (no-commit): \u0026#39;/.snapshots/1/snapshot\u0026#39; ❯ sudo btrfs subvolume delete .snapshots/2/snapshot Delete subvolume (no-commit): \u0026#39;/.snapshots/2/snapshot\u0026#39; ❯ sudo btrfs subvolume delete .snapshots Delete subvolume (no-commit): \u0026#39;//.snapshots\u0026#39; 在根子卷下并列@子卷创建一个@snapshots子卷\n1 2 sudo mount -o subvolid=5 /dev/nvme0n1p6 ~/rootsub # 挂载根子卷 sudo btrfs subvolume create ~/rootsub/@snapshots # 在根子卷下创建子卷 编辑/etc/fstab，将@subvolume子卷挂载到./snapshots\n1 2 # 仿照@和@home子卷的挂载更改 UUID=d26ed334-68d4-481d-894f-838783fa4f88 /.snapshots btrfs rw,relatime,compress=zstd:3,ssd,discard=async,space_cache=v2,subvolid=275,subvol=/@snapshots 0 0 挂载@snapshots子卷\n1 2 3 sudo mkdir /.snapshots sudo mount -a sudo chmod 750 /.snapshots # 阻止非root用户访问 Btrfs的快照不会备份其他子卷的内容，所以我们也可以把/var/cache/pacman/pkg、/var/abs、/var/tmp和/srv等经常读写但有没必要备份的文件夹挂载成子卷。\n从快照中恢复 假设我们已经为/创建了快照，如果想要从快照中恢复系统，可以先进入Arch Linux live USB，然后挂载Btrfs分区\n1 2 sudo mount /dev/nvme0n1p6 /mnt cd /mnt 移除旧的@子卷\n1 sudo mv /mnt/@ /mnt/@.broken 列出所有快照和对应的时间信息\n1 2 3 $ sudo grep -r \u0026#39;\u0026lt;date\u0026gt;\u0026#39; /mnt/@snapshots/*/info.xml /.snapshots/1/info.xml: \u0026lt;date\u0026gt;2023-07-11 06:21:53\u0026lt;/date\u0026gt; # Snapper使用UTC时间记录快照创建时间 /.snapshots/2/info.xml: \u0026lt;date\u0026gt;2022-07-11 06:22:39\u0026lt;/date\u0026gt; 由于Snapper创建的快照是只读子卷，所以需要从快照中创建可读写快照作为@子卷\n1 sudo btrfs subvolume snapshot /mnt/@snapshots/{number}/snapshot /mnt/@ 然后就可以删除旧的@子卷\n1 sudo btrfs subvolume delete /mnt/@.broken Reference ArchWiki-Btrfs Working with Btrfs – General Concepts Working with Btrfs – Subvolumes Working with Btrfs – Snapshots ArchWiki-Snapper BTRFS snapshots and system rollbacks on Arch Linux ","date":"2023-07-11T14:54:53+08:00","image":"https://zhiim.github.io/p/arch-linux-configuration-driver-and-software/arch.webp","permalink":"https://zhiim.github.io/p/arch-linux-configuration-btrfs-and-system-backup/","title":"Arch Linux 配置 — Btrfs与系统备份"},{"content":"由于滚动更新的机制，Arch Linux采用的linux内核更新较频繁，难免会碰到由于内核更新无法进入系统。为了解决由于内核更新导致的系统故障，我们可以同时安装更新周期更长的linux-lts内核，如果linux内核更新后无法进入系统，此时就可以选择加载linux-lts内核进入系统，回退linux内核或者等待修复。\n安装linux-lts内核 首先安装linux-lts内核\n1 sudo pacman linux-lts linux-lts-headers 此时重启如果使用GRUB引导系统的话，会出现“Advanced Options for Arch Linxu”，选择这一项就会进入选择启动linux内核或者linux-lts内核的自选单。\n如果使用rEFInd引导系统的话，此时选择进入Arch Linux会默认加载最后安装的内核即linux-lts，选中Arch Linux图标在按F2键的话就会进入选择加载不同内核的选单。\n为了更加方便地在Boot Loader界面选择想要加载的内核，我们可以分别对GRUB和rEFInd进行一些配置。\n配置GRUB 我们想要的是在进入GRUB界面后，直接列出了加载不同内核的选项，而不需要先进入子选单再选择内核。编辑/etc/default/grub，取消注释\n1 GRUB_DISABLE_SUBMENU=y 如果使用的文件系统不是btrfs，还可以配置GRUB自动选中上一次选择加载的选项，这样可以免除每次重新选择。同样编辑/etc/default/grub，对下面几项进行更改\n1 2 GRUB_DEFAULT=saved GRUB_SAVEDEFAULT=true 修改完成后重新更新引导配置即可\n1 sudo grub-mkconfig -o /boot/grub/grub.cfg 配置rEFInd rEFInd每次会默认加载最后安装的内核，所以如果不按F2进入子选单选择linux内核就会自动加载linux-lts内核。\n通过更改rEFInd配置文件，我们可以实现默认加载进入linux内核，然后通过子选单选择加载linux-lts内核。同时我们也可以选择附加内核参数，使得每次加载内核的同时加载微码，防止出现一些意外奔溃。\n为了让rEFInd支持Btrfs子卷，需要为rEFInd安装驱动\n1 2 # 我的ESP分区挂载在/efi下 sudo cp /usr/share/refind/drivers_x64/btrfs_x64.efi /efi/EFI/refind/drivers_x64/btrfs_x64.efi 编辑rEFInd配置文件/efi/EFI/refind/refind.conf，我的ESP分区挂载在/efi，如果ESP分区挂载在/boot下的话配置文件路径为/boot/efi/EFI/refind/refind.conf。\n让rEFInd能够匹配Arch Linux的不同内核\n1 extra_kernel_version_strings linux-hardened,linux-zen,linux-lts,linux 让rEFInd能够在Btrfs子卷下自动扫描内核\n1 also_scan_dirs boot,ESP2:EFI/linux/kernels,@/boot 为了实现rEFInd默认加载linux内核，修改配置文件/efi/EFI/refind/refind.conf，手动添加启动项\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 我使用的是Btrfs文件系统，请更加自己的文件系统和内核位置更改路径选项 menuentry \u0026#34;Arch Linux\u0026#34; { icon /EFI/refind/themes/refind-theme-regular/icons/128-48/os_arch.png volume \u0026#34;arch\u0026#34; # Arch 文件系统所在卷对应的标签（Label） loader /@/boot/vmlinuz-linux # 默认加载linux内核 initrd /@/boot/initramfs-linux.img # linux内核对应的initramfs # options选项需要更据GRUB配置文件中的内核参数更改 # options的最后一项设定加载Intel微码，需要确定微码的路径 options \u0026#34;root=UUID=d26ed334-68d4-481d-894f-838783fa4f88 rw rootflags=subvol=@ loglevel=5 nowatchdog initrd=@\\boot\\intel-ucode.img\u0026#34; # 子选单定义，用于加载备用initramfs和linux-lts内核 submenuentry \u0026#34;Boot using fallback initramfs\u0026#34; { initrd /@/boot/initramfs-linux-fallback.img # linux内核的备用initramfs } submenuentry \u0026#34;Boot to linux-lts\u0026#34; { loader /@/boot/vmlinuz-linux-lts # 加载linux-lts内核 initrd /@/boot/initramfs-linux-lts.img # linux-lts内核对应的initramfs } submenuentry \u0026#34;Boot to linux-lts using fallback initramfs\u0026#34; { loader /@/boot/vmlinuz-linux-lts initrd /@/boot/initramfs-linux-lts-fallback.img # linux-lts的备用initramfs } submenuentry \u0026#34;Boot to terminal\u0026#34; { add_options \u0026#34;systemd.unit=multi-user.target\u0026#34; # 进入终端 } # disabled # 注释掉disable才会生效 } 设定启动选项时需要在volume指定内核和initramfs的所处的子卷，volume的值可以设定成/或/boot所处的子卷标签或者GUID。\n查看子卷标签\n1 2 3 4 5 6 7 8 9 10 11 ❯ sudo lsblk -o name,mountpoint,label,size,uuid NAME MOUNTP LABEL SIZE UUID nvme0n1 476.9G ├─nvme0n1p1 SYSTEM_DRV 260M C2B6-5B79 ├─nvme0n1p2 16M ├─nvme0n1p3 Windows-SSD 175.7G 9292B82A92B81527 ├─nvme0n1p4 Windows 50G B88EF24F8EF20622 ├─nvme0n1p5 WINRE_DRV 1000M D800BC7600BC5CE6 ├─nvme0n1p6 /home arch 239.5G d26ed334-68d4-481d-894f-838783fa4f88 ├─nvme0n1p7 [SWAP] 10G d5792b35-68e7-421d-8dde-365e39e6a92b └─nvme0n1p8 /efi 512M 6CB0-E267 系统启动时会通过Boot Loader加载内核和ramfs，一般还要指定加载微码。内核文件vmlinuz-linux、内核对应的initramfs-linux.img以及微码intel-ucode.img默认位于/boot文件夹，在rEFInd配置时需要确定自己系统中这些文件实际存在的路径。\nArch Linux启动过程参考Arch boot process。\n保存对/efi/EFI/refind/refind.conf更改，此时重启进入rEFInd界面，会多出一个我们刚才定义的启动项，直接选择此项启动会直接加载linux内核，按F2进入此启动项的子选单会出现我们刚才定义的加载fallback initramfs和加载linux-lts内核等选项。选中原先的Arch Linux启动项按Delete隐藏即可。\n","date":"2023-07-09T14:10:06+08:00","image":"https://zhiim.github.io/p/arch-linux-configuration-driver-and-software/arch.webp","permalink":"https://zhiim.github.io/p/arch-linux-configuration-linux-lts-kernel/","title":"Arch Linux 配置 -- 安装linux-lts备用内核"},{"content":"最近趁着假期把Manjaro换成了Arch Linux，本文记录了我安装Arch Linux中的一些配置。\nArch Linux安装 安装教程参考archlinux 简明指南和ArchWiki。\n安装显卡驱动 本人电脑是联想小新Air13 IWL，有一个Intel核显和一个Nvidia独显。\n运行lspci -k | grep -A 2 -E \u0026quot;(VGA|3D)\u0026quot;可以查看显卡信息\n1 2 3 4 5 6 7 00:02.0 VGA compatible controller: Intel Corporation WhiskeyLake-U GT2 [UHD Graphics 620] Subsystem: Lenovo WhiskeyLake-U GT2 [UHD Graphics 620] Kernel driver in use: i915 -- 03:00.0 3D controller: NVIDIA Corporation GP108M [GeForce MX150] (rev a1) Subsystem: Lenovo GP108M [GeForce MX150] Kernel driver in use: nvidia Intel 驱动 参考ArchWiki-Intel graphics。\n首先安装mesa，它提供了用于3D加速得DRI驱动。对于Intel 8代以及更新的硬件选择软件包mesa，对于7代以及更老的芯片选择mesa-amber。\n为了支持32位程序，同时安装对应的lib32-mesa或者lib32-mesa-amber。 为了Vulkan支持，还需要安装vulkan-intel，以及32位支持包lib32-vulkan-intel。\n1 sudo pacman -S mesa lib32-mesa vulkan-intel lib32-vulkan-intel Nvidia 驱动 建议对照ArchWiki-NVIDIA安装，官方教程更加准确并且可能有更新。\n对于新于 Turing (NV160/TUXXX)系列的显卡，如果系统使用linux内核则安装nvidia-open，如果使用其他内核则安装nvidia-open-dkms。 对于较旧且新于 Maxwell (NV110/GMXXX)系列的显卡，如果使用linux内核则安装nvidia，如果使用linux-lts内核则安装nvidia-lts，其他内核则安装nvidia-dkms。 Kepler (NVE0/GKXXX)系列显卡，安装nvidia-470xx-dkms。 更旧的显卡安装开源驱动nouveau。 如果需要32位应用支持，还需安装lib32-nvidia-utils。同时可以安装nvidia-settings用来对nvidia驱动进行一些配置。\n1 sudo pacman -S nvidia nvidia-settings lib32-nvidia-utils 不同型号的显卡对应的软件包选择可以参考archlinux 简明指南-显卡驱动，最好同时对照ArchWiki选择。\n安装好驱动后，编辑/etc/mkinitcpio.conf，删去HOOKS那一项中得kms，阻止内核启动时加载nouveau。\n1 2 # HOOKS=(base udev autodetect modconf keyboard keymap kms consolefont block filesystems fsck) # 更改前 HOOKS=(base udev autodetect modconf keyboard keymap consolefont block filesystems fsck) # 更改后 更改后重新生成initramfs\n1 sudo mkinitcpio -P 重启系统，打开nvidia-settings如果可以看到显卡的各种详细信息，则说明安装成功。\n核显和独显动态切换 参考双显卡（核显 + 独显） 。\n安装optimus-manager，它提供了双显卡下切换仅用核显、仅用独显和动态切换三种模式，optimus-manager-qt则提供了模式切换的图形界面。\n1 yay -S optimus-manager optimus-manager-qt 启用视频硬件加速 参考ArchWiki-Hardware video acceleration。\n首先安装驱动\n1 sudo pacman -S intel-media-driver 不同型号的显卡需要安装的驱动包不同，具体要求可以查看ArchWiki。\n此时可以使用MPV播放视频检验硬件加速是否启用\n1 2 3 4 5 6 7 ❯ mpv --hwdec=auto test.mp4 (+) Video --vid=1 (*) (h264 1920x1080 29.970fps) (+) Audio --aid=1 (*) (aac 2ch 44100Hz) Using hardware decoding (vaapi). AO: [pulse] 44100Hz stereo 2ch float VO: [gpu] 1920x1080 vaapi[nv12] AV: 00:00:02 / 00:03:49 (1%) A-V: 0.000 从终端输出可以看到，此时硬件解码是正常的。\n在浏览器中启用视频硬件加速参考[HowTo] Enable Hardware Video Acceleration / Video Decode In Google Chrome, Brave, Vivaldi And Opera Browsers。以我使用的Vivaldi浏览器为例，只需要新建文件~/.config/vivaldi-stable.conf，在其中写入\n1 2 3 4 5 --use-gl=angle --ignore-gpu-blocklist --enable-features=VaapiVideoEncoder,VaapiVideoDecoder,CanvasOopRasterization --disable-features=UseChromeOSDirectVideoDecoder,UseSkiaRenderer --disable-gpu-driver-workarounds 此时在浏览器中进入vivaldi:gpu，已经正常启用了硬件加速。\nWindows 和 Linux 共用蓝牙 安装双系统后，每次切换系统都需要重新配对蓝牙设备。但是可以通过改变Linux系统中蓝牙设备的配对参数，实现两个系统共用一个蓝牙配对，这些切换系统后旧不用重新配对，直接可以自动连接蓝牙了。\n首先在Linux里配对蓝牙设备，然后重启进入Windows重新配对蓝牙。\n下载PSEXEC，以管理员身份启动终端，进入PSEXEC解压后的文件夹，运行\n1 ./psexec.exe -s -i regedit 在打开的页面中展开HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\BTHPORT\\Parameters\\Keys，里面有一个以Windows蓝牙控制器的MAC地址命名的文件夹，打开该文件夹，里面的文件夹名对应的是蓝牙设备的MAC地址，记下蓝牙设备的MAC地址和里面注册表的值。\n重启进入Linux，进入/var/lib/bluetooth，其中有一个以Linux蓝牙管理器MAC地址命名的文件夹，进入该文件夹，里面的文件夹以蓝牙设备的MAC地址命令，把这个文件夹重命名为和Windows中相同的MAC地址值。例如在我的电脑中，Windows中MAC地址为ffa68a9feeb2，在Linux中就改为FF:A6:8A:9F:EE:B2。\n1 2 3 4 5 6 7 F8:A2:D6:C9:7C:10 # Linux蓝牙管理器MAC地址 ├── cache │ └── FF:A6:8A:9F:EE:B2 ├── FF:A6:8A:9F:EE:B2 # 蓝牙设备MAC地址，改成和Windows相同的值 │ ├── attributes │ └── info └── settings 然后编辑FF:A6:8A:9F:EE:B2中的info文件，将相关的蓝牙配对参数改为和Windows相同，需要更改的值如下\n1 2 3 4 5 6 7 8 9 10 11 [LocalSignatureKey] Key=E03086C6AAAB88CF623E57F2361E6488 # 和Windows中CSRK相同 Counter=0 Authenticated=false [LongTermKey] Key=4BA30A70E71C729CC23D12EF29B8C889 # 和Windows中LTK相同 Authenticated=0 EncSize=16 EDiv=11107 # 和Windows中EDIV相同 Rand=12409689061704920446 # 和Windows中ERand相同 重启蓝牙，蓝牙设备就会自动连接\n1 sudo systemctl restart bluetooth 使用rEFInd引导系统 rEFInd是一个适用于UEFI的Boot Manager，支持自定义主题，可以替代界面简陋的GRUB，用来引导Linux和Windows双系统。 rEFInd和GRUB的关系可以参考这里。首先安装rEFInd软件包\n1 sudo pacman -S refind 将rEFInd引导文件安装到ESP分区\n1 refind-install 配置rEFInd主题，我选择的主题是refind-theme-regular，自动安装命令如下\n1 sudo bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/bobafetthotmail/refind-theme-regular/master/install.sh)\u0026#34; 官方默认安装主题到/boot/efi/EFI/refind/themes/refind-theme-regular，如果在安装Arch时选择将ESP分区挂在在/efi的话，需要按照官方的手动安装指导将主题安装到/efi/EFI/refind/themes/refind-theme-regular。 最终的主题效果如下\n如果引导页面有多余项的话，可以按键盘上的Delete键隐藏该启动项。如果未出现rEFInd引导页，可以在电脑的BIOS设置界面将rEFInd启动项置顶。\n安装MATLAB 我在安装时采用的是MATLAB 2022a安装包，在终端运行安装脚本，出现报错\n1 /home/xu/Downloads/R2022a_Linux/bin/glnxa64/MathWorksProductInstaller: error while loading shared libraries: libcrypt.so.1: cannot open shared object file: No such file or directory 这是由于系统缺少libxcrypt，使用pacman安装相应软件包\n1 sudo pacman -S libxcrypt-compat 再次运行安装脚本，出现报错信息\n1 2 3 terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): Failed to launch web window with error: Unable to launch the MATLABWindow application. The exit code was: 127 [1] 3019 IOT instruction (core dumped) ./install 此时可以按照ArchWiki-MATLAB中的指导进行排查错误来源，为了找出为何MATLABWindow无法加载，在MATLAB安装文件中执行以下命令，查看输出的报错信息\n1 2 ./bin/glnxa64/MATLABWindow ./bin/glnxa64/MATLABWindow: error while loading shared libraries: libgtk-x11-2.0.so.0: cannot open shared object file: No such file or directory 可以看到系统中无法找到libgtk-x11-2.0.so.0，安装缺失的软件包\n1 sudo pacman gtk2 再次运行./bin/glnxa64/MATLABWindow，出现错误信息\n1 ./bin/glnxa64/MATLABWindow: symbol lookup error: /usr/lib/libharfbuzz.so.0: undefined symbol: FT_Get_Color_Glyph_Layer 此时的报错信息已经和ArchWiki一致，按照说明添加环境变量，再次运行安装脚本\n1 2 export LD_PRELOAD=/lib64/libfreetype.so ./install 这时MATLAB安装界面可以正常运行，正常安装即可。\n如果在MATLAB运行时碰到问题，可以参考我之前写的Manjaro Linux 安装 MATLAB，里面记录了诸如设置无法保存、无法开启OpenGL加速等各种问题。\n","date":"2023-07-08T17:40:07+08:00","image":"https://zhiim.github.io/p/arch-linux-configuration-driver-and-software/arch.webp","permalink":"https://zhiim.github.io/p/arch-linux-configuration-driver-and-software/","title":"Arch Linux 配置 -- 驱动和软件安装"},{"content":"最近从 Windows 换到了 Manjaro，日常离不开 MATLAB，虽然 MATLAB 官方提供了 Linux 版本，但是没有对 Manjaro 做适配，所以在安装 MATLAB 的过程中遇到了各种问题，以此记录。任何其他问题建议查阅ArchWiki。\nMATLAB 安装报错 最初安装的时候，我下载的是 MATLAB 2020b 的 iso 镜像，即使按照 ArchWiki 的指导，移除了libfreetype.so*还是会继续报错。后来换成 MATLAB 2022a 的镜像才成功安装。\n进入 MATLAB 2022a 安装文件所处的文件夹，运行./install，出现报错\n1 2 3 terminate called after throwing an instance of \u0026#39;std::runtime_error\u0026#39; what(): Failed to launch web window with error: Unable to launch the MATLABWindow application. The exit code was: 127 [1] IOT instruction (core dumped) ./install 此时可以运行\n1 ./bin/glnxa64/MATLABWindow 应该会出现报错信息\n1 bin/glnxa64/MATLABWindow: symbol lookup error: /usr/lib/libcairo.so.2: undefined symbol: FT_Get_Color_Glyph_Layer FT_Get_Color_Glyph_Layer是 freetype2 里的一个符号，只要删除 MATLAB 安装文件夹里的libfreetype.so*，执行\n1 rm ./bin/glnxa64/libfreetype.so* 或者也可以添加环境变量\n1 LD_PRELOAD=/lib64/libfreetype.so 此时再执行./install就可以成功打开 MATLAB 安装引导。\n解决方案来源：ArchWiki-MATLAB\nMATLAB 界面内中文无法显示 安装完成，打开 MATLAB 却发现中文乱码，所有的中文都是方块。MATLAB 的界面是用 JAVA 开发的，中文无法显示是因为 JAVA 的中文字体配置问题。\n此时进入 MATLAB 的安装路径下的 jre 目录，我的 MATLAB 安装在/usr/local/MATLAB/R2022a，进入该文件夹\n1 cd /usr/local/MATLAB/R2022a/sys/java/jre/glnxa64/jre/lib/fonts 在此目录下新建fallback文件夹\n1 mkdir fallback 将中文字体移动到 fallback 文件夹，我使用的字体是 NotoSansCJK-Regular.ttc，想要获取改字体可以先运行sudo pacman noto-fonts-cjk，然后在/usr/share/fonts/noto-cjk就可找到字体文件。\n1 sudo cp /usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc /usr/local/MATLAB/R2022a/sys/java/jre/glnxa64/jre/lib/fonts/fallback 复制完字体文件后，在fallback文件夹内，运行\n1 mkfontscale 此时会生成fonts.scale，打开fonts.scale，将其中内容复制到上级目录/usr/local/MATLAB/R2022a/sys/java/jre/glnxa64/jre/lib/fonts中的fonts.dir\n注意fonts.dir没有可写权限，需要先授予可写权限\n1 sudo chmod 6 fonts.dir 此时进入 MATLAB 就可以正常显示中文了。\n解决方案来源：Linux 下 Matlab 的安装和中文显示支持。\n如果 MATLAB 的终端无法显示中文，可以在 MATLAB 内主页-预设-字体下的两个字体选项里选择中文字体（如果设置无法保存请参考下节）。\nMATLAB 的设置无法保存 在 MATLAB 的设置界面更改设置选项，发现退出 MATLAB 后在重新打开，所有的设置会恢复默认值，这是因为 MATLAB 无法使用/tmp 文件夹。新建文件夹\n1 mkdir ~/.cache/matlab-tmp 在/home/xuq/.local/share/applications为 MATLAB 建立一个启动方式matlab.desktop，文件内容为\n1 2 3 4 5 6 7 8 [Desktop Entry] Type=Application Version=R2022b Name=MATLAB Comment=Scientific computing software Icon=/home/\u0026lt;username\u0026gt;/.local/share/applications/matlab_logo.png Exec=bash -c \u0026#34;export TMPDIR=/home/\u0026lt;username\u0026gt;/.cache/matlab-tmp; /usr/local/MATLAB/R2022a/bin/glnxa64/MATLAB -desktop; rm -r $TMPDIR\u0026#34; Terminal=False 注意替换 MATLAB 可执行文件的路径，以及 MATLAB 的 Icon 路径。\n解决方案来源：Why aren\u0026rsquo;t my MATLAB preferences saved on Linux?。\nMATLAB 无法打开文本编辑器 在 MATLAB 中新建脚本或者打开脚本时，出现报错Unable to open this file in the current system configuration，终端输出\n1 2 3 4 5 6 7 8 9 Exception in thread \u0026#34;AWT-EventQueue-0\u0026#34;: java.lang.NullPointerException at com.mathworks.mde.desk.MLDesktop.updateTemplate(MLDesktop.java:3665) at com.mathworks.mde.desk.MLDesktop.access$2000(MLDesktop.java:225) at com.mathworks.mde.desk.MLDesktop$NewMFileAction.actionPerformed(MLDesktop.java:2853) at com.mathworks.mwswing.ChildAction.actionPerformed(ChildAction.java:214) at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2022) at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2348) at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402) ...... 此时只需要进入 MATLAB 的安装路径，去除libfreetype.so.6\n1 2 cd /usr/local/MATLAB/R2022a/bin/glnxa64 mv libfreetype.so.6 libfreetype.so.6.old 解决方案来源：MATLAB cannot create or open script files。\nMATLAB 界面缩放过小 MATLAB 的界面、字体和图标在 Linux 下很小，无法随着系统缩放而改变。 以 MATLAB 界面放大 1.25 倍为例，在 MATLAB 终端输入\n1 2 \u0026gt;\u0026gt; s = settings;s.matlab.desktop.DisplayScaleFactor \u0026gt;\u0026gt; s.matlab.desktop.DisplayScaleFactor.PersonalValue = 1.25 解决方案来源：Does MATLAB support High DPI screens on Linux?。\nMATLAB 无法使用 OpenGL 加速 MATLAB 绘图时终端输出\n1 2 3 4 Warning: MATLAB previously crashed due to a low-level graphics error. To prevent another crash in this session, MATLAB is using software OpenGL instead of using your graphics hardware. To save this setting for future sessions, use the opengl(\u0026#39;save\u0026#39;, \u0026#39;software\u0026#39;) command. For more information, see Resolving Low-Level Graphics Issues. 设置由于 MATLAB 无法使用 OpenGL 硬件加速，导致图形渲染能力下降，在 3D 绘图时可能出错。\n进入 MATLAB 可执行文件所在的路径\n1 2 cd /usr/local/MATLAB/R2022a/bin/glnxa64 MATLAB -nodesktop -nosplash -r \u0026#34;opengl info; exit\u0026#34; | grep Software 如果终端中输出的software rendering值不是false（0 也不行）,说明此时未启用硬件加速。查看系统的 OpenGL 配置\n1 glxinfo | grep \u0026#34;direct rendering\u0026#34; 如果direct rendering的值为yes说明配置正确，否则先要解决 OpenGL 的系统配置。如果系统的 OpenGL 配置正确，更改先前创建的 MATLAB 启动方式，在/home/xuq/.local/share/applications打开matlab.desktop，在 Exec 项添加 MATLAB 运行时的环境变量，将其更改为\n1 Exec=bash -c \u0026#34;export TMPDIR=/home/xuq/.config/matlab-temp; export LD_PRELOAD=/usr/lib/libstdc++.so; export LD_LIBRARY_PATH=/usr/lib/dri/; /usr/local/MATLAB/R2022a/bin/glnxa64/MATLAB -desktop; rm -r $TMPDIR\u0026#34; 然后在 MATLAB 可执行文件所在的路径下新建文件\n1 touch /usr/local/MATLAB/R2022a/bin/glnxa64/java.opts 在java.opts中写入-Djogl.disable.openglarbcontext=1。 解决方案来源：ArchWiki-MATLAB。\n","date":"2023-03-19T18:40:57+08:00","permalink":"https://zhiim.github.io/p/manjaro-linux-matlab/","title":"Manjaro Linux 安装 MATLAB"},{"content":"DOA 估计的模糊问题 一些经典的 DOA 估计算法，例如 MUSIC 算法，利用了阵列流型矩阵和噪声子空间的正交关系来确定入射信号的角度。因此在使用这些算法时，必须保证流型矩阵中各个导向矢量是线性无关的，这也是为什么要保证阵列天线的个数大于入射信号的个数。\n在讨论阵列的相位模糊时，如果某一个导向矢量可以表示成其他 K 个导向矢量的线性组合 $$a(\\phi_k,\\theta_k) = \\sum_{i = 1}^K l_ia(\\phi_i,\\theta_i), \\ \\ {l_i} \\ne 0$$ 则称该阵列存在K 阶模糊问题，此时会存在测向模糊。当$K=1$时，称一阶模糊或平凡模糊。\n高阶模糊的识别比较复杂，发生的条件也更加有限，一般只讨论一阶模糊的问题，即流型矩阵中的导向矢量存在 $$a({\\phi _i},{\\theta _i}) = a({\\phi _j},{\\theta _j}),\\ \\ i \\ne j$$均匀线阵的相位模糊 在讨论均匀线阵的模糊问题时，主要考虑阵元间距带来的相位模糊。\n设$\\theta$为实际的入射信号，如果存在$a(\\hat{\\theta})=a(\\theta)$，此时在运行 MUSIC 算法时，在$\\theta$和$\\hat{\\theta}$都会出现谱线峰值。\n在均匀线阵中 DOA 估计的角度范围为$[ - {\\pi \\over 2},{\\pi \\over 2}]$，根据导向矢量的表达式 $$a(\\theta ) = \\begin{bmatrix} e^{j\\omega {\\tau_0}} \u0026 e^{j\\omega {\\tau_1}} \u0026 \\dots \u0026 e^{j\\omega {\\tau_{M-1}}}\\end{bmatrix}$$ 一阶模糊出现的条件为 $$2\\pi \\frac{d}{\\lambda}sin\\hat{\\theta}=2\\pi \\frac{d}{\\lambda}sin\\theta+2k\\pi,\\ \\ k\\in Z$$ 其$d$为相邻两阵元的间距，化简得 $$sin\\hat{\\theta}=sin\\theta+\\frac{k}{d / \\lambda},\\ \\ k \\in Z$$ 根据$sin\\theta$在$[ - {\\pi \\over 2},{\\pi \\over 2}]$上的单调性，我们可以知道一阶模糊是否出现，等价于是否存在除零以外的整数 k 使得$(sin\\theta+\\frac{k}{d / \\lambda})\\in[-1,1]$。于是可以推导出均匀线阵出现一阶模糊的条件：\n$d\\le\\frac{1}{2}\\lambda$ 此时$|\\frac{k}{d/\\lambda}|\\ge 2|k|$，为使$(sin\\theta+\\frac{k}{d / \\lambda})\\in[-1,1]$，k 只能取 0。 $\\frac{1}{2}\\lambda","date":"2023-02-25T15:54:25+08:00","permalink":"https://zhiim.github.io/p/uca-doa-ambiguity/","title":"DOA估计中均匀圆阵的模糊问题"},{"content":"ChromeOS是Google推出的桌面操作系统，主打基于云应用的轻量操作系统，如今的ChromeOS已经支持Linux模式和直接运行Android应用。由于Google没有开放系统给Chromebook以外的设备，一般电脑安装ChromeOS可以借助开源项目brunch。\nbrunch能够利用ChromeOS的recovery文件，生成可用的系统镜像文件，从而在非Chromebook上安装原生ChromeOS。\n硬件要求 x86_64架构，UEFI启动模式 Intel CPU至少第四代，或者AMD Ryzen 不支持独显，不支持虚拟机，不支持Arm 可以安装Linux子系统（WSL2）的Windows或者Linux系统 空闲空间至少有16G 安装ChromeOS到U盘 下载ChromeOS recovery文件。\n参考自己的CPU型号，选择一个和自己电脑配置接近的chrome设备，在Chromium Dash下载该设备的recovery文件，一般选择最新的发行版本即可。\n我选择的是Lenovo Yoga C630 Chromebook，codename为pantheon，下载发行版本为109的recovery文件。\n如果不知道应该下载哪个recovery，brunch的项目文档里也给了推荐的recovery。\nIntel 1代-9代：选择codename为rammus的设备对应的recovery 10代和11代：选择codename为volteer的设备对应的recovery AMD Ryzen：选择codename为zork的设备对应的recovery 下载brunch的release文件\n在brunch的release页面下载和recovery版本号对应的文件，例如下载的recovery为109，则下载Brunch r109。\n制作ChromeOS镜像文件\n将下载好的Brunch和ChromeOS recovery放在一个文件夹，分别解压。\n打开WSL终端，安装必要软件。\n1 sudo apt update \u0026amp;\u0026amp; sudo apt -y install pv cgpt tar unzip 在终端中运行命令，制作ChromeOS镜像。把chromeos_filename.bin用从recovery中解压得到的文件名替代，脚本运行完成后回到的名为chromeos.img的系统镜像。\n1 sudo bash chromeos-install.sh -src chromeos_filename.bin -dst chromeos.img 命令运行完成后直接键入ENTER结束。\n将镜像文件写入U盘\n下载Rufus，选中要使用的U盘和镜像文件，将镜像写入U盘。\n进入ChromeOS\n重启电脑，进入BIOS设置界面，关闭secure boot，设置优先从USB启动。\n保存并退出BIOS设置，电脑会自动重启并从U盘启动系统，第一次进入系统时需要等待较长时间。\n安装ChromeOS和Windows双系统 获取文件以及解压同“安装ChromeOS到Windows”中的步骤1、2、3。\n安装ChromeOS双系统同样通过在WSL终端中运行脚本完成，首先新建一个文件夹用于安装系统如/mnt/d/brunch，表示在d盘新建文件夹brunch，然后输入命令。\n1 sudo bash chromeos-install.sh -src chromeos_filename.bin -dst /mnt/d/brunch/chromeos.img -s size 把命令中的chromeos_filename.bin用从recovery中解压得到的文件名替代，size用一个数字替代，定义了分配给ChromeOS磁盘空间，不小于14并且为4的倍数（单位GB）。\n等待脚本执行完成后输入dualboot然后键入ENTER。\n下一步可以安装Grub2Win，实现在开机时选择想要进入的系统。本人未作尝试，具体步骤可以参考brunch的官方安装教程。\n安装Grub2Win后，依次点击Manage Boot Menu-Add A New Entry-Type，选择“Create user section”。此时为自动用记事本新建一个文本文件，在之前设置的/mnt/d/brunch文件夹中打开“chromeos.grub.txt”，将该文件中的内容复制到新建的空文本中。保存Grub2Win的更改，重启电脑，选择ChromeOS进入系统。\n","date":"2023-02-05T11:02:40+08:00","permalink":"https://zhiim.github.io/p/windows-install-chromeos/","title":"Windows下制作ChromeOS Live USB"},{"content":"基于Python的Telegram Bot，使用pyTelegramBotAPI库。\n准备 创建Bot 点击START私聊@BotFather，输入/newbot开始创建Bot。\n等到BotFather回复后输入想要创建的Bot名称，BotFather再次回复后输入Bot的username，注意username必须以_bot结尾。\n创建完Bot后就可以获取到Bot的token，Telegram Bot API的调用必须提供token。\n安装pyTelegramBotAPI 使用pip安装pyTelegramBotAPI。\n1 pip install pyTelegramBotAPI Bot的运行原理 Bot和一般的用户一样，都有名称和id，可以通过@username查找到Bot。Bot的运行是通过和所有者服务器上的脚本联合实现的，用户通过Telegram向Bot发送信息，Bot的脚本根据Bot的token，就可以在服务器上通过Telegram提供的Bot API获取到用户发送给Bot的信息，从而实现根据用户发送的信息返回给用户相应的回复。\npyTelegramBotAPI是一个Python库，对Telegram Bot API进行了封装。使得开发者不用对API直接调用，使用pyTelegramBotAPI提供的函数和类就可以实现Telegram Bot API的各种功能。\npyTelegramBotAPI的基础使用 新建bot对象 pyTelegramBotAPI中调用API的methods全部定义在TeleBot类中，通过传入Bot token新建bot对象。\n1 2 3 import telebot bot = telebot.TeleBot(\u0026#34;TOKEN\u0026#34;, parse_mode=None) # You can set parse_mode by default. HTML or MARKDOWN TeleBot类中的函数和Telegram Bot API提供的method同名，但是更改了命名规则，例如API中的getMe在TeleBot类对应于get_me，API中的sendMessage被命名为send_message。\nBot提供给用户的回复是通过handler实现的，对于用户发送给Bot的消息，使用message handler进行处理。\n1 2 3 @bot.message_handler(commands=[\u0026#39;start\u0026#39;, \u0026#39;help\u0026#39;]) def send_welcome(message): bot.reply_to(message, \u0026#34;Howdy, how are you doing?\u0026#34;) 上述代码中表示对于用户发送给Bot的消息，定义一个message_handler，这个message_handler在用户发送了指令/start或/help时执行send_welcome函数，产生对用户的回复”Howdy, how are you doing?“。\nmessage对象 用户对Bot发送消息后，通过Telegram Bot API获得到的是一个名为message的JSON对象，包含了消息内容，用户id等相关信息，详情可参考Telegram官方文档。\n在上述给出的message_handler示例代码中，send_welcome函数传入参数message对象，reply_to通过message中包含的属性chat.id等获取用户id，从而向用户发送回复信息。\n需要注意的是，由于from是Python中的保留字，在pyTelegramBotAPI中，message的属性名from被替换为from_user。\nmessage handler message handler用来对用户发送的message类消息做出反应（用户发送的消息一般都为message，除此之外还有投票poll、频道推文channel post等其他类型），用于处理其他类消息的handler还有poll handler、channel post handler等。\n1 2 3 @bot.message_handler(filters) def function_name(message): bot.reply_to(message, \u0026#34;This is a message handler\u0026#34;) 在message handler的一般形式中，filters用来对message进行筛选，使该message_handler只对特定类型的message进行响应。filters的一般格式为name=argument，name表示筛选类型，argument代表筛选范围。例如commands=['start', 'help']表示只对命令/start和/help做出反应。\n官方给出的filters支持的类型如下：\nname argument Condition content_types list of strings (default [\u0026rsquo;text\u0026rsquo;]) True if message.content_type is in the list of strings. regexp a regular expression as a string True if re.search(regexp_arg) returns True and message.content_type == \u0026rsquo;text\u0026rsquo; (See https://docs.python.org/2/library/re.html) commands list of strings True if message.content_type == \u0026rsquo;text\u0026rsquo; and message.text starts with a command that is in the list of strings. chat_types list of chat types True if message.chat.type in your filter func a function (lambda or function reference) True if the lambda or function reference returns True 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import telebot bot = telebot.TeleBot(\u0026#34;TOKEN\u0026#34;) # 只处理命令 \u0026#39;/start\u0026#39; 和 \u0026#39;/help\u0026#39; @bot.message_handler(commands=[\u0026#39;start\u0026#39;, \u0026#39;help\u0026#39;]) def handle_start_help(message): pass # 只处理用户发送内容为“文件”和“音频”的message @bot.message_handler(content_types=[\u0026#39;document\u0026#39;, \u0026#39;audio\u0026#39;]) def handle_docs_audio(message): pass # Handles all text messages that match the regular expression @bot.message_handler(regexp=\u0026#34;SOME_REGEXP\u0026#34;) def handle_message(message): pass # Handles all messages for which the lambda returns True @bot.message_handler(func=lambda message: message.document.mime_type == \u0026#39;text/plain\u0026#39;, content_types=[\u0026#39;document\u0026#39;]) def handle_text_doc(message): pass # Which could also be defined as: def test_message(message): return message.document.mime_type == \u0026#39;text/plain\u0026#39; @bot.message_handler(func=test_message, content_types=[\u0026#39;document\u0026#39;]) def handle_text_doc(message): pass # Handlers can be stacked to create a function which will be called if either message_handler is eligible # This handler will be called if the message starts with \u0026#39;/hello\u0026#39; OR is some emoji @bot.message_handler(commands=[\u0026#39;hello\u0026#39;]) @bot.message_handler(func=lambda msg: msg.text.encode(\u0026#34;utf-8\u0026#34;) == SOME_FANCY_EMOJI) def send_something(message): pass ","date":"2023-01-10T19:39:43+08:00","permalink":"https://zhiim.github.io/p/telegram-bot/","title":"Telegram Bot（一）Python库pyTelegramBotAPI的使用"},{"content":"神经网络 neural network 神经网络由多个神经元互相连接组成，每个神经元是一个计算单元，位于同一层级的多个神经元组成一个网络层。一般的神经网络一般由三种网络层组成：输入层，隐藏层和输出层。\n神经网络的组成 激活函数 Activatioin Functioin 神经元的输入经过激活函数得到输出，激活函数的输出值定义了神经元是否被激活。激活函数有几种不同的形式，它的选取取决于期望的神经元输出。 Binary 输入为正数时神经元输出1，输入为负数时神经元输出0 $$ f(x)= \\small\\begin{cases} 0, \u0026 \\text{if } x \u003c 0 \\newline 1, \u0026 \\text{if } x\\geq 0 \\end{cases} $$ Sigmod 由输入值得到0和1之间的连续输出值 $$ f(x) = {\\large \\frac{1}{1+e^{-x}}} $$ Tanh 由输入值得到-1和1之间的连续输出值 $$ f(x) = {\\large \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}} $$ ReLU 当输入为负数时输出为0，输入为正数时保持输入值 $$ f(x)= \\small \\begin{cases} 0, \u0026 \\text{if } x \u003c 0\\newline x, \u0026 \\text{if } x\\geq 0 \\end{cases} $$ 权重 Weights 由前一神经元输出到下一神经元输入的加权 偏置 Bias $$ output=activation function(\\sum{(weights * inputs) + bias}) $$在PyTorch中建立神经网络 torch.nn提供了建立神经网络需要的组件，在PyTorch中神经网络是一个module，一个神经网络由多个同样是module的网络层组成。所有的神经网络模型都是nn.Module的子类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; print(\u0026#39;Using {} device\u0026#39;.format(device)) class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), nn.ReLU() ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) 通过向模型传递输入数据，模型会自动调用forward()，并返回模型输出。\n1 2 3 4 5 X = torch.rand(1, 28, 28, device=device) # 生成模型输入数据 logits = model(X) # 向模型传递输入数据，model.forward()自动执行，得到输出logits pred_probab = nn.Softmax(dim=1)(logits) y_pred = pred_probab.argmax(1) print(f\u0026#34;Predicted class: {y_pred}\u0026#34;) nn.Linear nn.Linear随机初始化每层的权重和偏置，并储存在tensors中\nnn.Flatten nn.Flatten将28x28的图片转换为一个784维的输入向量\nnn.Sequential nn.Sequential是一个module的容器，数据经过nn.Sequential后，会按照定义时的顺序，依次进行运算。\n1 2 3 4 5 6 7 8 9 10 self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU() ) logits = self.linear_relu_stack(x) # x输入linear_relu_stack后，依次经过module nn.Linear(28*28, 512)， # nn.ReLU()，nn.Linear(512, 512)，nn.ReLU() nn.ReLU 激活函数ReLU\n模型参数 神经网络的参数包括每一个网络层的weight和bais，通过模型的parameters()或named_parameters()可以获取模型中的所有参数\n1 2 3 4 print(\u0026#34;Model structure: \u0026#34;, model, \u0026#34;\\n\\n\u0026#34;) for name, param in model.named_parameters(): print(f\u0026#34;Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\u0026#34;) ","date":"2023-01-09T15:14:55+08:00","image":"https://zhiim.github.io/p/pytorch-tensor/pytorch.webp","permalink":"https://zhiim.github.io/p/pytorch-model/","title":"PyTorch基础 （三）网络模型"},{"content":"PyTorch provides two data primitives: torch.utils.data.DataLoaderand torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Datasetstores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\nloading a dataset load the Fashion-MNIST dataset from TorchVision. The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 %matplotlib inline import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor, Lambda import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=\u0026#34;data\u0026#34;, # data storage path train=True, # load training data download=True, # download data from internet to root if data don\u0026#39;t found at root transform=ToTensor() # data to tensor ) test_data = datasets.FashionMNIST( root=\u0026#34;data\u0026#34;, train=False, # load test data download=True, transform=ToTensor() ) Iterating and Visualizing the Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 labels_map = { 0: \u0026#34;T-Shirt\u0026#34;, 1: \u0026#34;Trouser\u0026#34;, 2: \u0026#34;Pullover\u0026#34;, 3: \u0026#34;Dress\u0026#34;, 4: \u0026#34;Coat\u0026#34;, 5: \u0026#34;Sandal\u0026#34;, 6: \u0026#34;Shirt\u0026#34;, 7: \u0026#34;Sneaker\u0026#34;, 8: \u0026#34;Bag\u0026#34;, 9: \u0026#34;Ankle Boot\u0026#34;, } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(\u0026#34;off\u0026#34;) plt.imshow(img.squeeze(), cmap=\u0026#34;gray\u0026#34;) plt.show() Training with DataLoaders While training a model, we use DataLoader to pass samples in \u0026ldquo;minibatches\u0026rdquo;, reshuffle the data at every epoch to reduce model overfitting, and use Python\u0026rsquo;s multiprocessing to speed up data retrieval. To use DataLoader, we need to set the followings paraments:\ndataset-dataset from which to load the data batch_size-how many samples per batch to load shuffle-set to True to have the data reshuffled at every epoch (default: False) 1 2 3 4 from torch.utils.data import DataLoader train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True) We have loaded that dataset into the Dataloader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels(containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Display image and label. train_features, train_labels = next(iter(train_dataloader)) # 得到batch_size=64的训练数据 print(f\u0026#34;Feature batch shape: {train_features.size()}\u0026#34;) # output: Feature batch shape: torch.Size([64, 1, 28, 28]) # batch_size为64，训练数据的每一项是一个28x28的图片 print(f\u0026#34;Labels batch shape: {train_labels.size()}\u0026#34;) # 绘制训练数据batch中第一个图像 img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=\u0026#34;gray\u0026#34;) plt.show() print(f\u0026#34;Label: {label}\u0026#34;) iter(object[, sentinel]) 用于生成迭代器，传入参数object必须为支持迭代的对象，next() 返回迭代器下一项。\nNormalizatioin Normalization is a common data pre-processing technique that is applied to scale or transform the data to make sure there\u0026rsquo;s an equal learning contribution from each feature.\nTransforms We use transforms to perform some manipulation of the data and make it suitable for training. transform to modify the features and target_transform to modify the labels. ToTensor converts a PIL image or NumPy ndarray into a FloatTensor and scales the image\u0026rsquo;s pixel intensity values in the range [0., 1.]\n","date":"2023-01-07T11:13:45+08:00","image":"https://zhiim.github.io/p/pytorch-tensor/pytorch.webp","permalink":"https://zhiim.github.io/p/pytorch-dataset/","title":"PyTorch基础 （二）Dataset和DataLoader"},{"content":"张量（tensor）是一种类似于数组和矩阵的特殊数据结构。tensor类似于NumPy中的ndarray，两者也可以使用相同的内存地址。\n创建Tensor 直接使用数据创建 1 2 3 4 5 import torch import numpy as np data = [[1, 2],[3, 4]] x_data = torch.tensor(data) 使用NumPy array创建 1 2 3 4 5 6 7 8 9 10 np_array = np.array(data) x_np = torch.from_numpy(np_array) print(f\u0026#34;Numpy np_array value: \\n {np_array} \\n\u0026#34;) print(f\u0026#34;Tensor x_np value: \\n {x_np} \\n\u0026#34;) np.multiply(np_array, 2, out=np_array) print(f\u0026#34;Numpy np_array after * 2 operation: \\n {np_array} \\n\u0026#34;) # x_np会和np_array一起改变 print(f\u0026#34;Tensor x_np value after modifying numpy array: \\n {x_np} \\n\u0026#34;) 由于np_array和x_np使用相同的内存地址，两者的值会同时改变\n使用其他tensor创建 tensor可以使用其他tensor的属性（包括tensor的shape，dtype）进行初始化\n1 2 3 4 5 6 7 x_ones = torch.ones_like(x_data) # x_ones会保持和x_data相同的属性，所有元素都为1 print(f\u0026#34;Ones Tensor: \\n {x_ones} \\n\u0026#34;) x_rand = torch.rand_like(x_data, dtype=torch.float) # x_rand保持x_data的属性，dtype设为torch.float print(f\u0026#34;Random Tensor: \\n {x_rand} \\n\u0026#34;) 使用随机数或常数创建 1 2 3 4 5 6 7 8 shape = (2,3,) rand_tensor = torch.rand(shape) ones_tensor = torch.ones(shape) zeros_tensor = torch.zeros(shape) print(f\u0026#34;Random Tensor: \\n {rand_tensor} \\n\u0026#34;) print(f\u0026#34;Ones Tensor: \\n {ones_tensor} \\n\u0026#34;) print(f\u0026#34;Zeros Tensor: \\n {zeros_tensor}\u0026#34;) Tensor的属性 1 2 3 4 5 tensor = torch.rand(3,4) print(f\u0026#34;Shape of tensor: {tensor.shape}\u0026#34;) print(f\u0026#34;Datatype of tensor: {tensor.dtype}\u0026#34;) print(f\u0026#34;Device tensor is stored on: {tensor.device}\u0026#34;) tensor的属性包括维度shape，数据类型dtype，和存储的设备类型device\nTensor的操作 tensor的参考文档 tensor创建时默认处于CPU中，如果要使用GPU进行tensor计算需要使用.to设置\n1 2 3 # 当GPU可用时，将tensor转移到GPU中 if torch.cuda.is_available(): tensor = tensor.to(\u0026#39;cuda\u0026#39;) Tensor索引 1 2 3 4 5 6 tensor = torch.ones(4, 4) print(\u0026#39;First row: \u0026#39;,tensor[0]) print(\u0026#39;First column: \u0026#39;, tensor[:, 0]) # 第一列 print(\u0026#39;Last column:\u0026#39;, tensor[..., -1]) # 最后一列 tensor[:,1] = 0 # 第二列元素置为0 print(tensor) Tensor合并 tensor的合并有两种方法torch.cat和torch.stack\n1 2 t1 = torch.cat([tensor, tensor, tensor], dim=1) t1 = torch.stack([tensor, tensor, tensor], dim=1) Tensor的数学运算 1 2 3 4 5 6 7 8 9 10 11 12 13 # 矩阵乘法 y1 = tensor @ tensor.T y2 = tensor.matmul(tensor.T) y3 = torch.rand_like(tensor) torch.matmul(tensor, tensor.T, out=y3) # 矩阵对应元素相乘 z1 = tensor * tensor z2 = tensor.mul(tensor) z3 = torch.rand_like(tensor) torch.mul(tensor, tensor, out=z3) 单元素的tensor 单元素的tensor可以使用item()转变为Python中的数值量\n1 2 3 agg = tensor.sum() # 将tensor中的元素相加 agg_item = agg.item() # 将单元素agg转为Python数值 print(agg_item, type(agg_item)) 自动赋值运算 自动赋值运算通常在方法后有 _ 作为后缀，在运算中会直接改变运算量\n1 2 3 print(tensor, \u0026#34;\\n\u0026#34;) tensor.add_(5) # add_改变了tensor的元素值，每个元素加上5 print(tensor) Tensor和NumPy Tensor转为NumPy array 1 2 3 4 t = torch.ones(5) print(f\u0026#34;t: {t}\u0026#34;) n = t.numpy() print(f\u0026#34;n: {n}\u0026#34;) tensor和NumPy array共享内存，两者会同时改变\nNumPy array转为Tensor 1 2 n = np.ones(5) t = torch.from_numpy(n) ","date":"2023-01-06T17:12:03+08:00","image":"https://zhiim.github.io/p/pytorch-tensor/pytorch.webp","permalink":"https://zhiim.github.io/p/pytorch-tensor/","title":"PyTorch基础 （一）Tensors"},{"content":"之前没有通过引导卸载VMware Workstation，直接把整个文件夹删了，导致卸载不彻底。VMware在安装时会先检测是否已经安装，使得我的电脑无法再次安装VMware Workstation。后来按照官方的卸载教程试了几次都没成功，偶然发现系统环境变量中还有VMware的路径，才解决了这个问题。关键在于卸载后没有删除环境变量，所以被认为没有完全卸载VMware Workstation。\n通过Workstation安装程序自动清理 下载对应版本的安装程序，在当前文件夹打开终端，在终端中输入\n1 VMware-workstation-full-xxx-xxx.exe /clean 停止VMware相关的服务 在Windows搜索框搜索services.msc，打开“服务”，停止所有VMware相关的服务。\nVMware Authorization Service VMware Authentication Service VMware Registration Service VMware DHCP Service VMware NAT Service VMware USB Arbitration Service VMware Workstation Server VMware WSX Service 删除VMware network bridge adapter 打开控制面板\\网络和 Internet\\网络连接 右键，属性，选择VMware Bridge Protocol，卸载 删除VMware相关的网络适配器 打开控制面板\\硬件和声音\\设备管理器 在“查看”工具栏勾选上“显示隐藏的设备” 点击“网络适配器”，卸载名字包含VMware的适配器 删除和VMware有关的文件夹 程序安装目录 数据目录\n默认路径C:\\Program Files(X86)\\VMware\\ 开始菜单中的VMware\n路径C:\\ProgramData\\VMware 快捷方式 其他文件 C:\\Windows\\system32\\vmnat.exe C:\\Windows\\system32\\vmnetbridge.exe C:\\Windows\\system32\\VMNetDHCP.exe C:\\Windows\\system32\\vmnetdhcp.leases C:\\Windows\\system32\\vmxw2ksetup.dll C:\\Windows\\system32\\vnetprobe.exe C:\\Windows\\system32\\vnetprobelib.dll C:\\Windows\\system32\\vnetinst.dll C:\\Windows\\system32\\vnetlib.dll C:\\Windows\\system32\\vnetlib.exe C:\\Windows\\system32\\drivers\\vmnet.sys C:\\Windows\\system32\\drivers\\vmnetx.sys C:\\Windows\\system32\\drivers\\VMparport.sys C:\\Windows\\system32\\drivers\\vmx86.sys C:\\Windows\\system32\\drivers\\vmnetadapter.sys C:\\Windows\\system32\\drivers\\vmnetbridge.sys C:\\Windows\\system32\\drivers\\vmnetuserif.sys C:\\Windows\\system32\\drivers\\hcmon.sys C:\\Windows\\system32\\drivers\\vmusb.sys 注册表\n打开注册表管理器，删除以下注册表 HKEY_CLASSES_ROOT\\Installer\\Features\\A57F49D06AE015943BFA1B54AFE9506C HKEY_CLASSES_ROOT\\Installer\\Products\\A57F49D06AE015943BFA1B54AFE9506C HKEY_CLASSES_ROOT\\Installer\\UpgradeCodes\\3F935F414A4C79542AD9C8D157A3CC39 HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\{0D94F75A-0EA6-4951-B3AF-B145FA9E05C6} HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\VMware, Inc.\\VMware Workstation HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\VMware, Inc.\\Installer\\VMware Workstation HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Applications\\vmware.exe 删除环境变量 打开系统环境变量设置，删除VMware Workstation的执行路径\n重启电脑，就完成了VMware Workstation的完全卸载。\n各版本路径存在一些不同，具体参考官方文档。\n","date":"2022-03-19T12:05:03+08:00","permalink":"https://zhiim.github.io/p/uninstall-vmware/","title":"完全卸载VMware Workstation"},{"content":"关于Hugo Hugo的便利之处在于，用户只需编辑markdown文档，Hugo会自动将markdown文档转换为网页。Hugo根据存放于content文件夹中的用户markdown文件，生成网页源文件，并存放于public文件夹中。\n将博客部署到GitHub Pages，只需将Hugo生成的public文件夹推送到GitHub仓库。\n关于Github Pages 官方文档定义：\nGitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.\nGithub Pages 有两种形式，个人/组织页面和项目页面，两者访问时的url不同，为了能够使用https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/访问个人博客，应当设置成个人页面。\nUser/Organization Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/) Project Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/\u0026lt;PROJECT\u0026gt;/) 创建User Page 在Github新建仓库时，个人页面的创建和项目页面不同：\n用于个人页面的仓库必须被用户（而不是组织）所有，并将仓库命名为\u0026lt;username\u0026gt;.github.io\n个人页面的源文件放置于仓库的默认分支中，项目页面需存放在特定分支\n创建仓库时需要注意，免费用户创建的页面仓库必须设置为Public\n选择 Initialize this repository with a README ，完成仓库创建\n将源文件推送到仓库 在创建的仓库中复制远程仓库地址\n在Hugo生成的文件夹中，在终端中输入\n1 2 3 4 5 6 7 hugo #生成网页源文件 cd public #生成的源文件存放在public文件夹中，只需将该文件夹推送到所创建的仓库中 git init #git初始化 git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git #关联远程仓库 git add . git commit -m \u0026#34;first commit\u0026#34; #在本地提交更改 git push -u origin master #将更改推送到远程仓库 此时GitHub仓库中拥有main和master两个分支，其中main分支是创建仓库是自动生成的默认分支，mater分支由本地推送，即博客的网页源文件\nGitHub Pages的个人页面默认从main分支提取网页源文件，所以还需要在仓库的settings-pages-source中将分支改为master分支\n之后即可通过\u0026lt;username\u0026gt;.github.io访问博客页面\n","date":"2022-02-12T19:34:01+08:00","permalink":"https://zhiim.github.io/p/hugo_blog/","title":"将Hugo博客部署到Github Pages"}]