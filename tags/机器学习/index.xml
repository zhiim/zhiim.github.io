<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on Zhiim&#39;s Blog</title>
        <link>https://zhiim.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on Zhiim&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Fri, 26 Sep 2025 20:03:09 +0800</lastBuildDate><atom:link href="https://zhiim.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习 20 kNN</title>
        <link>https://zhiim.github.io/p/ml_20_knn/</link>
        <pubDate>Fri, 26 Sep 2025 20:03:09 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_20_knn/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 20 kNN" /&gt;&lt;h2 id=&#34;nearest-neighbor-classification&#34;&gt;Nearest Neighbor Classification
&lt;/h2&gt;&lt;p&gt;给定一个查询样本 $q$，找到 k 个离 $q$ 最近的样本点。对于回归任务，返回这 k 个点的均值；对于分类任务，返回数量最多的类别或者分布表&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_20_knn/cb3a4fc2eecf909ffb846ee32153a2dc.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;随着超参数 k 的数值增大，kNN 一般会由过拟合变为欠拟合&lt;/p&gt;
&lt;h3 id=&#34;exhaustive-k-nn-算法&#34;&gt;Exhaustive k-NN 算法
&lt;/h3&gt;&lt;p&gt;给定一个样本点 $q$，计算所有的 n 个训练样本点到 $q$ 的距离，维护一个距离的小顶堆得到最小的 k 个距离&lt;/p&gt;
&lt;p&gt;k-NN 的训练复杂度为 0, 运行复杂度为 $O(nd + n\log k)$&lt;/p&gt;
&lt;h3 id=&#34;voronoi-diagrams&#34;&gt;Voronoi Diagrams
&lt;/h3&gt;&lt;p&gt;设 $X$ 是一个点集，对于 $w \in X$，它的 Voronoi cell 定义为&lt;/p&gt;
$$Vor(w) = \{ p \in \mathbb{R}^d : \|p - w\| \le \|p - v\|, \forall v \in X \}$$&lt;p&gt;$X$ 中 所有的 Voronoi cell 就是 Voronoi diagram&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_20_knn/6430250960958b92ba61c74efaa5a8b4.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;Voronoi diagram 的空间复杂度为 $O(n^{[d / 2]}$（用边来衡量），当维度 d 增加是空间复杂度会稍微低于这个公式，但是仍然是指数级增长，所以不适合于高维数据。但是在实际应用中复杂度一般都是 $O(n)$&lt;/p&gt;
&lt;p&gt;在运行时，给定一个点 $q$，找出一个点 $w$ 使得 $q \in Var(w)$，那么我们就可以使用 $w$ 的属性来预测 $q$&lt;/p&gt;
&lt;p&gt;对于二维数据，在训练时需要 $O(n\log n)$ 的复杂度计算 Voronoi diagram，并使用一个梯形图（trapezoidal map）存储数据；运行时查找 Voronoi cell 的复杂度为 $O(\log n)$。对于高维数据需要使用 binary space partition tree 来存储，并且复杂度难以衡量&lt;/p&gt;
&lt;p&gt;Voronoi 只是用了一个邻近点来估计 $q$，所以是一种 1-NN 算法，虽然可以推广到 k 阶，但是计算复杂度较大且难以实现&lt;/p&gt;
&lt;h3 id=&#34;k-d-trees&#34;&gt;k-d Trees
&lt;/h3&gt;&lt;p&gt;k-d Tree 使用类似决策树的方式来进行 NN 搜索，但是使用和决策树不同的准则来选择最优划分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择宽度最大的特征 $w$ 用于划分，即特征 i that $\max_{i, j,k}(X_{ji} - X_{ki})$，因为我们不希望一个样本点发散出去的邻接区跨越多个 box 区域，所以在宽度较大的特征上进行切分可以让 box 接近正方形&lt;/li&gt;
&lt;li&gt;选择特征的中位数或者中值作为划分边界。使用中位数可以保证树的深度为 $\log_2 n$，建树复杂度为 $O(nd \log n)$；如果使用中值可能导致树失衡&lt;/li&gt;
&lt;li&gt;每个中间节点都会存储一个训练数据点，所以每次搜索的时候不用下降到叶节点&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_20_knn/74f6575d219af2056137a8ae78e32041.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;在运行时，给定一个点 $q$，寻找一个训练样本点 $w$ 满足 $\|q - w \| \le (1 + \epsilon) \|q - u\|$，其中 $u$ 是训练样本中距离 $q$ 最近的距离，也就是我们找到的样本点 $w$ 不能远于最小距离的 $1+\epsilon$ 倍&lt;/p&gt;
&lt;p&gt;在搜索时，我们需要计算 $q$ 到区域 B 的最小距离 $dist(q, B) = \min_{z \in B} \|q - z\|$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_20_knn/41de3b27aac7e077137e5c8caec7d046.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;搜索过程中需要维护一个小顶堆，用来保存在还没有搜索的 box 的距离。每次搜索的时候优先搜索离 $q$ 最近的 box。如果当前搜索的 box 离 $q$ 的距离的 $1 + \epsilon$ 倍小于下一个最近 box 则继续，反之停止搜索&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_20_knn/2422184e4d1774c17980fa124dd54f2d.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;1-NN 算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将根节点的距离设置为 0，推入小顶堆 $Q$&lt;/li&gt;
&lt;li&gt;将当前距离 $r$ 设置为 $\infty$&lt;/li&gt;
&lt;li&gt;如果 $Q$ 不为空，并且 $(1 + \epsilon) \cdot \text{minkey}(Q) &gt; r$
&lt;ul&gt;
&lt;li&gt;将 $Q$ 中节点弹出，它对应的 box 是 $B$&lt;/li&gt;
&lt;li&gt;取出 $B$ 中的训练样本 v，如果 $\|q - v\| &lt; r$，那么 $w \leftarrow v; r \leftarrow \|q - v\|$&lt;/li&gt;
&lt;li&gt;继续查看子节点对应的区域 $B&#39;$ 和 $B&#39;&#39;$
&lt;ul&gt;
&lt;li&gt;如果 $(1 + \epsilon) \cdot dist(q, B&#39;) &lt; r$，将 $B&#39;$ 和距离插入 $Q$&lt;/li&gt;
&lt;li&gt;如果 $(1 + \epsilon) \cdot dist(q, B&#39;&#39;) &lt; r$，将 $B&#39;&#39;$ 和距离插入 $Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;返回 $w$&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习 19 AdaBoost</title>
        <link>https://zhiim.github.io/p/ml_19_adaboost/</link>
        <pubDate>Fri, 26 Sep 2025 19:59:30 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_19_adaboost/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 19 AdaBoost" /&gt;&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost
&lt;/h2&gt;&lt;p&gt;AdaBoost 是一个集成学习方法，它可以降低偏差（Bagging 法可以降低方差）&lt;/p&gt;
&lt;p&gt;AdaBoost 使用不同权重的训练样本训练学习器，如果某个样本被错误分类，那么它的权重将增加；并且对每个学习器使用不同的权重，更加准确的学习器施加更大的权重&lt;/p&gt;
&lt;p&gt;假设我们训练 T 个分类器 $G_1, \dots, G_T$，训练样本 $X_i$ 在训练 $G_t$ 是的权重取决于 $G_1, \dots, G_{t-1}$ 错误分类它的次数，此外如果 $X_t$ 被一个非常精准的分类器分类错误，它的权重将会增加的更多，每次 $X_i$ 被正确分类，它的权重也会相应缩小。&lt;/p&gt;
&lt;p&gt;组合学习器（Metalearner）是所有学习器的线性组合，即 $M(z) = \sum_{t = 1}^T \beta_t G_t(z)$，虽然每个学习器 $G_t$ 的输出是二元的分类，但是组合学习器的输出是连续的值&lt;/p&gt;
&lt;p&gt;在第 T 步时，我们需要选择一个新的学习器 $G_T$ 和对应的权重 $\beta_T$，为了实现最优选择，首先选定风险为&lt;/p&gt;
$$Risk = \frac{1}{n} \sum_{i = 1}^n L(M(X_i), y_i)$$&lt;p&gt;其中 AdaBoost 的损失函数为&lt;/p&gt;
$$
L(\hat{\lambda}, \lambda) = e^{- \hat{\lambda}\lambda} = \begin{cases}
e^{-\hat{\lambda}} \quad \lambda = + 1 \\
e^{\hat{\lambda}} \quad \lambda = -1
\end{cases}
$$&lt;p&gt;我们对风险推导可得&lt;/p&gt;
$$
\begin{align*}
n \cdot Risk &amp;= \sum_{i = 1}^n L(M(X_i), y_i) = \sum_{i = 1}^n e^{-y_i M(X_i)} \\
&amp;= \sum_{i = 1}^n \exp \left( -y_i\sum_{i = 1}^T \beta_t G_t(X_i) \right) = \sum_{i = 1}^n \prod_{t = 1}^T e^{-\beta_t y_i G_t(X_i)} \\
&amp;= \sum_{i = 1}^n w_i^{(T)} e^{-\beta_T y_i G_T(X_i)} , \quad where \quad w_i^{(T)} = \prod_{t = 1}^{T - 1} e^{-\beta_t y_i G_t(X_i)} \\
&amp;= e^{-\beta_T} \sum_{y_i = G_T(X_i)} w_i^{(T)} + e^{\beta T} \sum_{y_i \ne G_T(X_i)} w_i^{(T)} \\
&amp;= e^{-\beta_T} \sum_{i = 1}^n w_i^{(T)} + (e^{\beta_T} - e^{-\beta_T}) \sum_{y_i \ne G_T(X_i)} w_i^{(T)}
\end{align*}
$$&lt;p&gt;其中 $w_i^{(T)} = \prod_{t = 1}^{T - 1} e^{-\beta_t y_i G_t(X_i)}$ 只与 $G_1, \dots, G_{T -1}$ 有关，可以看作风险中每个样本的权重，并且该样本在前面几轮中被分错的次数越多，$w_i^{(T)}$ 越大&lt;/p&gt;
&lt;p&gt;同时，我们也可以得到 $w_i^{(T)}$ 的递推公式&lt;/p&gt;
$$
w_i^{(T + 1)} = w_i^{(T)} e^{-\beta_T y_i G_T(X_i)} = \begin{cases}
w_i^{(T)} e^{-\beta T} \quad y_i = G_T(X_i) \\
w_i^{(T)} e^{\beta_T} \quad y_i \ne G_T(X_i)
\end{cases}
$$&lt;p&gt;在风险的最终形态中，第一项 $e^{-\beta T} \sum_{i = 1}^n w_i^{(T)}$ 只与上面的几个分类器有关，所以在第 T 轮想要风险最小，就要最小化第二项中的 $\sum_{y_i \ne G_T(X_i)} w_i^{(T)}$，也就是所有被分类错误的样本的权重 $w_i^{(T)}$。&lt;strong&gt;所以在每一轮中选择 $G_T$ 的准则就是让分类错误的样本的权重之和最小&lt;/strong&gt;。虽然决策树一般可以将所有的样本点都分类正确，但是 boosting 中一般只会使用较短、非理想的决策树&lt;/p&gt;
&lt;p&gt;在选择 $\beta_T$ 时，令 $\frac{d}{d \beta_T}Risk = 0$, 即
&lt;/p&gt;
$$0 = -e^{\beta_T} \sum_{i = 1}^n w_ii^{(T)} + (e^{\beta_T} + e^{-\beta_T}) \sum_{y_i \ne G_T(X_i)} w_i^{(T)}$$&lt;p&gt;
令
&lt;/p&gt;
$$err_T = \frac{\sum_{y_i \ne G_T(X_i)}w_i^{(T)}}{\sum_{i=1}^n w_i^{(T)}}$$&lt;p&gt;
将 $err_T$ 称为 weighted error rate，可以算出
&lt;/p&gt;
$$\beta_T = \frac{1}{2} \ln \left( \frac{1 - err_T}{err_T} \right)$$&lt;p&gt;从上式可以看出，如果 $err_T = 0$ 那么 $\beta_T = \infty$，如果 $err_T = 1 / 2$ 那么 $\beta_T = 0$，也就是说如果一个学习器的 error 为 50%, 则它将对组合学习器无贡献&lt;/p&gt;
&lt;p&gt;更具上面的讨论我们就可以得到 AdaBoost 算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始每个训练样本的权重 $w_i \leftarrow \frac{1}{n},\forall{i} \in [1, n]$&lt;/li&gt;
&lt;li&gt;for 循环 1 至 T
&lt;ol&gt;
&lt;li&gt;计算加权错误率 $err = \frac{\sum_{mis} w_i}{\sum_{all} w_i}$，以及系数 $\beta_t = \frac{1}{2} \ln \left( \frac{1 - err}{err} \right)$&lt;/li&gt;
&lt;li&gt;更新权重 $w_i \leftarrow w_i \cdot \begin{cases} e^{\beta_T}, \quad G_t(X_i) \ne y_i \\ e^{-\beta_t}, \quad other \end{cases} = w_i \cdot \begin{cases} \sqrt{\frac{1 - err}{err}} \\ \sqrt{\frac{err}{1 - err}} \end{cases}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;返回组合学习器 $h(z) = sign\left( \sum_{t = 1}^T \beta_t G_t(z) \right)$&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_19_adaboost/d4cd3fea6576cbd89df241142d3e2524.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;如上图所示，初始时我们为每个训练样本施加相同的权重，然后根据分类器分类正确与错误的情况减小或者增加权重&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;为何 boost 往往搭配决策树，并且使用较短的树结构&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Boosting 可以减小偏差，但是不一定会减小方差，使用较短的树结构主要是为了防止过拟合&lt;/li&gt;
&lt;li&gt;树的训练一般较快，而在 boosting 法下我们需要训练多个学习器&lt;/li&gt;
&lt;li&gt;决策树不需要超参数&lt;/li&gt;
&lt;li&gt;AdaBoost 加上短树相当于是一种特征子集选择，无法提高 metalearning 估计能力的特征完全没有使用（err 为 50% 的）&lt;/li&gt;
&lt;li&gt;线性决策边界和 boost 的组合并不好&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>机器学习 18 主成分分析</title>
        <link>https://zhiim.github.io/p/ml_18_principal_components_analysis/</link>
        <pubDate>Fri, 26 Sep 2025 19:57:00 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_18_principal_components_analysis/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 18 主成分分析" /&gt;&lt;h2 id=&#34;无监督学习&#34;&gt;无监督学习
&lt;/h2&gt;&lt;p&gt;有时候我们的训练数据里面没有标签，但是我们还是希望寻找数据的结构信息&lt;/p&gt;
&lt;p&gt;无监督学习的应用主要分为如下几个大类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;聚类（clustering）：将数据划分成几个相似的点集&lt;/li&gt;
&lt;li&gt;数据降维（dimensionality reduction）：数据一般分布在特征空间的低纬子空间，或者矩阵一般有低阶秩近似&lt;/li&gt;
&lt;li&gt;分布学习（density estimation）：使用离散数据拟合一个连续概率分布&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;主成分分析-principal-components-analysis&#34;&gt;主成分分析 Principal Components Analysis
&lt;/h2&gt;&lt;p&gt;目标：给定一些 $\mathbb{R}^d$ 中的样本点，寻找 k 个方向，可以捕获大部分特征变化&lt;/p&gt;
&lt;p&gt;主成分分析一般有以下几个目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;降低维度可以降低计算复杂度&lt;/li&gt;
&lt;li&gt;移除了干扰维度可以防止过拟合，和特征子集选择类似，但是此时选出来的特征是其他特征的线性组合&lt;/li&gt;
&lt;li&gt;寻找一个较小的可以表征复杂变化的基&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设 $X$ 是一个 $n \times d$ 的设计矩阵，并且 $X$ 的均值为 0。假设有一些正交基 ${} v_1, \dots, v_k {}$，那么 x 可以在正交基下表示为 $\widetilde{x} = \sum_{i = 1}^k (x \cdot v_i) v_i$，其中 $x \cdot v_i$ 表示主坐标，是 x 在 $v_i$ 方向投影的长度&lt;/p&gt;
&lt;p&gt;在降维的时候我们希望最大程度保留原始信息，在主成分分析中信息被定义为方差，所以最佳的方向就是协方差矩阵 $X^TX$ 的特征向量。&lt;/p&gt;
&lt;p&gt;假设 $X^TX$ 的特征特征值为 $0 \le \lambda_1 \le \lambda_2 \le \dots \le \lambda_d$，对应到特征向量 $v_1, v_2, \dots, v_k$ 就是主成分。计算每个数据点在主成分下的主坐标，就是实现了对数据的降维&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 17 神经网络 II：梯度消失和爆炸</title>
        <link>https://zhiim.github.io/p/ml_17_gradient_vanish/</link>
        <pubDate>Fri, 26 Sep 2025 19:51:29 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_17_gradient_vanish/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 17 神经网络 II：梯度消失和爆炸" /&gt;&lt;h2 id=&#34;梯度消失与爆炸&#34;&gt;梯度消失与爆炸
&lt;/h2&gt;&lt;p&gt;当使用 sigmoid 函数时，如果 sigmoid 的输出接近 0 或 1，那么梯度 $s&#39; = s(1 - s) \approx 0$，此时梯度更新将会特别缓慢，这个问题被称为&lt;em&gt;梯度消失&lt;/em&gt;，在深层网络中，反向传播会使得梯度消失加剧&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_17_gradient_vanish/a9c103ee0fbac66eb94e884e5db1952b.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;将激活函数换为 ReLU 将会缓解梯度消失，ReLU 的定义为 $r(\gamma) = max\{0, \gamma\}$，它的导数为&lt;/p&gt;
$$
r&#39;(\gamma) = \begin{cases}
1, \quad \gamma \ge 0,  \\
0, \quad \gamma &lt; 0
\end{cases}
$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_17_gradient_vanish/c1b654a499edbf6249026f47069ba571.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;在现代神经网络中，ReLU 一般被用于中间层的激活函数，因为 ReLU 的梯度很少全部为 0，更多的情况是部分神经元的梯度为 0&lt;/p&gt;
&lt;p&gt;然而 ReLU 的输出可以无限大，带了&lt;em&gt;梯度爆炸&lt;/em&gt;问题，特别是在深层网络中，大输入造成了梯度随着网络深度不断变大&lt;/p&gt;
&lt;h2 id=&#34;输出层&#34;&gt;输出层
&lt;/h2&gt;&lt;p&gt;ReLU 一般被使用于隐藏层，而在输出层一般为线性单元（回归问题）或者 sigmoid/softmax 单元（分类问题），分别对应了线性回归、logistic 回归和 softmax 回归&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 16 神经网络 I：神经网络与梯度计算</title>
        <link>https://zhiim.github.io/p/ml_16_neural_network/</link>
        <pubDate>Fri, 26 Sep 2025 19:41:36 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_16_neural_network/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 16 神经网络 I：神经网络与梯度计算" /&gt;&lt;h2 id=&#34;神经网络&#34;&gt;神经网络
&lt;/h2&gt;&lt;p&gt;单个的、基础的感知机存在严重局限性，无法解决像 XOR（异或）这样看似简单但非线性的问题。如下图，异或的二维输入和输出表示了三维空间的点，无法被线性分类器分类&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/xor_probelm.png&#34; width=&#34;400&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;如果添加一个二次特征 $x_1x_2$，那么 XOR 将在 3 维空间线性可分&lt;/p&gt;
&lt;p&gt;当然，我们也有更佳的方法，通过多个单层线性分类器的叠加可以实现类似电路的逻辑连接，此外在每个线性分类器的输出添加一个非线性的函数 $s$ 可以实现类似电路逻辑门。所以通过多个线性连接+非线性函数（一般为 logistic 函数）可以轻易实现异或，其中每个线性线性连接+非线性函数组成了一个神经元&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/aa65087cd7ec36ba247340feba9e24f2.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;一层隐藏层的网络&#34;&gt;一层隐藏层的网络
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/fcc46df680c9253fabfe67759b5c9de1.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;多个神经元组成的三层网络如同&lt;/p&gt;
&lt;p&gt;集合函数 $s(\gamma) = \frac{1}{1 + e^{-\gamma}}$，被称为激活函数。对于一个向量 $u$，定义 $s(u) =\begin{bmatrix}s(u_1) \\ s(u_2) \\ \vdots\end{bmatrix}$，$s_1(u) = \begin{bmatrix} s(u_1) \\ s(u_2) \\ \vdots \\ 1\end{bmatrix}$ 表示包含偏置的形式&lt;/p&gt;
&lt;p&gt;那么有&lt;/p&gt;
$$
\begin{align*}
h &amp;= s_1(Vx) \\
\hat{y} &amp;= s(Wh) = s(Ws_1(Vx))
\end{align*}
$$&lt;p&gt;神经网络可以有多个输出节点，所以我们可以基于相同的隐状态训练多个分类器，有时候不同的分类器可以使用不同隐藏层神经元，并且多个分类器的结果可以互相补充&lt;/p&gt;
&lt;h3 id=&#34;训练&#34;&gt;训练
&lt;/h3&gt;&lt;p&gt;神经网络的训练一般使用随机梯度下降或者批梯度下降&lt;/p&gt;
&lt;p&gt;选择损失函数 $L(\hat{y}, y)$，例如 $L(\hat{y},y) = \|\hat{y} - y\|^2$, 寻找 $V$ 和 $W$ 可以最小化成本函数 $J(V,W) = \frac{1}{n} \sum_{i = 1}^n L(\hat{y}(X_i), Y_i)$&lt;/p&gt;
&lt;p&gt;神经网络的成本函数一般不是凸函数，存在非常多的局部最小值（local minima），一般可以通过增加更多的隐藏节点缓解这个问题&lt;/p&gt;
&lt;p&gt;神经网络的节点具有对称性，不同神经元之间结构一致，如果所有的神经元都初始化为相同点权重，那么所有的输出和梯度都一致，所有的神经元在参数更新中都会保持一致，所以一般使用随机权重初始化神经元&lt;/p&gt;
&lt;p&gt;神经元的权重不能初始化为过大或者过小，否则会处于 logistic 函数的饱和区，梯度过小&lt;/p&gt;
&lt;h3 id=&#34;梯度计算&#34;&gt;梯度计算
&lt;/h3&gt;&lt;p&gt;我们可以使用链式法则求每个节点的梯度&lt;/p&gt;
&lt;p&gt;对于公式 $f=((a + b) c)^2$，可以将其拆解成链式传播的形式，然后从 $f$ 开始逐次求解每个节点的梯度&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/55c5081feacddfc77a55a222e6f28a39.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;对于多元数据，同样可以使用多元微分
&lt;/p&gt;
$$\frac{\partial}{\partial \alpha} L(y_1(\alpha), y_2(\alpha)) = \frac{\partial L}{\partial y_1} \frac{\partial y_1}{\partial \alpha} + \frac{\partial L}{\partial y_2} \frac{\partial y_2}{\partial \alpha} = \nabla_y L\cdot \frac{\partial}{\partial \alpha} y$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/9c52dfcaeb7a9de6094621b6674aed24.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;同样我们也可以通过链式法则求成本函数，梯度计算是从后往前进行的，被称为反向传播算法&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_16_neural_network/a8519d1a53f7e4ba5259464c6d5a05c9.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

</description>
        </item>
        <item>
        <title>机器学习 15 决策树 II：决策树扩展</title>
        <link>https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/</link>
        <pubDate>Fri, 26 Sep 2025 19:32:13 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 15 决策树 II：决策树扩展" /&gt;&lt;h2 id=&#34;决策树的变种&#34;&gt;决策树的变种
&lt;/h2&gt;&lt;h3 id=&#34;决策树回归&#34;&gt;决策树回归
&lt;/h3&gt;&lt;p&gt;决策树回归就是通过一系列是否问题，创建了一个分段常数函数（Piecewise Constant Regression Fn），在函数的每一段，决策树的预测是一个固定的常数&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/1379e6faa8e219b3268c2a4bd72f1101.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;上图左边是一个对特征维度为 2 的样本进行决策回归的过程，数据顺着路径到达某一个叶子节点得到最终的预测值&lt;/li&gt;
&lt;li&gt;中间的图表示决策树将整个特征空间划分成多个区块，每个区块对应一个叶子节点&lt;/li&gt;
&lt;li&gt;右图表示对应的分段常数函数，每个特征空间中的区块都对应了一个常数的叶子节点预测值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每一个叶子节点的预测值，由落入这个叶子节点的所有样本的值的平均决定，即&lt;/p&gt;
$$\mu_S = \frac{1}{|S|}\sum_{i \in S}y_i$$&lt;p&gt;其中 $S$ 表示落入某个节点的样本索引的集合&lt;/p&gt;
&lt;p&gt;决策树的目标是使分裂后的子区域纯度尽可能高。对于分类数，目标是让子区域的类别尽可能一致；对于回归树，我们希望让子区域的样本真实值 $y$ 尽可能接近&lt;/p&gt;
&lt;p&gt;所以我们可以使用子区域内样本的方差作为 cost&lt;/p&gt;
$$J(S) = Var(\{ y_i: i\in S \}) = \frac{1}{|S|} \sum_{i \in S} (y_i - \mu_S)^2$$&lt;p&gt;所以每次选择可以使得划分出的两个子集方差的加权平均（根据样本点树加权）最小的划分点&lt;/p&gt;
&lt;h3 id=&#34;早停&#34;&gt;早停
&lt;/h3&gt;&lt;p&gt;在构造决策树的时候，没必要让每个叶节点到达纯净，可以实现加速以及防止过拟合&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/877b1a22fcc1f8be2e6752cca6dc733e.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;对于上图两个重合非常大的概率分布，得到一个叶子节点的纯净决策树一定会造成过拟合&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/2a6c07a89432ebc2b635c349581577d5.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;此外叶子节点不那么纯净，往往能包含更加丰富的信息。例如，对于上图的决策树划分，我们可以通过统计每个子区域内不同类别的点数，得到某种类别被分到该叶子节点的概率。对于分类树，直接去概率最大的类别最为叶子节点输出的类别；对于回归树，使用平均值作为叶子节点的预测值&lt;/p&gt;
&lt;p&gt;决策树可以使用的早停策略包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;继续划分不再降低熵或者误差（有可能一直划分到纯净节点）&lt;/li&gt;
&lt;li&gt;大多数（例如大于 90%）节点类别相同&lt;/li&gt;
&lt;li&gt;节点包含的样本点过少（例如小于 10）&lt;/li&gt;
&lt;li&gt;子区域过小&lt;/li&gt;
&lt;li&gt;树的深度太大&lt;/li&gt;
&lt;li&gt;使用验证集检验&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;剪枝&#34;&gt;剪枝
&lt;/h3&gt;&lt;p&gt;如果树太大，可以通过移除一些划分来提高在验证集的表现&lt;/p&gt;
&lt;p&gt;剪枝往往比早停更加有效，因为有时候某次划分带来的提升不明显，但是接下来可能出现提升明显的划分&lt;/p&gt;
&lt;p&gt;在剪枝时，如果每次去除一个分割后重新从头计算验证集指标会造成非常大的开销。相反，我们应该在构建深度树的时候记录下每个节点中包含了哪些样本，这样剪枝的时候只需将子节点的样本合并到父节点&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/09402d778b86912a85e12ed8bc37d0f4.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;多变量划分&#34;&gt;多变量划分
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_15_decision_tree_and_ensemble_learning/0c9125da35eb1f9b9da75261f0290f7a.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;一个标准的决策树一次只对一个特征进行划分（如左图），对于 SVM 可以轻易区分的斜线，决策树需要学习一个非常复杂的规则&lt;/p&gt;
&lt;p&gt;如果想要使用多个特征进行划分，可以在每个节点使用例如 SVM、逻辑回归和高斯判别分析，这些分类器通过决策树的层级结构可以学习到非线性的决策边界&lt;/p&gt;
&lt;h2 id=&#34;集成学习-ensemble-learning&#34;&gt;集成学习 Ensemble Learning
&lt;/h2&gt;&lt;p&gt;决策树快、简单并且可以解释性强，但是方差较大。例如，如果将两个数据集分成两份，分别训练两个决策树，这两个决策树可能差异非常大，特别是如果根节点选择的划分特征不同的时候&lt;/p&gt;
&lt;p&gt;我们可以通过集合多个弱估计器，来得到一个强估计器，如果一个弱估计器的方差较大，那么将多个弱估计器组合去平均将会大大降低方差&lt;/p&gt;
&lt;p&gt;为了得到一个强估计器，可以使用不同的方法取平均：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的学习算法&lt;/li&gt;
&lt;li&gt;使用不同数据集训练的同一算法&lt;/li&gt;
&lt;li&gt;bagging 法：使用一个数据集的不同随机子采样训练同一算法&lt;/li&gt;
&lt;li&gt;随机森林：使用一个数据集的不同随机子采样训练随机决策树&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;平均可以很好的降低方差，但是无法降低偏差，所以我们需要在平均之前得到一些小偏差的估计器（即使存在一定程度的过拟合也没问题）&lt;/p&gt;
&lt;h3 id=&#34;bagging&#34;&gt;Bagging
&lt;/h3&gt;&lt;p&gt;Bagging 是一种使用一个数据集训练多个不同学习器的方法，它对大多数算法都有效（kNN 除外）&lt;/p&gt;
&lt;p&gt;给定一个 $n$ 点的训练样本集，从里面又放回地随机选取 $n&#39;$ 个样本，这意味着某些样本被多次采样。如果 $n&#39; = n$，那么大概有 63.2%的样本被采样&lt;/p&gt;
&lt;p&gt;Bagging 法存在一个问题：某些样本被采样了 $j$ 次，所以他们在训练数据中占有更大的比重&lt;/p&gt;
&lt;h3 id=&#34;随机森林&#34;&gt;随机森林
&lt;/h3&gt;&lt;p&gt;仅仅使用 Bagging 法加上决策树仍然不够，因为可能不同的树模型非常相似。例如，某个特征非常强，那么不同的树都会从它开始划分&lt;/p&gt;
&lt;p&gt;为了得到不同的决策树，对于每个节点，随机选取 $m$ 个特征，从这 $m$ 个特征中一个可以最佳划分的特征。其中 $m$ 是超参数，$m$ 越小越随机，不同树之间越不相关，偏差更大&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 14 决策树 I：决策树基础</title>
        <link>https://zhiim.github.io/p/ml_14_decision_tree/</link>
        <pubDate>Fri, 26 Sep 2025 17:42:31 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_14_decision_tree/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 14 决策树 I：决策树基础" /&gt;&lt;p&gt;决策树是一种用于分类和回归的非线性方法&lt;/p&gt;
&lt;p&gt;决策树有两种节点状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内部节点：对特征值进行测试（通常是一个），并决定走向哪个分支&lt;/li&gt;
&lt;li&gt;叶节点：确定最终的类别&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/f5d21c304134513e4142469b65c70576.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;决策树对类别特征和数值特征都较为有效，并且可解释性强，上图是决策树的工作模式。如右图所示，决策树将 x-空间划分成了长方形区块，不过不同程度的分支组合，决策树的决策边界可以任意复杂&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/409c50b8f09006bfd345a7940d5385d5.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;设 $X$ 是一个 $n\times d$ 设计矩阵，$y\in \mathbb{R}^n$ 是标签，$S \in \{ 1, 2, \dots, n\}$ 是采样点的索引&lt;/p&gt;
&lt;p&gt;决策树的构造&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{Build Decision Tree} \\
&amp;\text{Procedure } GrowTree(S) \\
&amp;\quad \text{if }(y_i = C \text{ for all } i \in S) \\
&amp;\quad \quad \text{return new } Leaf(C) \\
&amp;\quad \text{else} \\
&amp;\quad \quad \text{choose best splitting feature } j \text{ and splitting value } \beta \\
&amp;\quad \quad S_l = \{i \in S: X_{ij} &lt; \beta\} \\
&amp;\quad \quad S_r = \{i \in S: X_{ij} \ge \beta \} \\
&amp;\quad \quad \text{return new } node(j, \beta, GrowTree(S_l), GrowTree(S_r) \\
&amp;\quad \text{end if} \\
&amp;\text{end procedure} \\
\end{aligned}
$$&lt;p&gt;其中的关键是如何选择最优的划分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尝试所有可能的划分&lt;/li&gt;
&lt;li&gt;对于一个集合 $S$，定义它的&lt;em&gt;成本&lt;/em&gt;为 $J(S)$ （用来衡量 S 的混乱程度，目标是划分后，子集尽可能纯洁）&lt;/li&gt;
&lt;li&gt;选择一个划分可以最小化 $J(S_l) + J(S_r)$ （或加权平均 $\frac{|S_l|J(S_l) + |S_r|J(S_r)}{|S_l| + |S_r|}$）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在我们只需要确定如何计算成本 $J (S)$&lt;/p&gt;
&lt;p&gt;方案一：假设 $S$ 中类别最多的类别是 $C$，那么定义 $J (S)$ 为 $S$ 中不属于类别 $C$ 的样本个数&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/1b92b4914ea2aa31f10cfd28040df325.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;但是对于上图这种划分，最后都能得到 $J(S_l) + J(S_r) = 10$ 的结果，但是左边的更好（划分后类别更纯净）， $\frac{|S_l|J(S_l) + |S_r|J(S_r)}{|S_l| + |S_r|}$ 甚至会优先选择右边&lt;/p&gt;
&lt;p&gt;方案二：衡量熵&lt;/p&gt;
&lt;p&gt;假设 $Y$ 是一个随机类别变量，$P (Y = C) = p_C$，那么 $Y$ 属于 $C$ 的惊讶度（degree of suprise）是 $-\log_2p_C$（如果一个非常可能发生的事发生了，我们并不惊讶；如果不太可能发生的事发生了，我们非常惊讶，它的信息量更大）&lt;/p&gt;
&lt;p&gt;那么集合 $S$ 的熵为&lt;/p&gt;
$$H(S) = - \sum p_C \log_2 p_C, \quad p_C = \frac{|\{ i \in S: y_i = C\}|}{|S|}$$&lt;p&gt;熵代表了在均匀分布下随机采样的子集 $S$ 中，为了正确识别它的类别，我们需要传输的信息量&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/f95b243606fb48d31c4fbbd3dbcf6239.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图分别是两个类别和三个类别的条件下，熵随着类别概率变化的曲线，可以看出如果一个集合中类别约纯净，它的熵越小&lt;/p&gt;
&lt;p&gt;在每次划分时，我们可以计算划分后的平均熵&lt;/p&gt;
$$H_{after} = \frac{|S_l|H(S_l) + |S_r|H(S_r)}{|S_l| + |S_r|}$$&lt;p&gt;那么我们选择划分的准则就是最大化&lt;em&gt;信息增益&lt;/em&gt; $H(S) - H_{after}$，也就是最小化 $H_{after}$&lt;/p&gt;
&lt;p&gt;信息增益将会永远大于等于 0，当某一个子集为空或者所有子集内部类别分布完全一致时，信息增益为 0&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/4bc92ff374632d67db16ca4381ad90b7.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;左图是使用熵作为成本函数的划分，可以看到，熵随 $p_C$ 的变化曲线是严格凸的，父节点的熵永远大于两个自节点熵的加权平均；左图是使用 $J(S)$ 作为成本函数的曲线，在很多情况下划分前后的成本相同，无法衡量划分好坏&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于二元特征 $x_i$，直接划分成 $x_i = 0$ 和 $x_i = 1$ 两类&lt;/li&gt;
&lt;li&gt;如果 $x_i$ 有三个或者以上的离散值，可以二元分裂，也可以多元分裂&lt;/li&gt;
&lt;li&gt;如果 $x_i$ 是一个数值，需要在每两个不同值间尝试分裂点&lt;br&gt;
为了加速计算，我们首先对数值进行排序（如下图），每两个不同值之间都存在一个分裂点（竖线），使用不同的分裂点只会将一个样本从右侧移动到左侧，所以只需初始一次排序，后续记录不同类别的数量变化，这样从左到右改变分裂点时只需要 $O (1)$ 的时间重新计算熵
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_14_decision_tree/75bb6cce5d0feffa5abde2e830f73ded.png&#34; width=&#34;800&#34;&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;决策树的计算复杂度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在测试阶段。从根节点到叶节点，最多走完树的深度，复杂度 $\le O(\log n)$&lt;/li&gt;
&lt;li&gt;在训练阶段。
&lt;ul&gt;
&lt;li&gt;对于每个节点。如果是二元特征，每个节点需要依次尝试利用 $d$ 个特征划分，复杂度为 $O (d)$；如果是数值特征，每个特征如果有 $n&#39;$ 个分裂点，那么复杂度为 $O (n&#39;d)$&lt;/li&gt;
&lt;li&gt;对于整个树。每个样本点参与 $O (depth)$ 个节点（从上至下），每个样本点在每个节点的计算复杂度为 $O(d)$（$n&#39;d$ 平均到 $n&#39;$ 个点），总共有 $n$ 个样本点，所以训练复杂度 $\le O(nd \cdot depth)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习 13 回归 IV：正则化</title>
        <link>https://zhiim.github.io/p/ml_13_regularization/</link>
        <pubDate>Fri, 26 Sep 2025 17:15:20 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_13_regularization/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 13 回归 IV：正则化" /&gt;&lt;h2 id=&#34;岭回归-ridge-regression&#34;&gt;岭回归 Ridge Regression
&lt;/h2&gt;&lt;p&gt;岭回归即是最小二乘线性规划加上 $l_2$ 正则化，可以描述为：&lt;br&gt;
寻找一个 $w$，最小化 $J(w) = \|Xw - y\|^2 + \lambda \|w&#39;\|^2$&lt;br&gt;
其中，$w&#39;$ 是使用 0 替代了 $w$ 中对应常数项权重的 $\alpha$ （不用对常数项进行惩罚）&lt;/p&gt;
&lt;p&gt;加入 $l_2$ 范数惩罚 $\lambda \|w&#39;\|^2$ 会鼓励参数学习向着 $\|w&#39;\|$ 较小的方向进行，原因如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;保证了代价函数是一个正定二次型，从而有唯一最优解&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;通过降低方差防止过拟合&lt;br&gt;
如果权重过大，细微的数据动荡将会造成较大的变化&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_13_regularization/02e72d7ed10bd0b1534d3cf0c3147e93.png&#34; width=&#34;400&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图中红色的表示 $\|Xw - y\|^2$ 的等值线，蓝色的表示 $\|w\|$ 的等值线，红色等值线第一次和蓝色等值线相交的点就是岭回归的解&lt;/p&gt;
&lt;p&gt;可以通过使代价函数的梯度为 0 得到最优 $w$，即求解线性方程组&lt;/p&gt;
$$(X^TX + \lambda I&#39;)w = X^T y$$&lt;p&gt;其中 $I&#39;$ 是一个特殊的单位矩阵，它的右下角元素被设为 0（不对常数项偏置惩罚）&lt;/p&gt;
&lt;p&gt;岭回归的方差为 $Var(z^T(X^TX + \lambda I&#39;)^{-1} X^T e)$，可以看出 $\lambda$ 增大时，方差减小，但是偏差会增大&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_13_regularization/61bc163d627382dd809faf4c337ef1c1.png&#34; width=&#34;400&#34;&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;岭回归的贝叶斯解释&#34;&gt;岭回归的贝叶斯解释
&lt;/h3&gt;&lt;p&gt;假设$w&#39;$的先验概率满足 $w&#39; \sim \mathcal{N}(0, \varsigma^2)$，则概率密度函数为 $f(w&#39;) \propto e^{- \|w&#39;\|^2 / (2 \varsigma^2)}$&lt;/p&gt;
&lt;p&gt;可以计算出后验概率为&lt;/p&gt;
$$f_{W|X,Y}(w) = \frac{f_{Y| X, W}(y)f(w&#39;)}{f_{Y|X}(y)}$$&lt;p&gt;后验概率的对数形式&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;为&lt;/p&gt;
$$
\begin{align*}
\ln f_{W|X,Y}(w) &amp;= \ln f_{Y| X, W}(y) + \ln f(w&#39;) - \text{const} \\
&amp; = -\text{const} \|Xw - y\|^2 - \text{const} \|w&#39;\|^2 -\text{const}
\end{align*}
$$&lt;p&gt;最大化后验概率，即最小化 $\|Xw - y\|^2 + \|w&#39;\|^2$&lt;/p&gt;
&lt;p&gt;最大似然估计：寻找参数 $w$，使得在该参数下，当前观测样本出现的概率最大&lt;/p&gt;
&lt;p&gt;最大后验概率估计：寻找参数 $w$，使得在观察到当前样本的前提下，参数的后验概率最大&lt;/p&gt;
&lt;p&gt;Ref: &lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/506449599&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/506449599&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;特征子集选择&#34;&gt;特征子集选择
&lt;/h2&gt;&lt;p&gt;添加特征会增加方差，但是不一定会降低偏差，通过去除难以预测的特征，可以降低偏差&lt;/p&gt;
&lt;p&gt;最直接的方法是，我们可是使用 $d$ 个特征的 $2^d - 1$ 个特征子集，训练不同的模型，然后通过交叉检验选择最优的模型。但是计算开销过大&lt;/p&gt;
&lt;p&gt;前向逐步选择：从 0 特征开始，每次加入一个最优的特征，直到验证集的误差开始升高。最优特征的选择，可以通过依次遍历不同的特征，并通过交叉检验获得。只需要训练 $O (d^2)$ 个模型，但是可能会错过组合在一起最优的多个特征&lt;/p&gt;
&lt;p&gt;后向逐步选择：从 d 个特征开始，逐渐移除特征，每次选择可以使验证集性能下降最小的特征&lt;/p&gt;
&lt;h2 id=&#34;lasso&#34;&gt;LASSO
&lt;/h2&gt;&lt;p&gt;最小二乘线性回归 + $l_1$ 惩罚平均损失&lt;/p&gt;
&lt;p&gt;LASSO 和岭回归一样，都是在最小二乘回归的基础上正则项，但是 LASSO 的 $l_1$ 会使某些维度的权重接近 0&lt;/p&gt;
&lt;p&gt;寻找 $w$ 可以最小化 $\|Xw-y\|^2 + \lambda \|w&#39;\|_1$，其中 $\|w&#39;\|_1 = \sum_{i = 1}^d|w_i|$（不对常数项 $\alpha$ 惩罚）&lt;/p&gt;
&lt;p&gt;岭回归的正则项 $\|w&#39;\|^2$ 是一个超球面，而 $\|w&#39;\|_1$ 对应的是一个正多面体&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_13_regularization/2ed62fd7cb9d7249c919fc7048d2729a.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_13_regularization/f4e3ee03d617d3bc3414a5c1e7f5f27e.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图中，红色的椭圆形表示 $\|Xw - y\|^2$，其中最小二乘法的解为中心的红点。左边蓝色的四边形表示 $l_1$ 正则项，如果 $\lambda$ 较大，对应的四边形约小，两个图形的第一次相交于竖轴，此时特征 $w_1$ 对应的权重为 0，这种方法可以用于去除无用特征&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;$\|Xw - y\|^2$ 中的二次型是 $w^T(X^TX)w$，$\lambda \|w&#39;\|^2$ 中的二次型是 $\lambda w^T I w$，相当于加入一个一定为正的对角矩阵&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;由于 $y = Xw + e$，其中噪声 $e$ 符合高斯分布，所以 $y$ 符合均值为 $Xw$ 的高斯分布，即 $f_{Y| X, W}(y) \propto \exp(-(y - Xw)^2) / 2 \sigma^2$，所以 ${} \ln f_{Y| X, W}(y) =-\text{const} \|Xw - y\|^2 {}$&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习 12 回归 III：回归的统计解释</title>
        <link>https://zhiim.github.io/p/ml_12_statistical_justifications_for_regression/</link>
        <pubDate>Fri, 26 Sep 2025 17:05:20 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_12_statistical_justifications_for_regression/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 12 回归 III：回归的统计解释" /&gt;&lt;h2 id=&#34;统计解释&#34;&gt;统计解释
&lt;/h2&gt;&lt;p&gt;一种典型的模型假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采样点来自一个未知的概率分布，即 $X_i \sim D$&lt;/li&gt;
&lt;li&gt;标签 y 的值是一个未知的非随机函数 $g$ 的结果加上随机噪声，即&lt;br&gt;
$\forall X_i, \quad y_i = g(X_i) + \varepsilon_i, \quad \varepsilon_i \sim D&#39;$，其中 $D&#39;$ 的均值为 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回归问题的目标：找到一个映射 $h$ 来近似 $g$&lt;br&gt;
理想的途径是，选择 $h(x) = E_Y[Y|X=x] = g(x) + E[\varepsilon] = g(x)$&lt;/p&gt;
&lt;h3 id=&#34;从最大似然法推导最小平方代价函数&#34;&gt;从最大似然法推导最小平方代价函数
&lt;/h3&gt;&lt;p&gt;假设 $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$，那么 $y_i \sim \mathcal{N}(g(X_i), \sigma^2)$，那么 $y_i$ 的概率密度函数的对数形式为&lt;/p&gt;
$$\ln f(y_i) = -\frac{(y_i - g(X_i))^2}{2\sigma^2} - C$$&lt;p&gt;其中 $C$ 表示一个常量&lt;/p&gt;
&lt;p&gt;于是对数似然为&lt;/p&gt;
$$l(g;X,y) = \ln (f(y_1)f(y_2) \cdots f(y_n)) = -\frac{1}{2\sigma^2}\sum(y_i - g(X_i))^2 - C&#39;$$&lt;p&gt;从上式可以看出，最大化似然 $l (g; X, y)$，等价于选择一个函数 $g$，可以最小化 $\sum(y_i - g(X_i))^2$&lt;/p&gt;
&lt;h3 id=&#34;经验风险&#34;&gt;经验风险
&lt;/h3&gt;&lt;p&gt;假设 $h$ 的&lt;em&gt;风险&lt;/em&gt;是在联合概率分布 $(X, Y)$ 上的损失的期望，即 $R(h) = E[L]$&lt;/p&gt;
&lt;p&gt;如果我们知道 $X$ 的分布，我们可以通过生成模型估计 $X$ 和 $Y$ 的联合概率分布。但是实际分布通常未知，只能假设所有的样本点都满足实际概率分布，然后最小化&lt;em&gt;经验风险&lt;/em&gt;&lt;/p&gt;
$$\hat{R}(h) = \frac{1}{n} \sum_{i = 1}^n L(h(X_i), y_i)$$&lt;p&gt;理论上，当样本数量 $n\rightarrow \infty$ 时，经验风险将会收敛到实际风险。通过最小化 $\hat{R}(h)$ 来选择 $h$ 的过程被称为&lt;em&gt;经验风险最小化&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;从最大似然推导-logistic-损失&#34;&gt;从最大似然推导 logistic 损失
&lt;/h3&gt;&lt;p&gt;设样本点 $X_i$ 属于类别 C 的实际概率为 $y_i$，预测概率为 $h (X_i)$&lt;/p&gt;
&lt;p&gt;假设总共有 $\beta$ 个和 $X_i$ 相同的数据点，那么将有 $y_i \beta$ 个属于类别 $C$，$(1-y_i)\beta$ 个不属于 $C$&lt;/p&gt;
&lt;p&gt;那么似然为&lt;/p&gt;
$$\prod_{i = 1} ^n h(X_i)^{y_i \beta}(1 - h(X_i))^{(1 - y_i)\beta}$$&lt;p&gt;对数似然为&lt;/p&gt;
$$
\begin{align*}
l(h) &amp; = \ln \mathcal{L}(h) \\
&amp; = \beta \sum_i
\underbrace{\left( y_i \ln h(X_i) + (1 - y_i) \ln (1 - h(X_i)) \right)}_{\text{logistic loss}}
\end{align*}
$$&lt;p&gt;最大化似然，等价于最小化 logistic 损失之和&lt;/p&gt;
&lt;h2 id=&#34;偏差-方差分解&#34;&gt;偏差-方差分解
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;偏差&lt;/em&gt;：由于 $h$ 无法完美拟合 $g$ 导致的误差&lt;/p&gt;
&lt;p&gt;&lt;em&gt;方差&lt;/em&gt;：由于拟合数据中的随机噪声造成的误差&lt;/p&gt;
&lt;p&gt;由于训练数据 $X$ 和 $y$ 来自一个联合概率分布，我们用这些数据来选择一个权重，所以最终选择的拟合函数 $h$ 也来自一个概率分布&lt;/p&gt;
&lt;p&gt;对风险进行分解（$\gamma$ 和 $h(z)$ 互相独立）&lt;/p&gt;
$$
\begin{align*}
R(h) &amp;= E[L(h(z), \gamma)] \\
&amp; = E[(h(z) - \gamma)^2] \\
&amp; = E[h^2(z)] + E[\gamma^2] - 2 E[\gamma h(z)] \\
&amp; = (Var(h(z)) + E[h(z)]^2) + (Var(\gamma) + E[\gamma]^2) - 2 E[\gamma]E[h(z)] \\
&amp; = \underbrace{(E[h(z)] - E[\gamma])^2}_{偏差的平方} + \underbrace{Var(h(z))}_{方差} + \underbrace{Var(\varepsilon)}_{噪声}
\end{align*}
$$&lt;p&gt;上式被称为风险函数的&lt;em&gt;偏差-方差分解&lt;/em&gt;&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_12_statistical_justifications_for_regression/d8e36849ec7632e77e8fc9709fd67052.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;偏差-方差分解&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;上图中，左侧的 sine 曲线时实际函数 $g$，黑色直线是 50 条使用最小二乘线性回归拟合的结果 $h$，模型的风险可以分成右侧偏差、方差和噪声三个部分&lt;/p&gt;
&lt;p&gt;基于偏差-方差分解，我们可以得到如下结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;欠拟合意味着偏差过大&lt;/li&gt;
&lt;li&gt;大多数情况下，过拟合是由于方差过大&lt;/li&gt;
&lt;li&gt;训练误差一般反映偏差但是体现不出方差，但是测试误差可以体现两者&lt;/li&gt;
&lt;li&gt;对于大多数分布来说，当训练数量量 $n\rightarrow \infty$ 时，方差趋近于 0&lt;/li&gt;
&lt;li&gt;如果 $h$ 可以很好地拟合 $g$，那么对于大多数分布而言，当训练数量量 $n\rightarrow \infty$ 时，偏差趋近于 0&lt;/li&gt;
&lt;li&gt;如果 $h$ 不能很好地拟合 $g$，那么对于大多数样本点，偏差较大&lt;/li&gt;
&lt;li&gt;添加一个好的特征可以降低偏差，但是添加一个坏的特征很少会增加偏差&lt;/li&gt;
&lt;li&gt;添加特征通常会增加偏差&lt;/li&gt;
&lt;li&gt;噪声误差无法去除&lt;/li&gt;
&lt;li&gt;在测试集中，噪声只影响 $Var (\varepsilon)$；在训练集中，噪声同时影响偏差和方差&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_12_statistical_justifications_for_regression/768e12a5f7e9de77d1e229b0d99807f4.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;左图中黑色线条为真实的函数，所有点在真实的分布上添加噪声采样而得，不同颜色的曲线分别是使用不同阶数拟合数据得到的曲线。中间灰色的曲线表示训练误差随模型阶数变化的曲线，红色的是测试误差随训练阶数变化的曲线。右边红色曲线是测试集上的平方误差（包含了偏差和方差）随结束变化的曲线，绿色的表示偏差，红色的表示方差&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 11 回归 II：牛顿法和ROC曲线</title>
        <link>https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/</link>
        <pubDate>Fri, 26 Sep 2025 16:50:48 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 11 回归 II：牛顿法和ROC曲线" /&gt;&lt;h2 id=&#34;最小二乘多项式回归&#34;&gt;最小二乘多项式回归
&lt;/h2&gt;&lt;p&gt;我们可以使用一个非线性的多项式特征 $\Phi(X_i)$ 来替代 $X_i$，例如
&lt;/p&gt;
$$\Phi(X_i) = \begin{bmatrix}X_{i1}^2 &amp; X_{i1}X_{i2} &amp; X_{i2}^2 &amp; X_{i1} &amp; X_{i2} &amp; 1\end{bmatrix}$$&lt;p&gt;一旦我们创建了多项式特征向量，接下来的流程将和线性回归或 logistic 回归一致&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/cf75b6b805ca2a0356e57df81c422205.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同次数的拟合效果&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;如果多项式的次数过大，非常容易过拟合&lt;/p&gt;
&lt;h2 id=&#34;加权最小二乘回归&#34;&gt;加权最小二乘回归
&lt;/h2&gt;&lt;p&gt;如果某些样本点比其他的样本更加可信，或者有些样本点我们希望更好地拟合，那么我们可以对这些点施加更高的权重，而对异常点施加更低的权重&lt;/p&gt;
&lt;p&gt;为每个样本点设定一个权重 $\omega_i$，$n$ 个权重组成一个对角矩阵 $\Omega$。于是加权的条件下问题描述变成：&lt;br&gt;
寻找一个 $w$ 使得可以最小化 $(Xw - y)^T \Omega (Xw - y) = \sum_{i = 1}^n \omega_i (X_i \cdot w - y_i)^2$&lt;/p&gt;
&lt;p&gt;同样，我们可以通过计算梯度为 0 的点求解 $w$，即 $X^T\Omega X w = X^T \Omega y$&lt;/p&gt;
&lt;h2 id=&#34;牛顿法&#34;&gt;牛顿法
&lt;/h2&gt;&lt;p&gt;牛顿法是一种迭代寻找函数 $J(w)$ 的最优解的方法（$J(w)$ 必须是光滑的），通常比梯度下降更快&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/2fd9b53968905775ad1c3b725df770c4.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;牛顿法的迭代过程&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;牛顿法的核心是通过二次函数（抛物线）来局部逼近最优解，每次都移动到抛物线的顶点（上图中棕色曲线是用于近似的二次曲线，从左到右棕色曲线的顶点逐渐逼近最优点）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;给定一个点 $v$，使用二次函数在 $v$ 附近近似 $J(w)$&lt;/li&gt;
&lt;li&gt;跳转到二次函数的顶点&lt;/li&gt;
&lt;li&gt;重复上述步骤直到到达停止条件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在每一次迭代中寻找顶点的方式如下：&lt;br&gt;
以 $v$ 为基准，在 $w$ 处梯度的泰勒展开为&lt;/p&gt;
$$\nabla J(w) = \nabla J(v) + (\nabla^2 J(v)) (w - v) + O(\| w - v \|^2)$$&lt;p&gt;其中 $\nabla^2 J(v)$ 是&lt;em&gt;Hessian 矩阵&lt;/em&gt;，通过计算梯度为 0 的点得到顶点&lt;/p&gt;
$$w \approx v - (\nabla^2 J(v))^{-1} \nabla J(v)$$&lt;p&gt;上式中存在计算矩阵的逆的过程，直接计算矩阵的逆通常计算量非常大并且不稳定，一般将其转换为一个线性方程组求解，即 $(\nabla^2 J(w))e = - \nabla j(w)$&lt;/p&gt;
&lt;p&gt;牛顿法的算法流程可以表述如下：&lt;/p&gt;
$$
\begin{align*}
&amp; \text{pick starting point } w \\
&amp; \text{repeat until convergence} \\
&amp; \quad e \leftarrow \text{ solution to liner system } (\nabla^2 J(w))e = - \nabla j(w) \\
&amp; \quad w \leftarrow w + e
\end{align*}
$$&lt;p&gt;牛顿法无法区分最小值点、最大值点和鞍点，所以初始点需要足够接近目标点&lt;/p&gt;
&lt;p&gt;如果目标函数是一个二次函数，牛顿法只需要一步就可以找到精确解，$J$ 越接近二次函数，牛顿法越快收敛&lt;/p&gt;
&lt;p&gt;对于某些优化问题，牛顿法相比梯度下降可以更快收敛。首先，牛顿法会寻找到达最小值的正确步长，而不是任意距离；其次，牛顿法会尝试寻找最优的下降方向，而不是梯度最陡的方向&lt;/p&gt;
&lt;p&gt;然而，牛顿法也有一些缺点。牛顿法需要计算 Hessian 矩阵，在维度较高的时候计算开销非常大，所以神经网络一般不使用牛顿法。此外，牛顿法要求目标函数必须光滑，例如感知机的风险函数就无法使用牛顿法&lt;/p&gt;
&lt;h2 id=&#34;使用牛顿法优化-logistic-回归&#34;&gt;使用牛顿法优化 Logistic 回归
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;Notes/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0/CS189/10%20%e5%9b%9e%e5%bd%92I%ef%bc%9a%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e5%9b%9e%e5%bd%92%e5%92%8c%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92.md##%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&#34; &gt;10 回归I：最小二乘回归和逻辑回归&lt;/a&gt;中已经计算了 $\nabla J(w) = - X^T (y - s)$ ，可以进一步计算牛顿法需要的二阶梯度（Hessian 矩阵）
&lt;/p&gt;
$$\nabla_w^2 J(w) = \sum_{i = 1} ^ n s_i (1 - s_i) X_i X_i^T = X^T \Omega X$$&lt;p&gt;
其中&lt;/p&gt;
$$
\Omega = \begin{bmatrix}
s_1(1 - s_1) &amp; 0 &amp; \dots &amp; 0  \\
0 &amp; s_2(1 - s_2) &amp; &amp; 0  \\
\vdots &amp; &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; s_n(1 - s_n)
\end{bmatrix}
$$&lt;p&gt;$s_i \in (0,1)$ （logistic 函数的输出），所以 $\Omega$ 对于任意 $w$ 是正定的&lt;br&gt;
$\Rightarrow$ Hessian 矩阵 $\nabla_w^2 J(w) =X^T \Omega X$ 是半正定的&lt;br&gt;
$\Rightarrow$ 函数 $J$ 是凸函数 （二阶导数大于等于零，Hessian 矩阵半正定）&lt;/p&gt;
&lt;p&gt;由于 logistic 函数的代价函数 $J (w)$ 是凸的，所以如果可以收敛的话，牛顿法可以找到全局最优点&lt;/p&gt;
&lt;p&gt;logistic 回归的牛顿法优化可以表示成：&lt;/p&gt;
$$
\begin{align*}
&amp; w \leftarrow 0 \\
&amp; \text{repeat until convergence} \\
&amp; \quad e \leftarrow \text{solution to normal equations} (X^T\Omega X) e = X^T (y - s) \\
&amp; \quad w \leftarrow w + e
\end{align*}
$$&lt;p&gt;牛顿法优化 logistic 回归是，$e$ 受远离决策边界且被分类错误的点影响最大（$y_i - s_i$ 最大），受远离决策边界且被分类正确的点影响最小（$y_i - s_i$ 最小）&lt;/p&gt;
&lt;p&gt;如果数据集非常大，为了加快训练，开始可以失效所以样本点的子集（初期的更新比较粗略），随着迭代进行逐渐增大子集的大小（后期需要精细的参数更新）&lt;/p&gt;
&lt;h3 id=&#34;lda-和-logistic-回归对比&#34;&gt;LDA 和 logistic 回归对比
&lt;/h3&gt;&lt;p&gt;LDA 的优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于可以很好分离的类别，LDA 很稳定，而 logistic 回归不稳定&lt;/li&gt;
&lt;li&gt;对于大于 2 个类别的情况，LDA 仍然简洁，而 logistic 回归需要改为 softmax 回归&lt;/li&gt;
&lt;li&gt;LDA 一般对于样本符合高斯分布的情况表现更好，特别是样本量 $n$ 较小的情况&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;logistic 回归的优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;logistic 回归更加强调决策边界
logistic 回归对于距离边界不同距离的点施加的权重不同，而 LDA 会一视同仁。所以 logistic 回归能够更好地降低训练损失，但是对异常数据更敏感&lt;/li&gt;
&lt;li&gt;logistic 回归对非高斯数据的鲁棒性更好&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;roc-曲线&#34;&gt;ROC 曲线
&lt;/h2&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_11_newtons_method_and_roc_curve/5c9e60da29954ce37ea6a4c984ed5340.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;ROC曲线&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;ROC 曲线展示了在不同的判别门限设置下，假阳性和真阳性的比例变化的曲线，ROC 曲线可以通过在不同的判别门限下测试得到&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x 轴表示假阳性（负类别别判别成正类别）的比例&lt;/li&gt;
&lt;li&gt;y 轴表示真阳性的比例&lt;/li&gt;
&lt;li&gt;曲线到顶部的垂直距离表示假阴性的比例&lt;/li&gt;
&lt;li&gt;曲线到右边的水平距离表示真阴性的比例&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;右上角的点表示永远被判别为正例，左下角的点表示永远被判别为负例，对角线表示一个随机分类器&lt;/p&gt;
&lt;p&gt;通过计算 ROC 曲面下的面积可以衡量一个分类器的好坏，面积为 1 表示分类器永远可以正确分类，面积为 0.5 表示一个随机分类器&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 10 回归 I：最小二乘回归和逻辑回归</title>
        <link>https://zhiim.github.io/p/ml_10_regression/</link>
        <pubDate>Sun, 22 Jun 2025 20:29:21 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_10_regression/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 10 回归 I：最小二乘回归和逻辑回归" /&gt;&lt;h2 id=&#34;回归问题&#34;&gt;回归问题
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;分类问题：给定样本点 $x$，预测类别&lt;/li&gt;
&lt;li&gt;回归问题：给定样本点 $x$，预测数值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QDA和LDA不仅仅估计了一个分类器，它们也可以给出估计正确的概率，所以QDA和LDA是对概率的回归&lt;/p&gt;
&lt;p&gt;回归问题的拟合可以分成两个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择一个&lt;strong&gt;回归函数&lt;/strong&gt; $h(x;w)$，其中参数为 $w$&lt;/li&gt;
&lt;li&gt;选择一个优化的代价函数（通常基于&lt;strong&gt;损失函数&lt;/strong&gt;定义）&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一些回归函数：&lt;br&gt;
（1） 线性：$h(x;w,\alpha)=w\cdot x + \alpha$&lt;br&gt;
（2） 多项式型&lt;br&gt;
（3） logistic&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;：$h(x;w,\alpha)=s(w\cdot x + \alpha)$，其中 $s$ 是logistic函数 $s(\gamma)=\frac{1}{1+e^{-\gamma}}$&lt;/p&gt;
&lt;p&gt;一些损失函数：&lt;br&gt;
（A） 平方误差：$L(\hat{y}, y) = (\hat{y} - y)^2$&lt;br&gt;
（B） 绝对值误差：$L(\hat{y}, y) = |\hat{y} - y|$&lt;br&gt;
（C） logistic损失（交叉熵）：$L(\hat{y}, y) = -y \ln \hat{y} - (1 - y) \ln (1 - \hat{y})$&lt;/p&gt;
&lt;p&gt;一些代价函数：&lt;br&gt;
（a） 平均：$J(h) = \frac{1}{n} \sum_{i = 1}^n L(h(X_i), y_i)$&lt;br&gt;
（b） 最大：$J(h) = \max_{i=1}^n L(h(X_i), y_i)$&lt;br&gt;
（c） 加权：$J(h) = \sum_{i = 1}^n w_i L(h(X_i), y_i)$&lt;br&gt;
（d） $l_2$ 正则化：$J + \lambda \|w\|_2^2$&lt;br&gt;
（e） $l_1$ 正则化：$J + \lambda \|w\|_1$&lt;/p&gt;
&lt;p&gt;上述不同的回归和损失与代价函数的选择，可以组成不同的算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最小二乘回归（Least-squares linear regression）：(1) + (A) + (a)，成本函数是二次的，可以通过微积分直接求解&lt;/li&gt;
&lt;li&gt;岭回归（Ridge regression）：(1) + (A) + (a) + (d)，成本函数是二次的，可以通过微积分直接求解，$l_2$ 范数限制权重不过大防止过拟合&lt;/li&gt;
&lt;li&gt;Lasso：(1) + (A) + (a) + (e)，二次规划问题，$l_1$ 正则化倾向于某些权重变为0，适合特征较多时的特征选择&lt;/li&gt;
&lt;li&gt;逻辑回归（logistic regression）：(3) + (C) + (a)，成本函数是凸函数，可以通过梯度下降法求解&lt;/li&gt;
&lt;li&gt;最小绝对偏差（least absolute deviations）：(1) + (B) + (a)，线性规划，绝对误差对离群点不那么敏感&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;最小二乘回归&#34;&gt;最小二乘回归
&lt;/h2&gt;&lt;p&gt;线性回归函数 + 平方损失 + 平均代价&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;：寻找$w$，$\alpha$使得$\sum_{i = 1}^n (X_i \cdot w + \alpha - y_i)^2$最小。如果将偏置$\alpha$包含在权重$w$中，使$w$为一个$d+1$维的向量，那么上述优化问题可以表述成最小化$RSS(w) = \| X w - y \|^2$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_10_regression/elfebiapeohphceccdmmh_lineregress.png&#34; width=&#34;300&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;$RSS(w)$为凸函数，在导数为$0$达到最小值，即$2X^TXw - 2X^T y = 0$，解得$w = (X^T X)^{-1} X^T y$&lt;/p&gt;
&lt;p&gt;上式包含一个假设，即$X^T X$是非奇异的（如果某些样本点共线或者共面），反之解不唯一或不存在&lt;/p&gt;
&lt;p&gt;可以看出$((X^T X)^{-1} X^T)X=I$，所以$X^+ = (X^T X)^{-1} X^T$被称为矩阵$X$的&lt;em&gt;伪逆&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;将$w$的解代入可得$y=X(X^T X)^{-1} X^T y = Hy$，其中矩阵$H$被称为&lt;em&gt;帽子矩阵（hat matrix）&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：容易计算，有唯一、稳定的解&lt;/li&gt;
&lt;li&gt;缺点：对异常值敏感，因为误差被平方；如果$X^T X$是奇异的则无法适用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归
&lt;/h2&gt;&lt;p&gt;logistic 函数 + logistic 损失 + 平均代价&lt;/p&gt;
&lt;p&gt;拟合概率，通常被用于分类问题，标签$y_i$可以为任意概率值，但是通常为$0$或$1$&lt;/p&gt;
&lt;p&gt;QDA和LDA是生成模型，而逻辑回归时判别模型&lt;/p&gt;
&lt;p&gt;问题描述：寻找$w$使其可以最小化代价函数&lt;/p&gt;
$$
J = \sum_{i = 1} ^n L(s(X_i \cdot w), y_i) = -\sum_{i = 1}^n \left( y_i \ln s(X_i \cdot w) + (1 - y_i) \ln(1 - s(X_i \cdot w)) \right)
$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_10_regression/mgkmohcmgbfcdfmpjapkc_logloss.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;可以看出，在预测概率$\hat{y}$和真实概率相同的条件下，logistics损失达到最小值&lt;/p&gt;
&lt;p&gt;逻辑回归的代价函数$J(w)$是凸的，可以通过梯度下降法求解&lt;/p&gt;
&lt;p&gt;logistic函数的导数为&lt;/p&gt;
$$
s&#39;(\gamma) = \frac{\mathrm{d}}{\mathrm{d}\gamma} \frac{1}{1 + e^{-\gamma}} = \frac{e^{-\gamma}}{(1 + e^{-\gamma})^2} = s(\gamma)(1 - s(\gamma))
$$&lt;p&gt;$J(w)$管于$w$的梯度为&lt;/p&gt;
$$
\begin{aligned}
\nabla_w J &amp;= - \sum \left( \frac{y_i}{s_i} \nabla s_i - \frac{1 - y_i}{1 - s_i} \nabla s_i \right) \\
&amp;= - \sum \left( \frac{y_i}{s_i} - \frac{1 - y_i}{1 - s_i} \right) s_i (1 - s_i) X_i \\
&amp;= - \sum(y_i - s_i) X_i \\
&amp;= -X^T (y - s)
\end{aligned}
$$&lt;p&gt;其中$s = \begin{bmatrix} s_1 \\ s_2 \\ \vdots \\ s_n \end{bmatrix}$是$n$个估计值组成的向量&lt;/p&gt;
&lt;p&gt;于是梯度下降的准则为&lt;/p&gt;
$$
w \leftarrow w + \varepsilon X^T (y - s)
$$&lt;p&gt;对于线性可分数据，逻辑回归的梯度下降法虽然能保证找到一个完美的分类边界，并且这个边界最终会是理论上最优的最大间隔，但其权重会趋于无穷大，且收敛速度极慢&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;损失函数衡量的是单个训练样本的预测值与真实值之间的误差；代价函数衡量的是整个数据集的误差  &amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;logistic的形式和LDA类似，所以我们也可以不使用高斯分布拟合数据，而是直接通过logistic函数去拟合概率&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习 09 高斯判别分析 IV：各向异性高斯</title>
        <link>https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/</link>
        <pubDate>Sat, 21 Jun 2025 20:02:09 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 09 高斯判别分析 IV：各向异性高斯" /&gt;&lt;p&gt;我们已经定义了多维正态分布&lt;/p&gt;
$$
f(x)=n(q(x)),\quad n(q) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}}e^{-q/2},\quad q(x)=(x-\mu)^T \Sigma^{-1} (x-\mu)
$$&lt;p&gt;其中有三个重要的矩阵：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\Sigma = V\Gamma V^T$ 是协方差矩阵，其中它的特征值是沿着对应的特征向量方向的方差，即 $Var(v_n^TX)=\lambda_n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Sigma^{1/2}=V\Gamma^{1/2}V^T$ 将球面映射到椭球面，它的特征值是椭球面的半轴长&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Sigma^{-1}=V\Gamma^{-1}V^T$ 是精度矩阵，它的二次型决定了曲面的形状&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;各向异性高斯的参数的最大似然参数估计&#34;&gt;各向异性高斯的参数的最大似然参数估计
&lt;/h2&gt;&lt;p&gt;给定训练样本 $X_1, \dots, X_n$ 和类别 $y_1, \dots, y_n$，我们期望拟合一个最优的高斯分布，使得每个样本点都能归属到对应的类别&lt;/p&gt;
&lt;p&gt;首先估计出先验概率 $\hat{\pi}_C = a / n$，均值 $\hat{\mu}_C = \frac{1}{n} \sum_{i = 1}^n X_i$&lt;/p&gt;
&lt;p&gt;对于 QDA，可以估计出条件协方差&lt;/p&gt;
$$\hat{\Sigma}_C = \frac{1}{n_C} \sum_{i:y_i = C} (X_i - \hat{\mu}_C)(X_i - \hat{\mu}_C)^T$$&lt;p&gt;其中 $\hat{\Sigma}_C$ 是一个半正定矩阵，如果它有零特征值，那么不存在逆矩阵 $\hat{\Sigma}_C^{-1}$，并且行列式 $| \hat{\Sigma}_C|=0$，QDA 将会失效，可以通过降维解决&lt;/p&gt;
&lt;p&gt;对于 LDA，所有类别的协方差相同&lt;/p&gt;
$$
\hat{\Sigma} = \frac{1}{n} \sum_{C} \sum_{i:y_i = C} (X_i - \hat{\mu}_C)(X_i - \hat{\mu}_C)^T
$$&lt;h3 id=&#34;qda&#34;&gt;QDA
&lt;/h3&gt;&lt;p&gt;选定类别 $C$ 使得 $P(Y=C|X=x)$ 最大，其实就是选定 $C$ 最大化&lt;em&gt;二次判别函数&lt;/em&gt;&lt;/p&gt;
$$
\begin{align*}
Q_C(x) &amp;= \ln \left( (\sqrt{2\pi})^d f_{X|Y=C}(x) \pi_C \right) \\
&amp;= -\frac{1}{2} (x - \mu_C)^T \Sigma_C^{-1} (x - \mu_C) - \frac{1}{2} \ln |\Sigma_C | + \ln \pi_C
\end{align*}
$$&lt;p&gt;在多类别的条件下，只需要选择最大的判别函数对应的类别&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/idbaiabafbhhfjokhpjge_qdaaniso.png&#34; width=&#34;700&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图中，在两个类别的场景下  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;左图是两个类别的$C$和$D$的条件概率分布&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;中间的决策函数 $Q_C(x) - Q_D(x)$ 是一个二次函数，所以决策边界是一个二次曲面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;右图中是后验概率$P(Y=C | X= x) = s(Q_C(x) - Q_D(x))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果我们准确已知真实的参数$\pi_C$，$\mu_C$和$\Sigma_C$，我们可以直接得到贝叶斯分类器和贝叶斯最优决策边界；如果我们只能从数据中估计$\hat{\pi}_C$，$\hat{\mu}_C$和$\hat{\Sigma}_C$，这个过程就是QDA算法，QDA分类器只能逼近贝叶斯分类器&lt;/p&gt;
&lt;h3 id=&#34;lda&#34;&gt;LDA
&lt;/h3&gt;&lt;p&gt;如果所有类别的协方差$\Sigma$相同，决策函数中的二次项互相抵消，决策边界是一个线性超平面&lt;/p&gt;
$$
Q_C(x) - Q_D(x) =\underbrace{ (\mu_C - \mu_D)^T \Sigma^{-1} x }_{w^T x} \underbrace{ - \frac{\mu_C^T \Sigma^{-1} x - \mu_D^T \Sigma^{-1} \mu_D}{2} + \ln\pi_C -\ln\pi_D}_{+\alpha}
$$&lt;p&gt;其中决策边界为$w^T x + \alpha = 0$  &lt;/p&gt;
&lt;p&gt;后验概率为$P(Y = C | X = x) = s(w^T x + \alpha)$&lt;/p&gt;
&lt;p&gt;多类别LDA的情况：选择使线性判别函数最大的类别$C$，判别函数的定义为&lt;/p&gt;
$$
\mu_C^T \Sigma^{-1} x - \frac{\mu_C^T \Sigma^{-1} \mu_C}{2} + \ln \pi_C
$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/mgflipbagfemehelhcdco_lda.png&#34; width=&#34;700&#34;&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;qda和lda的对比&#34;&gt;QDA和LDA的对比
&lt;/h3&gt;&lt;p&gt;在两个类别的条件下，虽然我们估计了高斯分布地较多参数，但是在决策函数$Q_C(x) - Q_D(x)$中  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LDA有$d+1$个参数（包括$w$的$d$个参数和$\alpha$的1个参数），所以LDA更加容易欠拟合&lt;/li&gt;
&lt;li&gt;QDA有$\frac{d(d+3)}{2}+1$个参数&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，所以QDA更加容易过拟合&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/bhbfopmiekcmoiaojilbm_ldaqda.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;在上图中，贝叶斯决策边界是紫色的虚线，QDA决策边界是绿色的实线，LDA的决策边界是黑色的点线。当贝叶斯决策边界是线性的时，LDA可以得到更加稳定的拟合，而QDA可能过拟合；当贝叶斯决策边界是曲线时，QDA可能得到更好的拟合效果&lt;/p&gt;
&lt;p&gt;改变先验概率$\pi$或者损失，等价于对决策函数施加额外的常数偏置&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;在二分类的情况下，选择$p$的决策阈值，等价于选择$\pi_C=1-p，\pi_D=p$；或者选定非对称损失，假阳性损失为$p$，假阴性损失为$1-p$&lt;/p&gt;
&lt;h3 id=&#34;一些额外的术语&#34;&gt;一些额外的术语
&lt;/h3&gt;&lt;p&gt;假设$X$是一个$n\times d$的&lt;em&gt;设计矩阵（design matrix）&lt;/em&gt;，它的每一行是一个$d$维的样本点$X_i^T$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对$X$进行中心化&lt;/strong&gt;：从$X$的每一行减去$\hat{\mu}^T$，得到$\dot{X}$，其中$\hat{\mu}^T$是X的所有行的平均，所以$\dot{X}$的所有行的均值为0 &lt;br&gt;
此时可以计算出所有样本点的协方差$\text{Var}(R)=\frac{1}{n}\dot{X}^T\dot{X}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对&lt;/strong&gt;$\dot{X}$&lt;strong&gt;去相关&lt;/strong&gt;：对$\dot{X}$进行旋转，即$Z=\dot{X}V$，其中$\text{Var}(R)=V\Lambda V^T$ &lt;br&gt;
上式通过旋转将样本点转换到了特征向量对应的坐标系&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，此时特征向量就是坐标轴，所以$\text{Var}(Z)=\Lambda$，数据点在不同坐标轴方向的相关性为0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对&lt;/strong&gt;$\dot{X}$&lt;strong&gt;球化&lt;/strong&gt;：$W=\dot{X} \text{Var}(R)^{-1/2}$&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对$X$白化&lt;/strong&gt;：中心化+球化，$X \rightarrow \dot{X} \rightarrow W$&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_09_anisotropic_gaussian_discriminant_analysis/pdmaoalfaajlfpbdfmaho_white.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;白化被应用到很多机器学习算法，例如支持向量机和神经网络。某些特征的数值可能远大于其他特征，例如它们的测量单位不同。SVM 对由数值大的特征的惩罚，会比对数值小的特征更重&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;QDA的决策函数$Q_C(x) - Q_D(x)=x^T \underbrace{(\Sigma_D^{-1} - \Sigma_C^{-1})}_{d(d-1) / 2 \text{ params}}x + \underbrace{(\mu_C^T \Sigma_C^{-1} - \mu_D^T \Sigma_D^{-1})}_{d \text{ params}} x + \underbrace{\dots}_{1 \text{ params}}$&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;贝叶斯决策规则：$\text{if } L(-1,1)P(Y=1|X=x) &gt; L(1, -1)P(Y=-1|X=x) \text{ then } r^*(x) = 1$&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;矩阵$V$是一个由特征向量组成的正交矩阵，用它乘原数据，相当于将原坐标变换到这些特征向量对应的新坐标系&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;$\Sigma^{1/2}$将球面映射到椭球面&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>机器学习 08 高斯判别分析 III：特征向量和多维正态分布</title>
        <link>https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/</link>
        <pubDate>Wed, 18 Jun 2025 20:45:13 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 08 高斯判别分析 III：特征向量和多维正态分布" /&gt;&lt;h2 id=&#34;特征向量&#34;&gt;特征向量
&lt;/h2&gt;&lt;p&gt;给定一个方阵 $A$，如果存在一个向量 $v \neq 0$ 和常量 $\lambda$，满足 $Av = \lambda v$，那么 $v$ 是 $A$ 的一个&lt;em&gt;特征向量&lt;/em&gt;，$\lambda$ 是与其对应的 $A$ 的&lt;em&gt;特征值&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;特征向量乘以 $A$ 后，仍然指向相同的或者相反的方向&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/a7a757292e2db8db86aa71b068909864.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：如果 $v$ 是 $A$ 的特征向量，并且其对应的特征值为 $\lambda$，那么 $v$ 也是 $A^k$ 的特征向量，对应的特征值为 $\lambda^k$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;谱定理（Spectral Theorem）&lt;/em&gt;：每个 $n \times n$ 维的实对称矩阵有 $n$ 个实数特征值和 $n$ 个特征向量，并且不同特征值的特征向量之间正交&lt;/p&gt;
&lt;h3 id=&#34;使用特定的特征向量构建矩阵&#34;&gt;使用特定的特征向量构建矩阵
&lt;/h3&gt;&lt;p&gt;选定 n 个互相正交的单位向量 $v_1,\dots,v_n$，那么矩阵 $V=\begin{bmatrix}v_1 &amp; v_2 &amp; \cdots &amp; v_n\end{bmatrix}$ 满足 $V^TV=VV^TI$，$V$ 是一个&lt;em&gt;正交矩阵&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;正交矩阵乘一个向量，不会改变向量的长度；正交矩阵同乘两个向量，不会改变两个向量之间的夹角&lt;/p&gt;
&lt;p&gt;选定特征值构成对角矩阵&lt;/p&gt;
$$
\Lambda = \begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; &amp; 0 \\
\vdots &amp; &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp;  \lambda_n
\end{bmatrix}
$$&lt;p&gt;根据特征向量的定义，有：$AV=V\Lambda$&lt;/p&gt;
&lt;p&gt;两侧同乘 $V^T$ 可得
&lt;/p&gt;
$$A=V\Lambda V^T = \sum_{i=1}^n \lambda_iv_iv_i^T$$&lt;p&gt;
上式即为矩阵的特征分解&lt;/p&gt;
&lt;p&gt;如果已知一个对称概率密度矩阵 $\Sigma$，我们可以通过以下方式得到它的平方根 $A=\Sigma^{1/2}$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算 $\Sigma$ 的特征向量和特征值&lt;/li&gt;
&lt;li&gt;计算 $\Sigma$ 的特征值的平方根&lt;/li&gt;
&lt;li&gt;使用 $\Sigma$ 的特征向量和特征值的平方根构造矩阵 $A$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;二次型-quadratic-form&#34;&gt;二次型 Quadratic Form
&lt;/h3&gt;&lt;p&gt;矩阵 $M$ 的&lt;em&gt;二次型&lt;/em&gt;为 $x^T M x$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/ba5e1ff1111e4bdf4842bcbe5606415a.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;假设我们已知一个二次型 $q_s(z)=z^TIz =\|z\|^2$ ，它的等值线为圆形如上图左边&lt;br&gt;
我们有一个变化矩阵&lt;/p&gt;
$$
A = V\Lambda V^T = \begin{bmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}  \\
\frac{1}{\sqrt{2}} &amp;  -\frac{1}{\sqrt{2}}
\end{bmatrix} \begin{bmatrix}
2 &amp; 0 \\
0 &amp; -\frac{1}{2}
\end{bmatrix} \begin{bmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}  \\
\frac{1}{\sqrt{2}} &amp;  -\frac{1}{\sqrt{2}}
\end{bmatrix}
$$&lt;p&gt;通过变换 $z = Ax$ 将 $z-$ 空间中的圆形等高线变为上图右边 $x-$ 空间中的椭圆等高线 $q_e(x)$。在等高线 $q_e(x)$ 中，$(1,1)$ 被放大 2 倍，$(1, -1)$ 方向被收缩 -1/2 倍&lt;/p&gt;
&lt;p&gt;可以求出等高线的表达式
&lt;/p&gt;
$$q_e(x) = q_s(A^{-1}x) = \|A^{-1}x\|^2=x^TA^{-2}x$$&lt;p&gt;
可知二次方程 $x^TA^{-2}x$ 的等高线是一个由 $A$ 的特征向量和特征值决定的椭圆&lt;/p&gt;
&lt;p&gt;同理可得，$\{x:x^TA^{-2}x=1\}$ 是一个椭球体，它的轴线是 $v_1,v_2,\dots,v_n$，半轴径是 $\lambda_1,\lambda_2,\dots,\lambda_n$（$\|Av_n\|=\lambda_n$）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;所以我们可以知道，矩阵 $M$ 的二次型 $x^TMx$ 的等高面是一个由 $M^{-1/2}$ 的特征向量和特征值决定的椭球面&lt;/strong&gt;&lt;br&gt;
特殊情况：如果 $M$ 是对角矩阵，那么特征向量就是坐标轴，及椭球面的轴线和坐标轴平行&lt;/p&gt;
&lt;p&gt;已知一个对称矩阵 $M$，那么&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $w^TMw&gt;0$ 对于所有的 $w\neq0$ 都成立，那么 $M$ 是&lt;em&gt;正定矩阵&lt;/em&gt;（positive definite matrix），所有特征值都是正数；&lt;/li&gt;
&lt;li&gt;如果 $w^TMw\ge0$ 对于所有的 $w\neq0$ 都成立，那么 $M$ 是&lt;em&gt;半正定矩阵&lt;/em&gt;（positive semi-definite matrix），所有特征值都是非负数；&lt;/li&gt;
&lt;li&gt;如果 $w^TMw\le0$ 对于所有的 $w\neq0$ 都成立，那么 $M$ 是&lt;em&gt;半负定矩阵&lt;/em&gt;（negative semi-definite matrix），所有特征值都是非正数；&lt;/li&gt;
&lt;li&gt;如果既不是半正定矩阵又不是半负定矩阵，则 $M$ 是&lt;em&gt;不定矩阵&lt;/em&gt;（indefinite matrix）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;可逆矩阵&lt;/em&gt;$\Leftrightarrow$ 行列式不为 0 $\Leftrightarrow$ 没有 0 特征值&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/006cc46996cef0e17c1fedafb5494261.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;对于正定矩阵，它在所有方向上束缚住了曲面，形成了一个封闭的椭球面；而半正定矩阵在某些方向松开，使得曲面可以在这些方向无限延生，形成了柱面；如果不是正定矩阵，则为双曲面&lt;/p&gt;
&lt;h2 id=&#34;各向异性高斯-anisotropic-gaussians&#34;&gt;各向异性高斯 Anisotropic Gaussians
&lt;/h2&gt;&lt;p&gt;对于一个多维高斯分布 $X\sim \mathcal{N}(\mu,\Sigma)$，$X$ 和 $\mu$ 是一个 $d$ 维向量，如果在不同的方向有不同的方差，概率密度函数可以表示为&lt;/p&gt;
$$f(x) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$&lt;p&gt;其中 $\Sigma$ 是一个半正定矩阵，被称为&lt;em&gt;协方差矩阵&lt;/em&gt;；$\Sigma^{-1}$ 也是半正定矩阵，被称为&lt;em&gt;精度矩阵（precision matrix）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;如果把概率密度函数写成 $f(x)=n(q(x))$，其中 $q(x)=(x-\mu)^T\Sigma^{-1}(x-\mu)$&lt;br&gt;
$q(x)$ 是一个中心在 $\mu$ 的柱面，由精度矩阵 $\Sigma^{-1}$ 决定；$n(\cdot)$ 是一个单调凸函数，不会改变等值面的形状（只改变数值，不改变性质和位置，如下图）&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_08_eigenvectors_and_multivariate_normal/1e87ef8d639d80002631fba7a0531532.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;对于高斯随机过程的概率密度函数来说，对应的等值面和 $q(x)=(x-\mu)^T\Sigma^{-1}(x-\mu)$ 相同，只是经过了数值缩放。它在均值 $\mu$ 处到达最大值，在远离 $\mu$ 处逐渐趋近于零&lt;/p&gt;
&lt;p&gt;$q(x)$ 也等于 $\Sigma^{-1 / 2}x$ 到 $\Sigma^{-1 / 2}\mu$ 的距离的平方，即
&lt;/p&gt;
$$d(x, \mu) = \|\Sigma^{-1 / 2}x - \Sigma^{-1 / 2}\mu\| = \sqrt{(x- \mu)^T \Sigma^{-1} (x - \mu)} = \sqrt{q(x)}$$&lt;h3 id=&#34;协方差&#34;&gt;协方差
&lt;/h3&gt;&lt;p&gt;假设 $R, S$ 是随机变量，可以是列向量或者常量，那么&lt;br&gt;
协方差 $Cov(R, S) = E[(R- E[R])(S - E[S])^T]=E[RS^T] - \mu_R \mu_S^T$&lt;br&gt;
方差 $Var(R) = Cov(R, R)$&lt;/p&gt;
&lt;p&gt;如果 $R$ 是一个向量，那么 $R$ 的协方差矩阵为&lt;/p&gt;
$$
Var(R) = \begin{bmatrix}
Var(R_1) &amp; Cov(R_1, R_2) &amp; \cdots &amp; Cov(R_1, R_d)  \\
Cov(R_2, R_1) &amp; Var(R_2) &amp; &amp; Cov(R_2, R_d) \\
\vdots &amp; &amp; \ddots &amp; \vdots \\
Cov(R_d, R_1) &amp; Cov(R_d, R_2) &amp; \cdots &amp; Var(R_d)
\end{bmatrix}
$$&lt;p&gt;对于一个符合高斯分布的变量 $R \sim \mathcal{N}(\mu, \Sigma)$，有 $Var(R) = \Sigma$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果两个随机变量 $R_i, R_j$ 独立，那么 $Cov(R_i, R_j) = 0$ （独立 $\Rightarrow$ 不相关）&lt;/li&gt;
&lt;li&gt;如果 $Cov(R_i, R_j) = 0$，并且他们一起满足多维高斯分布，那么他们独立（高斯分布限制了只能存在线性关系+不相关 $\Rightarrow$ 独立）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 $Var(R)=\Sigma$ 是一个对角矩阵，并且满足多维高斯分布（向量中的每个元素互相独立）&lt;br&gt;
$\Leftrightarrow f(R) = f(R_1)f(R_2)\cdots f(R_d)$&lt;br&gt;
$\Rightarrow$ 椭球面是轴对齐的，并且 $\Sigma$ 的对角元素等于半径的平方&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 07 高斯判别分析 II：各向同性高斯</title>
        <link>https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/</link>
        <pubDate>Fri, 23 May 2025 23:21:38 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 07 高斯判别分析 II：各向同性高斯" /&gt;&lt;h2 id=&#34;高斯判别分析-gaussian-discriminant-analysis&#34;&gt;高斯判别分析 Gaussian Discriminant Analysis
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;基本假设&lt;/strong&gt;：每个类别服从高斯分布&lt;/p&gt;
$$X \sim \mathcal{N}(\mu, \sigma^2): f(x) = \frac{1}{(\sqrt{2 \pi} \sigma)^d}\exp{\left( -\frac{\|x - \mu\|^2}{2\sigma^2} \right)}$$&lt;p&gt;其中 $x,\mu$ 是向量，$\sigma$ 是标量，$d$ 表示维度。（注：这里使用的正态分布公式是一个简化版本，它假设数据在所有的特征维度上的方差相同，并且不考虑不同特征之间的相关性）&lt;/p&gt;
&lt;p&gt;对于每个类别 $C$，假设我们知道了均值 $\mu_C$ 和协方差 $\sigma_C^2$，那么我们就可以根据上述高斯分布公式，将均值和方差代入确定概率密度函数 $f_{X|Y=C}(x)$。同时假设前验概率 $\pi_C=P(Y = C)$ 已知&lt;/p&gt;
&lt;p&gt;在给定观测 $x$ 的条件下，贝叶斯决策准则 $r^*(x)$ 根据最大化 $f_{X|Y=C}(x)$ 来判决类别 $C$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/9265fadf674aa9dfce38e558124f5253.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;两个类别的概率密度函数&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;等价于最大化&lt;/p&gt;
$$Q_C(x) = \ln\left( (\sqrt{2\pi})^d f_{X|Y=C}(x) \pi_C \right)=-\frac{\|x - \mu_C\|^2}{2\sigma_C^2}-d\ln\sigma_C + \ln\pi_C$$&lt;p&gt;$Q_C(x)$ 是一个关于 $x$ 的二次函数&lt;/p&gt;
&lt;h3 id=&#34;二次判别分析-quadratic-discriminant-analysis-qda&#34;&gt;二次判别分析 Quadratic Discriminant Analysis (QDA)
&lt;/h3&gt;&lt;p&gt;假设只有两个类别 $C$ 和 $D$，那么贝叶斯分类器为&lt;/p&gt;
$$
r^*(x) = \begin{cases}
C &amp; \text{if } Q_C(x) - Q_D(x) &gt; 0 \\
D &amp; otherwise
\end{cases}
$$&lt;p&gt;决策函数为 $Q_C(x) - Q_D(x)$，贝叶斯决策边界为 $\{ x: Q_C(x) - Q_D(x) = 0 \}$。如果样本点是 $1$ 维的，那么贝叶斯决策边界可能有 $1$ 个或者 $2$ 个点（二次方程的解）；如果样本点是 $d$ 维的，那么贝叶斯决策边界是一个二次曲面。&lt;/p&gt;
&lt;p&gt;除了根据贝叶斯规则判别样本点的类别外，也可以得到判别正确的概率，即
&lt;/p&gt;
$$P(Y = C | X = x) = \frac{f_{X | Y = C}\pi_C}{f_{X | Y = C}\pi_C + f_{X | Y = D}\pi_D}$$&lt;p&gt;
上式也可以进一步写成
&lt;/p&gt;
$$P(Y = C | X = x) = \frac{e^{Q_C(x)}}{e^{Q_C(x)} + e^{Q_D(x)}} = s(Q_C(x) - Q_D(x))$$&lt;p&gt;
其中 $s(\gamma)=\frac{1}{1 + e^{-\gamma}}$ 是 Logistic 函数也即是 Sigmoid 函数&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/19ef8537b1068b93b5d7ac865151ebe4.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;Logistic函数&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Logistic 函数满足 $s(0) = 0.5$，也就是说当 $Q_C(x) = Q_D(x)$ 时，判别成两种类型的概率均等&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/3560589dbc4fdc3bf12bef2f35ec6c2d.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;多类别QDA判决边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;多类别 QDA 将特征空间划分为多个区域。在二维或更高维度中，通常会形成多个决策边界，这些边界在连接点处相互邻接&lt;/p&gt;
&lt;h3 id=&#34;线性判决分析-linear-discriminant-analysis-lda&#34;&gt;线性判决分析 Linear Discriminant Analysis (LDA)
&lt;/h3&gt;&lt;p&gt;LDA 时 QDA 的一个变种，它有线性的决策边界，相较于 QDA 不容易过拟合&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基本假设&lt;/strong&gt;：所有的高斯分布有相同的方差 $\sigma^2$&lt;/p&gt;
&lt;p&gt;此时贝叶斯分类器是一个线性分类器&lt;/p&gt;
$$Q_C(x) - Q_D(x) = \underbrace{\frac{(\mu_C - \mu_D) \cdot x}{\sigma^2}}_{w \cdot x} \underbrace{- \frac{\|\mu_C\|^2 - \|\mu_D\|^2}{2\sigma^2} + \ln\pi_C - \ln\pi_D} _{+ \alpha}$$&lt;ul&gt;
&lt;li&gt;决策边界为 $w \cdot x + \alpha = 0$&lt;/li&gt;
&lt;li&gt;前验概率为 $P(Y=C | X= x) = s(w \cdot x + \alpha)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特殊情况：如果 $\pi_C = \pi_D = 0.5$，那么决策边界变为 $(\mu_C - \mu_D) \cdot x - (\mu_C - \mu_D) \cdot \left( \frac{\mu_C + \mu_D}{2} \right) = 0$，此时变成了质心法&lt;/p&gt;
&lt;p&gt;多类别 LDA：选择类别 C，最大化线性决策函数 $\frac{\mu_C \cdot x}{\sigma^2} - \frac{\|\mu_C\|^2}{2\sigma^2} + \ln\pi_C$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_07_gaussian_discriminant_analysis/823390d4a186991b57d5fe3eb459e2ad.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;多类别LDA决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;概率分布的参数估计&#34;&gt;概率分布的参数估计
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;最大似然估计&lt;/em&gt;：通过选择使似然函数 $\mathcal{L}$ 最大化的参数来得到统计模型的参数估计值&lt;/p&gt;
&lt;h3 id=&#34;估计前验概率&#34;&gt;估计前验概率 $\pi_C$
&lt;/h3&gt;&lt;p&gt;一个不均匀硬币，有概率 $p$ 得到正面，$1 - p$ 的概率得到反面。在 $n$ 次试验中，有 $a$ 次为正面，则似然函数为
&lt;/p&gt;
$$\mathcal{L} = C_n^a p^a (1 - p)^{n - a}$$&lt;p&gt;
通过最大似然法，使 $\mathcal{L}$ 最大，$\frac{\partial \mathcal{L}}{\partial p} = 0$ ，得到 $p = \frac{a}{n}$&lt;/p&gt;
&lt;p&gt;类似的，如果共有 $n$ 个训练样本点，其中 $a$ 个属于类别 C，则前验概率 $\hat{\pi}_C = a / n$&lt;/p&gt;
&lt;h3 id=&#34;估计均值--和方差&#34;&gt;估计均值 $\mu_C$ 和方差 $\sigma_C^2$
&lt;/h3&gt;&lt;p&gt;根据 $n$ 个样本点 $X_1,X_2,\dots,X_n$ 求出最拟合的高斯分布&lt;/p&gt;
&lt;p&gt;似然函数为
&lt;/p&gt;
$$\mathcal{L}(\mu, \sigma;X_1,X_2,\dots,X_n) = f(X_1)f(X_2)\cdots f(X_n)$$&lt;p&gt;
为了方便计算，最大化对数似然函数&lt;/p&gt;
$$
\begin{align*}
l(\mu, \sigma;X_1,\dots,X_n) &amp;= \ln{f(X_1)} + \dots \ln{f(X_n)} \\
&amp;= \sum_{i = 1}^n \left( -\frac{\| X_i - \mu\|^2}{2 \sigma^2} - d\ln{\sqrt{2\pi} -d \ln{\sigma}} \right)
\end{align*}
$$&lt;p&gt;可得&lt;/p&gt;
$$
\begin{align*}
&amp; \nabla_ul = \sum_{i = 1}^n \frac{X_i - \mu}{\sigma^2} = 0  \quad \Rightarrow \quad \hat{\mu} = \frac{1}{n} \sum_{i = 1}^nX_i \\
&amp; \frac{\partial l}{\partial \sigma} = \sum_{i = 1}^n \frac{\|X_i - \mu\|^2 - d\sigma^2}{\sigma^2} = 0 \quad \Rightarrow \quad \hat{\sigma}^2 = \frac{1}{dn} \sum_{i = 1}^n \|X_i - \hat{\mu} \|^2
\end{align*}
$$&lt;p&gt;对于 QDA，分别使用类别内的数据点每个类别 C 的均值和方差，使用所有数据点估计前验概率 $\hat{\pi}_C = \frac{n_C}{\sum_D n_D}$&lt;/p&gt;
&lt;p&gt;对于 LDA，使用同样的方法估计每个类别的均值以及前验概率，然后使用所有数据估计方差
&lt;/p&gt;
$$\hat{\sigma}^2 = \frac{1}{dn} \sum_C \sum_{\{i:y_i = c\}} \|X_i - \hat{\mu}_C\|^2$$</description>
        </item>
        <item>
        <title>机器学习 06 高斯判别分析 I：决策理论</title>
        <link>https://zhiim.github.io/p/ml_06_decision_theory/</link>
        <pubDate>Fri, 23 May 2025 14:43:55 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_06_decision_theory/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 06 高斯判别分析 I：决策理论" /&gt;&lt;h2 id=&#34;决策理论亦风险最小化&#34;&gt;决策理论亦风险最小化
&lt;/h2&gt;&lt;p&gt;如果多个属于不同类别的样本点可能处于同一个位置（例如训练数据中，升高为 1.70m 的样本，可能男性和女性同样存在），此时我们没法通过一个分类器直接将样本划分为某一种类别，所以我们需要一个概率分类器，输出当前样本为某种类别的概率&lt;/p&gt;
&lt;p&gt;贝叶斯定理：&lt;/p&gt;
$$P(Y| X) = \frac{P(X| Y) P(Y)}{P(X)}$$&lt;p&gt;其中 $P(Y | X)$ 表示观测到 $X$ 的条件下事件 $Y$ 发生的后验概率，$ P(Y)$ 表示事件发生的先验概率。&lt;/p&gt;
&lt;p&gt;例如，假设人群中 $10\%$ 的人患有癌症，$90\%$ 的人没有癌症，不同职业的人患有癌症的概率分布为&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;job $(X)$&lt;/th&gt;
          &lt;th&gt;miner $(X = 0)$&lt;/th&gt;
          &lt;th&gt;farmer $(X = 1)$&lt;/th&gt;
          &lt;th&gt;other ${} (X = 2)$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;cancer ${} (Y = 1)$&lt;/td&gt;
          &lt;td&gt;$20\%$&lt;/td&gt;
          &lt;td&gt;$50\%$&lt;/td&gt;
          &lt;td&gt;$30\%$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;no cancer $(Y = -1)$&lt;/td&gt;
          &lt;td&gt;$1\%$&lt;/td&gt;
          &lt;td&gt;$10\%$&lt;/td&gt;
          &lt;td&gt;$89\%$&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们可以计算出，农民没有癌症的概率为&lt;/p&gt;
$$P(Y = -1 | X = 0) = \frac{P(X = 0 | Y = -1) P (Y = -1)}{P(X = 0)}=\frac{0.1 \times 0.9}{0.1 \times 0.5 + 0.9 \times 0.1} = \frac{9}{14}$$&lt;p&gt;在我们的概率分类器中，可能不同的错误判决类型带来的影响不同。例如将癌症判决为假阴性（本来有癌症但被判决为没有癌症）和假阳性（本来没有被判决为有）的后果差别非常大，前者会造成癌症被忽视，后者只会带来额外的检查开销。此时，我们可以通过损失函数衡量不同错误的代价&lt;/p&gt;
$$
L(\hat{y}, y) = \begin{cases}
1 &amp; \text{if } \hat{y} = 1, y = -1  \\
5 &amp; \text{if } \hat{y} = -1, y = 1 \\
0 &amp; \text{if } \hat{y} = y
\end{cases}
$$&lt;p&gt;其中 $\hat{y}$ 表示预测的类别，$y$ 表示真实的类别， 假阴性在损失函数中被施加了更多的权重，上述损失函数被称为是&lt;em&gt;非对称&lt;/em&gt;的&lt;/p&gt;
&lt;p&gt;相反的，一个对称的&lt;em&gt;0-1 损失函数&lt;/em&gt;可以表示为&lt;/p&gt;
$$
L(\hat{y}, y) = \begin{cases}
1 &amp; \text{if } \hat{y} \neq y \\
0 &amp; \text{if } \hat{y} = y
\end{cases}
$$&lt;p&gt;定义 $r: \mathbb{R}^d \rightarrow \pm 1$ 是一个&lt;em&gt;决策规则&lt;/em&gt;，也被称为&lt;em&gt;分类器&lt;/em&gt;，它表示一个将特征向量 $x$ 映射到 1 或 -1 的函数&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$r$ 的&lt;em&gt;风险&lt;/em&gt;指在所有的可能的数据点 $(x, y)$ 上的期望损失&lt;/strong&gt;&lt;/p&gt;
$$
\begin{align*}
R(r) &amp;= E[L(r(X), Y)] \\
&amp;= \sum_x \left( L(r(x), 1)P(Y = 1 | X = x) + L(r(x), -1)P(Y = -1 | X = x) \right) P(X = x) \\
&amp;= P(Y = 1) \sum_{x} L(r(x), 1) P(X = x | Y = 1) +\\
&amp; \quad P(Y = -1) \sum_x L(r(x), -1) P(X = x | Y = -1)
\end{align*}
$$&lt;p&gt;&lt;em&gt;贝叶斯决策规则&lt;/em&gt;（贝叶斯分类器）就是可以最小化风险函数 $R(r)$ 的函数 $r^*$，当 $L(1, 1) = L(-1, -1) = 0$，即预测正确损失函数为 0 的情况下&lt;/p&gt;
$$
r^* (x) = \begin{cases}
1 &amp; \text{if } L(-1, 1)P(Y = 1 | X = x) &gt; L(1, -1)P(Y = -1 | X = x) \\
-1 &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;当损失函数 $L$ 是对称的时，$r^*$ 就是&lt;strong&gt;选择后验概率最大的类别&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果我们可以知道真实的概率分布，可以直接根据上式构建一个理想的概率分类器 $r^*$。但是真实场景中，我们只能通过一些统计手段估计概率分布&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;获得使风险函数最小的 $r^*$ 的过程被称为&lt;em&gt;风险最小化&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;连续概率分布&#34;&gt;连续概率分布
&lt;/h3&gt;&lt;p&gt;假设 $X$ 有一个连续的概率密度函数，则&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_06_decision_theory/cd3ee7bf682b292e11e2f7242823ff31.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;概率密度函数&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;$X \in [x_1, x_2]$ 的概率为 $\int_{x_1}^{x_2} f(x) dx$&lt;/li&gt;
&lt;li&gt;概率密度函数包含的整个区域面积为 $\int_{-\infty}^{\infty} f(x)dx = 1$&lt;/li&gt;
&lt;li&gt;$g(X)$ 的期望为 $E[g(X)] = \int_{-\infty}^{\infty} g(x)f(x) dx$&lt;/li&gt;
&lt;li&gt;$X$ 的均值为 $\mu = E[X] = \int_{-\infty}^{\infty}x f(x)dx$，方差为 $\sigma^2 = E[(X - \mu)^2] = E[X^2] - \mu^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设我们使用 0-1 损失函数，在连续概率分布的情况下，此时贝叶斯决策规则为&lt;/p&gt;
$$
r^* (x) = \begin{cases}
1 &amp; \text{if } f_{X | Y = 1}(x)P(Y = 1) &gt; f_{X | Y = -1}(x)P(Y = -1) \\
-1 &amp; \text{otherwise}
\end{cases}
$$&lt;p&gt;上式表示，在观测到数据 $x$ 时，如果它有更多的可能为类别 1 则被判别为类别 1，反之不为类别 1&lt;/p&gt;
&lt;p&gt;我们可以画出 $f_{X | Y = 1}(x)P(Y = 1)$ 和 $f_{X | Y = -1}(x)P(Y = -1)$ 的概率密度曲线&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_06_decision_theory/872da1136c31970df2a4dc704d3e39fe.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;贝叶斯最优决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;贝叶斯决策规则对应了一个&lt;em&gt;贝叶斯最优决策边界&lt;/em&gt;，即 $f_{X | Y = 1}(x)P(Y = 1) = f_{X | Y = -1}(x)P(Y = -1)$，它是两个概率密度函数的交点。在贝叶斯最优边界左边的点 $x$ 会被判别为类别 1，右边的点 $x$ 会被判别为类别 -1&lt;/p&gt;
&lt;p&gt;贝叶斯最优决策边界是两个后验概率密度函数相交的边界，在这个边界上两者的概率密度相等，所以贝叶斯最优决策边界为 $\{ x: P(Y = 1 | X = x) = 0.5 \}$&lt;/p&gt;
&lt;p&gt;在连续概率分布下，风险函数可以写成&lt;/p&gt;
$$
\begin{align*}
R(r) &amp;= E[L(r(X), Y)] \\
&amp;= P(Y = 1) \int L(r(x), 1) f_{X | Y = 1}(x) dx + \\
&amp; \quad P(Y = -1) \int L(r(x), -1) f_{X | Y = -1}(x) dx
\end{align*}
$$&lt;p&gt;在 $r$ 为贝叶斯决策规则 $r^*$ 的条件下，贝叶斯风险就是两个概率密度函数重合部分的面积
&lt;/p&gt;
$$R(r^*) = \int \min_{y = \pm 1} L(-y, y) f_{X |Y=y}(x) P(Y = y) dx$$&lt;h2 id=&#34;构建分类器的三种方法&#34;&gt;构建分类器的三种方法
&lt;/h2&gt;&lt;p&gt;现在我们可以总结出三种构建分类器的方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;生成模型 Generative models（例如LDA）
&lt;ul&gt;
&lt;li&gt;假设样本点来自特定的概率分布，并且不同类别的分布不同&lt;/li&gt;
&lt;li&gt;猜测概率分布的形式&lt;/li&gt;
&lt;li&gt;对每个类别，给定了概率分布的形式 $f_{X | Y = C}(x)$， 使得概率分布的参数拟合类别 C 的点&lt;/li&gt;
&lt;li&gt;对每个类别 C 估计 $P(Y = C)$&lt;/li&gt;
&lt;li&gt;根据贝叶斯定理计算 $P(Y | C)$&lt;/li&gt;
&lt;li&gt;根据后验概率的大小，得出最终的类别，即使得 $f_{X|Y=C}(x)P(Y = C)$ 的 C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;判别模型 Discriminative models（例如 logistic 回归）
&lt;ul&gt;
&lt;li&gt;直接对 $P(Y|X)$ 建模&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;寻找决策边界（例如 SVM）
&lt;ul&gt;
&lt;li&gt;直接对 $r(x)$ 建模&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相较于方法 3，方法 1 和 2 都可以直接给出估计正确的概率。相较于方法 2，方法 1 一般在有异常值和少量训练样本下更加稳定，但是现实问题中很难准确估计概率分布&lt;/p&gt;
&lt;p&gt;如果数据服从高斯分布或者一个特定的概率分布，一般使用生成模型效果更稳定&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 05 机器学习抽象和数值优化</title>
        <link>https://zhiim.github.io/p/ml_05_machine_learning_abstractions_and_numerical_optimization/</link>
        <pubDate>Fri, 23 May 2025 14:33:10 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_05_machine_learning_abstractions_and_numerical_optimization/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 05 机器学习抽象和数值优化" /&gt;&lt;h2 id=&#34;机器学习问题的抽象&#34;&gt;机器学习问题的抽象
&lt;/h2&gt;&lt;p&gt;机器学习问题可以抽象成数据、模型、优化问题、优化算法四个层级&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;数据&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;数据是否有标签？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是：标签是类别（分类）还是数值（回归）&lt;/li&gt;
&lt;li&gt;否：相似性（聚类）还是定位（降维）&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;决策函数选择：线性、多项式、逻辑、神经网络，&amp;hellip;&lt;/li&gt;
&lt;li&gt;最近邻，决策树&lt;/li&gt;
&lt;li&gt;特征选择&lt;/li&gt;
&lt;li&gt;低容量 vs 高容量考虑（影响过拟合、欠拟合、推断）&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;最优化问题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;变量、目标函数、约束条件&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;最优化算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;梯度下降、单纯形、SVD&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;最优化问题&#34;&gt;最优化问题
&lt;/h2&gt;&lt;h3 id=&#34;无约束问题&#34;&gt;无约束问题
&lt;/h3&gt;&lt;p&gt;目标：寻找一个 $w$ 最小化一个连续的目标函数 $f(w)$&lt;/p&gt;
&lt;p&gt;如果 $f$ 的导数也是连续的则称 $f$ 是&lt;em&gt;光滑&lt;/em&gt;的&lt;/p&gt;
&lt;p&gt;&lt;em&gt;全局最小值&lt;/em&gt;：对于任意 $v$，有 $f(w) \le f(v)$&lt;br&gt;
&lt;em&gt;局部最小值&lt;/em&gt;：一个 $w$ 附近的区间内，对于任意 $v$，有 $f(w)\le f(v)$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_05_machine_learning_abstractions_and_numerical_optimization/bd8dc79dab48cd26c8f44288954b173a.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;全局最小值和局部最小值&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;凸函数&lt;/em&gt;：函数 $f$ 是凸的，如果对于每个 $x, y \in \mathbb{R}^d$，连接 $(x, f(x))$ 和 $(y, f(y))$ 的切线在函数 $f$ 之上。&lt;br&gt;
也可以表示为：对于任意 $x, y \in \mathbb{R}^d$，以及 $\beta \in [0, 1]$，有 $f(x + \beta(y - x)) \le f(x) + \beta (f(y) - f(x))$&lt;/p&gt;
&lt;p&gt;凸函数的和也是凸函数。感知机的风险函数是凸损失函数的和，所以它也是凸函数。&lt;/p&gt;
&lt;p&gt;一个连续函数如果在封闭的凸定义域内，则&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有最小值（趋近于 $-\infty$）&lt;/li&gt;
&lt;li&gt;只有一个最小值&lt;/li&gt;
&lt;li&gt;一个连通的最小值集合，且值相等（一个线段或者平面）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于后两种情况，我们可以通过梯度下降法寻找最小值&lt;/p&gt;
&lt;p&gt;很多时候我们没法找到一个凸目标函数，所以梯度下降只能找到一个局部最小值而不是全局最小值。但是在很多情形下，例如在训练神经网络时，随机梯度下降仍然是一个比较好的选择&lt;/p&gt;
&lt;h3 id=&#34;线性规划&#34;&gt;线性规划
&lt;/h3&gt;&lt;p&gt;线性目标函数 + 线性约束函数&lt;/p&gt;
&lt;p&gt;目标：寻找 $w$ 最大化（或者最小化）$c \cdot w$，并满足 $Aw \le b$&lt;br&gt;
其中 $A$ 是一个 $n \times d$ 维度的矩阵，$b \in \mathbb{R}^n$。所以上述不等式包含了 $n$ 个线性约束&lt;/p&gt;
$$A_i \cdot w \le b_i, \quad i \in [1, n]$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_05_machine_learning_abstractions_and_numerical_optimization/dc81dbe98ddeaa023a6b39498a59a4f0.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;线性规划&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;满足所有约束条件的 $w$ 的集合是一个&lt;em&gt;凸多面体&lt;/em&gt;，被称为&lt;em&gt;可行域&lt;/em&gt;$F$。优化过程是在 $F$ 中寻找一个沿方向 $c$ 最远的点&lt;/p&gt;
&lt;p&gt;&lt;em&gt;凸多面体&lt;/em&gt;：对于任意 $p,q \in P$，连接 $p,q$ 的线段完全在 $P$ 内&lt;/p&gt;
&lt;p&gt;在线性规划中，最优点通常位于边界或者顶点上，所以某些等式形式的约束被称为&lt;em&gt;积极约束&lt;/em&gt;，积极约束对应的对偶变量通常非零。&lt;/p&gt;
&lt;p&gt;在支持向量机中，在对偶问题中如果 $a_i &gt; 0$，则原始问题中 $y_i (w^T x_i + b) = 1 - \xi_i$，所以支持向量在活跃约束上&lt;/p&gt;
&lt;p&gt;线性规划最著名的算法是&lt;em&gt;单纯性算法&lt;/em&gt;（simplex algorithm）&lt;/p&gt;
&lt;h3 id=&#34;二次规划&#34;&gt;二次规划
&lt;/h3&gt;&lt;p&gt;二次凸目标函数 + 线性不等式约束&lt;/p&gt;
&lt;p&gt;目标：寻找 $w$ 最小化 $f(w) = w^T Q w + c^T w$，并满足 $Aw \le b$&lt;br&gt;
其中 $Q$ 是一个对称半正定矩阵&lt;/p&gt;
&lt;p&gt;如果 $Q$ 是一个&lt;em&gt;正定矩阵&lt;/em&gt;，目标函数是严格凸的，那么二次规划只有一个唯一的全局最优解（如果有解的话）。如果 $Q$ 是一个&lt;em&gt;半正定矩阵&lt;/em&gt;，目标函数是凸的，但不一定是严格凸的，可能有多个目标函数值相同的全局最优解。如果 $Q$ 是非正定的，目标函数是非凸的，可能存在多个局部最优解，求解全局最优解通常是 NP-hard&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 04 支持向量机</title>
        <link>https://zhiim.github.io/p/ml_04_support_vector_machine/</link>
        <pubDate>Wed, 21 May 2025 12:41:59 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_04_support_vector_machine/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 04 支持向量机" /&gt;&lt;h2 id=&#34;软间隔支持向量机&#34;&gt;软间隔支持向量机
&lt;/h2&gt;&lt;p&gt;硬间隔 SVM 存在两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果数据不是线性可分的，则算法将会失效；&lt;/li&gt;
&lt;li&gt;对极端值（outliers）敏感，例如右图添加了一个极端值，严重改变了决策边界。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/2f171bd2708ce6c0d926e02aabfacc60.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;异常值造成了决策边界的偏移&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;软间隔支持向量机&lt;/em&gt;：通过引入&lt;em&gt;松弛变量&lt;/em&gt;，允许一些样本点违背最小间隔的约束，此时约束条件可以变为&lt;/p&gt;
$$y_i (X_i \cdot w + \alpha) \ge 1 - \xi_i$$&lt;p&gt;其中 $\xi_i$ 为引入的松弛变量，满足 $\xi_i \ge 0$。只有当样本点违背最小间隔约束时，松弛变量 $\xi_i$ 不为 $0$。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/f3ea99726ceb561f856e542debef3c17.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;松弛变量存在的情况&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;此时，仍然定义间隔为 $1 / \|w\|$，为了防止松弛变量的滥用，我们在目标函数中添加一个损失项对其进行约束&lt;/p&gt;
$$
\begin{align*}
&amp; \text{Find } w, \alpha, \xi_i \text{ that minimize } \|w\|^2 + C\sum_{i = 1}^n\xi_i \\
&amp; \begin{aligned}
\text{subject to } &amp; \quad  y_i(X_i \cdot w + \alpha) \ge 1 - \xi_i &amp; \text{for all } i \in [1, n] \\
&amp; \quad \xi_i \ge 0 &amp; \text{for all } i \in [1, n]
\end{aligned}
\end{align*}
$$&lt;p&gt;这是一个 $d + n + 1$ 维空间的、具有 $2n$ 个约束项的二次规划问题。其中 $C &gt; 0$ 是一个&lt;em&gt;正则化超参数&lt;/em&gt;（regularization hyperparameter）。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;较小C&lt;/th&gt;
          &lt;th&gt;较大C&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;最大化间隔 ${} 1 / \|w\|$&lt;/td&gt;
          &lt;td&gt;保持大多数松弛变量为零或很小&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;风险&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;欠拟合（误分类许多训练数据）&lt;/td&gt;
          &lt;td&gt;过拟合（训练效果好，测试效果差）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;异常值&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;不太敏感&lt;/td&gt;
          &lt;td&gt;非常敏感&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;边界（非线性时）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;更“平坦”&lt;/td&gt;
          &lt;td&gt;更曲折&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/8d7ca67f5ea4176993da2f0615155dae.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同C值的决策边界，右下方C更大&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;特征与非线性&#34;&gt;特征与非线性
&lt;/h2&gt;&lt;p&gt;非线性决策边界：&lt;strong&gt;通过创建非线性特征将样本点提升到高维空间，那么高维的线性分类器等价于低维的非线性分类器&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;样例-1抛物线提升映射the-parabolic-lifting-map&#34;&gt;样例 1：抛物线提升映射（The parabolic lifting map）
&lt;/h3&gt;&lt;p&gt;定义一个非线性映射，将 $d$ 维空间的点 $x$ 提升为 $d + 1$ 维空间的抛物面&lt;/p&gt;
$$
\begin{align*}
&amp; \Phi: \mathbb{R}^d \rightarrow \mathbb{R}^{d + 1} \\
&amp; \Phi(x) = \begin{bmatrix}
x \\ \|x\|^2
\end{bmatrix}
\end{align*}
$$&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/851322ecf7bca26253ddb6646d6d3ab3.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;点被提升到抛物面&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;上图中样本点由二维空间的点，经过非线性映射被提升到三维空间的抛物面。此时二维平面的球形决策边界，等价于三维平面的线性决策边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：$\Phi(X_1),\Phi(X_2),\dots,\Phi(X_n)$ 线性可分 $\leftrightarrow$ $X_1,X_2,\dots,X_n$ 可以被超球面分离&lt;/p&gt;
&lt;p&gt;证明：考虑 $\mathbb{R}^d$ 中的超球面，它的球心为 $c$，半径为 $\rho$。$x$ 在超球面内当且仅当&lt;/p&gt;
$$
\begin{align*}
&amp; \|x-c\|^2 &lt; \rho^2 \\
&amp; \|x\|^2 - 2c \cdot x + \|c\|^2 &lt; \rho^2 \\
&amp; \begin{bmatrix}
-2c^T &amp; 1
\end{bmatrix} \begin{bmatrix}
x \\ \|x\|^2
\end{bmatrix} &lt; \rho^2 - \|c\|^2
\end{align*}
$$&lt;p&gt;其中 $\begin{bmatrix}-2c^T &amp; 1\end{bmatrix}$ 是 $\mathbb{R}^{d + 1}$ 中的法向量，而 $\Phi(x)$ 表示 $\mathbb{R}^{d + 1}$ 中的一个点，所以 $\mathbb{R}^{d}$ 中的超球面内等价于 $\mathbb{R}^{d + 1}$ 中的超平面以下。&lt;/p&gt;
&lt;h3 id=&#34;样例-2椭球体双曲面抛物面决策边界&#34;&gt;样例 2：椭球体/双曲面/抛物面决策边界
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/354198eae8587f9626e0af64267a4c63.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同的二次曲面&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;对于式
&lt;/p&gt;
$$A x_1^2 + B x_2^2 + C x_3^2 + D x_1x_2 + E x_2x_3 + F x_3x_1 + G x_1 + H x_2 + I x_3 + \alpha = 0$$&lt;p&gt;
可以表示上图中的任意一个二次曲面。&lt;/p&gt;
&lt;p&gt;所以，我们可以定义非线性映射&lt;/p&gt;
$$
\begin{align*}
&amp; \Phi(x) = \begin{bmatrix}
x_1^2 &amp; x_2^2 &amp; x_3^2 &amp; x_1x_2 &amp; x_2x_3 &amp; x_3x_1 &amp; x_1 &amp; x_2 &amp; x_3
\end{bmatrix}^T \\
&amp; \begin{bmatrix}
A &amp; B &amp; C &amp; D &amp; E &amp; F &amp; G &amp; H &amp; I
\end{bmatrix} \cdot \Phi(x) + \alpha
\end{align*}
$$&lt;p&gt;此时，决策函数可以为任意二次多项式，决策超曲面可以为任意二次曲面。$\Phi-$ 空间的线性决策边界等价于 $x-$ 空间的二次曲面分类边界。&lt;/p&gt;
&lt;h3 id=&#34;样例-3-次多项式作为决策函数&#34;&gt;样例 3：$p$ 次多项式作为决策函数
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/3e4fe1b91d90ca7b5db1e13b5952e64c.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;拥有不同次方的决策函数的硬支持向量机：1，2，5&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;增加决策函数的最高次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性不可分的数据可能由于最够高的非线性变为线性可分&lt;/li&gt;
&lt;li&gt;高维提供了更高的自由度，可能找到更大的间隔，可能提升决策边界的鲁棒性&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;核技巧&#34;&gt;核技巧
&lt;/h2&gt;&lt;h3 id=&#34;支持向量机的学习算法&#34;&gt;支持向量机的学习算法
&lt;/h3&gt;&lt;p&gt;在训练软间隔支持向量机时，我们可以使用如下学习算法（基于&lt;em&gt;拉格朗日对偶性&lt;/em&gt;，推过过程参考李航-《统计学习方法》）：&lt;/p&gt;
&lt;p&gt;输入：训练数据集 $T={(x_1,y_1),(x_2, y_2),\dots,(x_n,y_n)}$，其中 $x_i \in \mathbb{R}^{d}, y_i \in \{-1,1\}$&lt;br&gt;
输出：决策超平面和决策函数&lt;br&gt;
（1）选择一个正则化超参数 $C &gt; 0$, 求解以下凸二次规划问题（软间隔支持向量机的原始问题的对偶问题）：&lt;/p&gt;
$$
\begin{align*}
&amp; \min_{\alpha} \frac{1}{2} \sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i = 1}^n \alpha_i \\
&amp;\begin{aligned} \text{s.t. }
&amp; \sum_{i = 1}^n \alpha_i y_i = 0 \\
&amp; 0 \le \alpha_i \le C, \quad i = 1,2,\dots,n
\end{aligned}
\end{align*}
$$&lt;p&gt;求解最优解 $\alpha^* = (\alpha_1^*, \alpha_2^*, \dots, \alpha_n^*)$&lt;br&gt;
（2）计算 $w^* = \sum_{i = 1}^n \alpha_i^* y_i x_i$&lt;br&gt;
选择一个下标 $j$，满足 $0&lt; \alpha_j^* &lt; C$，计算&lt;/p&gt;
$$b^* = y_j - \sum_{i = 1}^n y_i \alpha_i^*(x_i \cdot x_j)$$&lt;p&gt;（3）求得决策超球面
&lt;/p&gt;
$$w^* \cdot x + b^* = 0$$&lt;p&gt;
以及决策函数&lt;/p&gt;
$$f(x) = w^* \cdot x + b^* = \sum_{i = 1}^n \alpha_i^* y_i(x \cdot  x_i) + b^*$$&lt;p&gt;在上述算法中，$w^*$ 和 $b^*$ 只依赖于 $\alpha_i^* &gt; 0$ 的样本点 $x_i$，称这些样本点为&lt;em&gt;支持向量&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;可以看出在支持向量机学习算法中，无论优化的目标函数还是决策函数都只涉及两个向量之间的点积，例如 $x_i \cdot x_j$ 和 $x \cdot x_i$。&lt;/p&gt;
&lt;h3 id=&#34;核函数&#34;&gt;核函数
&lt;/h3&gt;&lt;p&gt;我们已经知道，低维空间的非线性决策曲面可以等价为高维空间的线性决策平面。&lt;/p&gt;
&lt;p&gt;但是如果特征的维度 $d$ 非常高，此时使用非线性映射 $\Phi(x)$ 得到的高次多项式特征维度非常大，$\Phi(x_i) \cdot \Phi(x_j)$ 的计算复杂度非常大。&lt;/p&gt;
&lt;p&gt;于是，我们希望找到一个低维空间&lt;em&gt;核函数&lt;/em&gt;，等价于高维空间的点积运算&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设 $\mathcal{X}$ 是一个低维输入空间，$\mathcal{H}$ 是一个高维特征空间，如果存在一个映射
&lt;/p&gt;
$$\Phi(x): \mathcal{X} \rightarrow \mathcal{H}$$&lt;p&gt;
使得对所有的 $x, z \in \mathcal{X}$，函数 $K(x, z)$ 满足
&lt;/p&gt;
$$K(x, z) = \Phi(x) \cdot \Phi(z)$$&lt;p&gt;
则称 $K(x, z)$ 为核函数。&lt;/p&gt;
&lt;p&gt;如果我们不显示指定一个非线性映射 $\Phi(x)$，而是使用一个特定的核函数 $K(x, z)$ 替代高维空间的点积 $\Phi(x_i) \cdot \Phi(x_j)$ ，那么支持向量机的优化目标可以写成&lt;/p&gt;
$$
\begin{align*}
&amp; \min_{\alpha} \frac{1}{2} \sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_j y_i y_j K(x_i, x_j) - \sum_{i = 1}^n \alpha_i \\
&amp;\begin{aligned} \text{s.t. }
&amp; \sum_{i = 1}^n \alpha_i y_i = 0 \\
&amp; 0 \le \alpha_i \le C, \quad i = 1,2,\dots,n
\end{aligned}
\end{align*}
$$&lt;p&gt;此时我们可以在核函数 $K(x, z)$ 对应的高维特征空间 $\mathcal{H}$ 中寻找一个线性决策超平面，但是参数学习是在低维输入空间 $\mathcal{X}$ 进行的，这称为支持向量机的&lt;em&gt;核技巧&lt;/em&gt;。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 03 感知机 II：感知机学习和最大间隔分类器</title>
        <link>https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/</link>
        <pubDate>Mon, 19 May 2025 16:29:23 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 03 感知机 II：感知机学习和最大间隔分类器" /&gt;&lt;h2 id=&#34;感知机算法&#34;&gt;感知机算法
&lt;/h2&gt;&lt;p&gt;在 &lt;a class=&#34;link&#34; href=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/#%e6%84%9f%e7%9f%a5%e6%9c%ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器学习 02 线性分类器和感知机——感知机&lt;/a&gt;中，我们将在 $x-$ 空间寻找分类超平面的问题转换为在 $w-$ 空间寻找一个最优点 $w$ 的问题。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;$x$-space&lt;/th&gt;
          &lt;th&gt;$w$-space&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;hyperplane: $\{ x : w \cdot x = 0 \}$&lt;/td&gt;
          &lt;td&gt;point: $w$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;point: $x$&lt;/td&gt;
          &lt;td&gt;hyperplane: $\{ z : w \cdot z = 0 \}$&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;在 $x-$ 空间的超平面被转换为在 $w-$ 空间的一个点，这个点是超平面的法线&lt;/li&gt;
&lt;li&gt;在 $x-$ 空间的样本点被转换为在 $w-$ 空间的超平面，这个超平面的法线为样本点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果我们需要找到的 $w$ 满足：当 $x$ 为类别 $C$ 时，$x \cdot w \ge 0$，那么&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 $x-$ 空间，存在一个超平面，分离了不同类别的点。所有属于类别 $C$ 的点与 $w$ 在超平面的同侧；所有不属于类别 $C$ 的点与 $w$ 在超平面的不同侧。&lt;/li&gt;
&lt;li&gt;在 $w-$ 空间，每一个点 $X_i$ 对应了 $w-$ 空间一个超平面 $H_i$。如果 ${} X_i \in \text{class C}$，那么 $w$ 与 $X_i$ 在 $H_i$ 的同侧；反之，在不同侧。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/53660548c2812635c4ce367acba8da3b.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;在左图中，$x-$ 空间内寻找一个超平面，可以将蓝色的样本点 $C$ （类别为 $C$ 的点）和红色的样本点 $X$ （类别不为 $C$ 的点）正确分类，$w$ 是这个超平面的法线。并且 $w$ 和样本点 $C$ 处于同侧。&lt;/li&gt;
&lt;li&gt;在右图中，每一个样本点在 $w-$ 空间对应一个超平面，$w$ 需要处于红色超平面的反侧，处于蓝色超平面的正侧，即图中阴影区。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;现在我们需要在 $w-$ 空间中找出最优的 $w$，使其最小化 $R(w)$ 。&lt;/p&gt;
&lt;p&gt;优化算法：对风险函数 $R(w)$ &lt;em&gt;梯度下降&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先初始化 $w$ 的值，然后计算 $R(w)$ 关于 $w$ 的梯度（即 $R(w)$ 增长最快的方向），然后在相反的方向移动一步。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;a class=&#34;link&#34; href=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/#%e6%84%9f%e7%9f%a5%e6%9c%ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器学习 02 线性分类器和感知机——感知机&lt;/a&gt; 中定义的风险函数&lt;/p&gt;
$$R(w) = \frac{1}{n} \sum_{i = 1}^n L(X_i \cdot w, y_i) = \frac{1}{n} \sum_{i = 1}^n -y_iX_i \cdot w$$&lt;p&gt;计算其梯度为&lt;/p&gt;
$$
\nabla R(w) = \begin{bmatrix}
\frac{\partial R}{\partial w_1} \\
\frac{\partial R}{\partial w_2} \\
\vdots \\
\frac{\partial R}{\partial w_d}
\end{bmatrix} = \nabla \sum_{i \in V}-y_i X_i \cdot w = -\sum_{i\in V}y_i X_i
$$&lt;p&gt;其中 $V$ 表示所有满足 $y_i X_i \cdot w &lt; 0$ 的下标 $i$ 的集合。&lt;/p&gt;
&lt;p&gt;对 $R(w)$ 的梯度下降可以表示为&lt;/p&gt;
$$
\begin{align*}
&amp; w \leftarrow \text{任意非零初始化} \\
&amp; \text{while } R(w) &gt; 0 \\
&amp; \quad \quad V \leftarrow \text{所有满足 } y_i X_i \cdot w &lt; 0 \text{ 的下标 } i \text{ 的集合} \\
&amp; \quad \quad w \leftarrow w + \varepsilon \sum_{i \in V} y_i X_i \\
&amp; \text{return } w
\end{align*}
$$&lt;p&gt;其中 $\varepsilon$ 是&lt;em&gt;学习率&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;梯度下降的每一步复杂度为 $O(nd)$，我们可以通过&lt;em&gt;随机梯度下降&lt;/em&gt;提升算法性能。&lt;/p&gt;
&lt;p&gt;随机梯度下降的核心思想是在每一步的梯度下降中，随机选择一个满足条件的下标 $i$，并使用损失函数 $L(X_i \cdot w, y_i)$ 的梯度，该方法被称为&lt;em&gt;感知机算法&lt;/em&gt;，其每一步的计算复杂度为 $O(d)$。&lt;/p&gt;
$$
\begin{align*}
&amp; \text{while } \text{找到某一个 } i \text{ 满足 } y_i X_i \cdot w &lt; 0 \\
&amp; \quad \quad w \leftarrow x + \varepsilon y_i X_i \\
&amp; \text{return } w
\end{align*}
$$&lt;p&gt;&lt;strong&gt;虽然随机梯度下降法非常高效，但是并不是所有梯度下降法可以解决的问题都可以使用随机梯度下降。&lt;/strong&gt; 如果数据线性可分，感知机算法一定可以找到全局最优，随机梯度法一定可行。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;如果我们需要找到不经过原点的超平面，此时只需要额外增加一个维度用于表示 $\alpha$&lt;/p&gt;
$$
f(x) = w \cdot x + \alpha = \begin{bmatrix}
w_1 &amp; w_2 &amp; \alpha
\end{bmatrix} \cdot \begin{bmatrix}
x_1 \\ x_2 \\ 1
\end{bmatrix}
$$&lt;p&gt;此时我们的每个样本数据点存在于实数空间 $\mathbb{R}^{d + 1}$，其中最后一个维度的特征 $x_{d+1} = 1$。仍然可以在 $d + 1$ 维空间中使用感知机算法找到解。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;感知机收敛理论&lt;/em&gt;：如果数据是线性可分的，感知机算法可以在最多 $O(r^2/\gamma^2)$ 的迭代中找到一个可以将所有数据正确分类的分类器。其中 $r=\max{\|X_i\|}$ 是数据的半径，$\gamma$ 是最大间隔。&lt;/p&gt;
&lt;h2 id=&#34;最大边界分类器&#34;&gt;最大边界分类器
&lt;/h2&gt;&lt;p&gt;一个线性分类器的&lt;em&gt;间隔&lt;/em&gt;是超平面到最近的一个训练样本点的距离。如果我们可以找到一个超平面，它的间隔最大，那么这个分类器最优（决策边界离所有的点距离最远，最不容易分类错误）。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/1fde55281481c2de74c3cd3b96b57416.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;如图，一个满足上述条件的超平面可以表示为 $w \cdot x + \alpha = 0$，那么平行于这个超平面的、经过最近的样本点的超平面可以表示为 $|w \cdot x + \alpha| = 1$（$1$ 只是为了方便讨论，也可以为 $w \cdot x + \alpha = k$，对 $k$ 归一化后也为 $1$）。&lt;/p&gt;
&lt;p&gt;由于所有的样本点都在这两个最近超平面之外，所有样本点满足
&lt;/p&gt;
$$w \cdot X_i + \alpha \ge 1 \quad \text{for } i \in [1, n]$$&lt;p&gt;
由于标签 $y_i \in \{1, -1\}$，约束条件可以表示成
&lt;/p&gt;
$$y_i(w \cdot X_i + \alpha) \ge 1 \quad \text{for } i \in [1, n]$$&lt;p&gt;已知，如果 $\|w\| = 1$，那么 $X_i$ 到超平面的有符号距离为 $w \cdot X_i + \alpha$。所以我们对 $w$ 归一化，可得
&lt;/p&gt;
$$\frac{1}{\|w\|}|w \cdot X_i + \alpha| \ge \frac{1}{\|w\|}$$&lt;p&gt;
即间隔为 $\frac{1}{\|w\|}$，为了使间隔最大化，需要最小化 $\|w\|$。&lt;/p&gt;
&lt;p&gt;于是可以求解优化问题&lt;/p&gt;
$$
\begin{align*}
&amp; \text{找到 } w, \alpha \text{ 最小化 } \|w\|^2 \\
&amp; \text{同时满足 } y_i (X_i \cdot w + \alpha) \ge 1 \quad \text{for all } i \in [1, n]
\end{align*}
$$&lt;p&gt;上述问题被称为在 $d + 1$ 维空间中，有 $n$ 个约束的的二次规划（quadratic program）。如果所有样本点线性可分，它有唯一解，反之无解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最小化 $\|w\|^2$ 而不是 $\|w\|$ 的原因：$\|w\|$ 在 0 处不平滑。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上述方法可以得到一个&lt;em&gt;最大间隔分类器&lt;/em&gt;，即&lt;em&gt;硬间隔支持向量机&lt;/em&gt;（hard-margin support vector machine， SVM）。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/eb9976478a95b52b81156b59409096c8.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图是 $(w_1, w_2, \alpha)$ 的三维空间。类别 $C$ 的样本点为法线的超平面为绿色，非类别 $C$ 的样本点为法线的超平面为红色。图中为 3 个样本点的情况。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了满足约束条件 $y_i (X_i \cdot w + \alpha) \ge 1$，$w, \alpha$ 存在于绿色超平面之上，红色超平面之下&lt;/li&gt;
&lt;li&gt;为了满足 $\|w\|^2$ 最小，$w, \alpha$ 应该最接近 $\alpha$ 轴&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;和感知机算法类似，只有所有数据点线性可分时，硬边界 SVM 可行。&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 02 感知机 I：线性分类器和感知机</title>
        <link>https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/</link>
        <pubDate>Sun, 18 May 2025 16:32:02 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 02 感知机 I：线性分类器和感知机" /&gt;&lt;h2 id=&#34;分类器&#34;&gt;分类器
&lt;/h2&gt;&lt;p&gt;假设我们有 $n$ 个观测样例（bofservations），每个观测样例有 $d$ 个特征（features），有些样例属于类别 C，有些不是&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/decision_boudaries.webp&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同的决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- ![决策边界](decision_boudaries.webp) --&gt;
&lt;p&gt;&lt;em&gt;决策边界&lt;/em&gt;：分类器选择的可以将样例分成不同类别的边界&lt;/p&gt;
&lt;p&gt;&lt;em&gt;过拟合&lt;/em&gt;：决策边界拟合得太好，以至于无法用于未来样本的分类&lt;/p&gt;
&lt;p&gt;&lt;em&gt;决策函数&lt;/em&gt;：将一个点映射到一个值，例如&lt;/p&gt;
$$
\begin{matrix}
f(x) &gt; 0 &amp; &amp; \text{if } x \in \text{class } C; \\
f(x) \le 0 &amp; &amp; \text{if } x \notin \text{class } C;
\end{matrix}
$$&lt;p&gt;则分类边界为 $\{x\in \mathbb{R}^{d}: f(x)=0\}$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/dc_2.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;分类边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;linear-classifier-math&#34;&gt;Linear Classifier Math
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;内积&lt;/em&gt;：$x\cdot y = x_1y_1+x_2y_2+\dots + x_dy_d$，也可以写成 $x^Ty$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;欧拉范数&lt;/em&gt;：$\|x\|=\sqrt{x\cdot x}=\sqrt{x_1^2 + x_2^2 + \dots + x_d^2}$。可以表示向量 $x$ 的欧拉长度 (Euclidean length)。&lt;/p&gt;
&lt;p&gt;归一化向量 $x$： ${x}/{\|x\|}$&lt;/p&gt;
&lt;p&gt;给定一个线性决策函数 $f(x) = w \cdot x + \alpha$，决策边界为 $H = \{x: w \cdot x = -\alpha\}$。集合 $H$ 被称为&lt;em&gt;超平面&lt;/em&gt;（hyperplane）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超平面是 $d-1$ 维，并且分割了一个 $d$ 维空间&lt;/li&gt;
&lt;li&gt;它是无限且平展的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$w$ 和超平面 $H$ 上的任何直线正交。所以 $w$ 被称为 $H$ 的&lt;em&gt;法线&lt;/em&gt;（normal vector）。如果 $w$ 是一个单位向量，那么 $f(x) = w \cdot x + \alpha$ 是 $x$ 到 $H$ 的有符号距离（singed distance）。此外，$H$ 到原点的距离为 $\alpha$。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/normal_vector.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;平面的法线&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;如果存在一个超平面可以将所有的训练样本点分类，则它们&lt;em&gt;线性可分&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;一个简单的分类器&#34;&gt;一个简单的分类器
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;质心法&lt;/em&gt;（Centroid method）：首先分别计算属于类别 $C$ 的所有点的平均 $\mu_C$，以及所有不属于 $C$ 的平均 $\mu_X$。则决策函数可以表示为
&lt;/p&gt;
$$f(x) = (\mu_C - \mu_X) \cdot x - (\mu_C - \mu_X) \cdot \frac{\mu_C + \mu_X}{2}$$&lt;p&gt;质心法非常简洁，但是也只能处理简单的情况。例如下图中的数据点无法使用质心法正确分类。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/centorid_method.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;质心法失效的情况&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;感知机&#34;&gt;感知机
&lt;/h2&gt;&lt;p&gt;假设有 $n$ 个采样点数据 $X_1, X_2,\dots,X_n$。对于每个采样点，对应的&lt;em&gt;标签&lt;/em&gt;为&lt;/p&gt;
$$
y_i = \begin{cases}
\begin{matrix}
1 &amp; \text{if } X_i \in \text{class C} \\
-1 &amp; \text{if } X_i \notin \text{class C}
\end{matrix}
\end{cases}
$$&lt;p&gt;为了简化讨论，这里只考虑经过原点的决策边界。于是我们寻找决策边界的目标可以转换成寻找一个权重 $w$，使其满足&lt;/p&gt;
$$
\begin{matrix}
X_i \cdot w \ge 0 &amp; &amp; \text{if } y_i = 1 \\
X_i \cdot w \le 0 &amp; &amp; \text{if } y_i = -1
\end{matrix}
$$&lt;p&gt;等价为
&lt;/p&gt;
$$y_iX_i\cdot w \ge 0$$&lt;p&gt;
上式即为&lt;em&gt;约束&lt;/em&gt;（constraint）。&lt;/p&gt;
&lt;p&gt;为了找到最优的 $w$，我们定义一个&lt;em&gt;风险函数&lt;/em&gt;$R$，如果约束被违背，则风险函数为正值，然后通过最小化 $R$ 选择最优 $w$。&lt;/p&gt;
&lt;p&gt;首先定义&lt;em&gt;损失函数&lt;/em&gt;&lt;/p&gt;
$$
L(\hat{y}, y_i) = \begin{cases}
\begin{matrix}
0 &amp; &amp; \text{if } y_i\hat{y} \ge 0 \\
-y_i \hat{y} &amp; &amp; \text{otherwise}
\end{matrix}
\end{cases}
$$&lt;p&gt;其中 $\hat{y}$ 是分类器的预测值。&lt;/p&gt;
&lt;p&gt;于是，我们可以定义风险函数（即优化问题的目标函数）为
&lt;/p&gt;
$$R(w) = \frac{1}{n} \sum_{i = 1}^n L(X_i \cdot w, y_i) = \frac{1}{n} \sum_{i = 1}^n -y_iX_i \cdot w$$&lt;p&gt;
所以寻找超平面的问题变为一个最优化问题：寻找 $w$ ，使得 $R(w)$ 最小。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 01 引言</title>
        <link>https://zhiim.github.io/p/ml_01_introduction/</link>
        <pubDate>Sun, 18 May 2025 15:40:22 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_01_introduction/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 01 引言" /&gt;&lt;p&gt;机器学习系列是我学习 &lt;a class=&#34;link&#34; href=&#34;https://people.eecs.berkeley.edu/~jrs/189/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CS 189/289A Introduction to Machine Learning&lt;/a&gt; Spring 2025 时记录的学习笔记，也可以当作是对课程笔记的翻译，经供参考。&lt;/p&gt;
&lt;h2 id=&#34;分类问题&#34;&gt;分类问题
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;收集具有标签的训练样本&lt;/li&gt;
&lt;li&gt;预测新样本的类别&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_01_introduction/classifier.webp&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;两种不同分类器的决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;训练验证和测试&#34;&gt;训练、验证和测试
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;我们拥有一些设置好标签的数据&lt;/li&gt;
&lt;li&gt;从数据中取出一部分作为&lt;em&gt;验证集&lt;/em&gt;（通常 20），其余作为&lt;em&gt;训练集&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;训练一个或者多个分类器：训练多种不同算法，或者具有不同超参数的同一算法&lt;/li&gt;
&lt;li&gt;使用验证集选出具有最低验证误差的分类器&lt;/li&gt;
&lt;li&gt;使用新的数据测试算法的效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;训练误差&lt;/em&gt;：训练集中分类错误的比例&lt;br&gt;
&lt;em&gt;验证误差&lt;/em&gt;：验证集中分类错误的比例&lt;br&gt;
&lt;em&gt;测试误差&lt;/em&gt;：测试集中分类错误的比例&lt;/p&gt;
&lt;p&gt;大多数机器学习算法的一些超参数改变会造成欠拟合或者过拟合&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_01_introduction/overfitting.webp&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;[k邻接算法中超参数k的改变带来的影响]&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;过拟合&lt;/em&gt;：当验证/测试误差恶化时，原因是分类器对异常值或其他虚假模式变得过于敏感。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;欠拟合&lt;/em&gt;：当验证/测试误差恶化时，原因是分类器不够灵活，无法拟合模式。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;异常值&lt;/em&gt;：具有非典型标签的点（例如，富裕的借款人仍然违约）。增加过拟合的风险。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
