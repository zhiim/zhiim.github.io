<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on Zhiim&#39;s Blog</title>
        <link>https://zhiim.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on Zhiim&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 21 May 2025 12:41:59 +0800</lastBuildDate><atom:link href="https://zhiim.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习 04 支持向量机</title>
        <link>https://zhiim.github.io/p/ml_04_support_vector_machine/</link>
        <pubDate>Wed, 21 May 2025 12:41:59 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_04_support_vector_machine/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 04 支持向量机" /&gt;&lt;h2 id=&#34;软间隔支持向量机&#34;&gt;软间隔支持向量机
&lt;/h2&gt;&lt;p&gt;硬间隔 SVM 存在两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果数据不是线性可分的，则算法将会失效；&lt;/li&gt;
&lt;li&gt;对极端值（outliers）敏感，例如右图添加了一个极端值，严重改变了决策边界。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/2f171bd2708ce6c0d926e02aabfacc60.png&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;异常值造成了决策边界的偏移&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;软间隔支持向量机&lt;/em&gt;：通过引入&lt;em&gt;松弛变量&lt;/em&gt;，允许一些样本点违背最小间隔的约束，此时约束条件可以变为&lt;/p&gt;
&lt;p&gt;$$y_i (X_i \cdot w + \alpha) \ge 1 - \xi_i$$&lt;/p&gt;
&lt;p&gt;其中 $\xi_i$ 为引入的松弛变量，满足 $\xi_i \ge 0$。只有当样本点违背最小间隔约束时，松弛变量 $\xi_i$ 不为 $0$。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/f3ea99726ceb561f856e542debef3c17.png&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;松弛变量存在的情况&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;此时，仍然定义间隔为 $1 / ||w||$，为了防止松弛变量的滥用，我们在目标函数中添加一个损失项对其进行约束&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \text{Find } w, \alpha, \xi_i \text{ that minimize } ||w||^2 + C\sum_{i = 1}^n\xi_i \\
&amp;amp; \begin{aligned}
\text{subject to } &amp;amp; \quad  y_i(X_i \cdot w + \alpha) \ge 1 - \xi_i &amp;amp; \text{for all } i \in [1, n] \\
&amp;amp; \quad \xi_i \ge 0 &amp;amp; \text{for all } i \in [1, n]
\end{aligned}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;这是一个 $d + n + 1$ 维空间的、具有 $2n$ 个约束项的二次规划问题。其中 $C &amp;gt; 0$ 是一个&lt;em&gt;正则化超参数&lt;/em&gt;（regularization hyperparameter）。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;较小C&lt;/th&gt;
          &lt;th&gt;较大C&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;目标&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;最大化间隔 ${} 1 / ||w||$&lt;/td&gt;
          &lt;td&gt;保持大多数松弛变量为零或很小&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;风险&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;欠拟合（误分类许多训练数据）&lt;/td&gt;
          &lt;td&gt;过拟合（训练效果好，测试效果差）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;异常值&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;不太敏感&lt;/td&gt;
          &lt;td&gt;非常敏感&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;边界（非线性时）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;更“平坦”&lt;/td&gt;
          &lt;td&gt;更曲折&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/8d7ca67f5ea4176993da2f0615155dae.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同C值的决策边界，右下方C更大&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;特征与非线性&#34;&gt;特征与非线性
&lt;/h2&gt;&lt;p&gt;非线性决策边界：&lt;strong&gt;通过创建非线性特征将样本点提升到高维空间，那么高维的线性分类器等价于低维的非线性分类器&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;样例-1抛物线提升映射the-parabolic-lifting-map&#34;&gt;样例 1：抛物线提升映射（The parabolic lifting map）
&lt;/h3&gt;&lt;p&gt;定义一个非线性映射，将 $d$ 维空间的点 $x$ 提升为 $d + 1$ 维空间的抛物面&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \Phi: \mathbb{R}^d \rightarrow \mathbb{R}^{d + 1} \\
&amp;amp; \Phi(x) = \begin{bmatrix}
x \\ ||x||^2
\end{bmatrix}
\end{align*}
$$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/851322ecf7bca26253ddb6646d6d3ab3.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;点被提升到抛物面&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;上图中样本点由二维空间的点，经过非线性映射被提升到三维空间的抛物面。此时二维平面的球形决策边界，等价于三维平面的线性决策边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：$\Phi(X_1),\Phi(X_2),\dots,\Phi(X_n)$ 线性可分 $\leftrightarrow$ $X_1,X_2,\dots,X_n$ 可以被超球面分离&lt;/p&gt;
&lt;p&gt;证明：考虑 $\mathbb{R}^d$ 中的超球面，它的球心为 $c$，半径为 $\rho$。$x$ 在超球面内当且仅当&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; ||x-c||^2 &amp;lt; \rho^2 \\
&amp;amp; ||x||^2 - 2c \cdot x + ||c||^2 &amp;lt; \rho^2 \\
&amp;amp; \begin{bmatrix}
-2c^T &amp;amp; 1
\end{bmatrix} \begin{bmatrix}
x \\ ||x||^2
\end{bmatrix} &amp;lt; \rho^2 - ||c||^2
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;其中 $\begin{bmatrix}-2c^T &amp;amp; 1\end{bmatrix}$ 是 $\mathbb{R}^{d + 1}$ 中的法向量，而 $\Phi(x)$ 表示 $\mathbb{R}^{d + 1}$ 中的一个点，所以 $\mathbb{R}^{d}$ 中的超球面内等价于 $\mathbb{R}^{d + 1}$ 中的超平面以下。&lt;/p&gt;
&lt;h3 id=&#34;样例-2椭球体双曲面抛物面决策边界&#34;&gt;样例 2：椭球体/双曲面/抛物面决策边界
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/354198eae8587f9626e0af64267a4c63.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同的二次曲面&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;对于式
$$A x_1^2 + B x_2^2 + C x_3^2 + D x_1x_2 + E x_2x_3 + F x_3x_1 + G x_1 + H x_2 + I x_3 + \alpha = 0$$
可以表示上图中的任意一个二次曲面。&lt;/p&gt;
&lt;p&gt;所以，我们可以定义非线性映射&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \Phi(x) = \begin{bmatrix}
x_1^2 &amp;amp; x_2^2 &amp;amp; x_3^2 &amp;amp; x_1x_2 &amp;amp; x_2x_3 &amp;amp; x_3x_1 &amp;amp; x_1 &amp;amp; x_2 &amp;amp; x_3
\end{bmatrix}^T \\
&amp;amp; \begin{bmatrix}
A &amp;amp; B &amp;amp; C &amp;amp; D &amp;amp; E &amp;amp; F &amp;amp; G &amp;amp; H &amp;amp; I
\end{bmatrix} \cdot \Phi(x) + \alpha
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;此时，决策函数可以为任意二次多项式，决策超曲面可以为任意二次曲面。$\Phi-$ 空间的线性决策边界等价于 $x-$ 空间的二次曲面分类边界。&lt;/p&gt;
&lt;h3 id=&#34;样例-3p-次多项式作为决策函数&#34;&gt;样例 3：$p$ 次多项式作为决策函数
&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_04_support_vector_machine/3e4fe1b91d90ca7b5db1e13b5952e64c.png&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;拥有不同次方的决策函数的硬支持向量机：1，2，5&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;增加决策函数的最高次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性不可分的数据可能由于最够高的非线性变为线性可分&lt;/li&gt;
&lt;li&gt;高维提供了更高的自由度，可能找到更大的间隔，可能提升决策边界的鲁棒性&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;核技巧&#34;&gt;核技巧
&lt;/h2&gt;&lt;h3 id=&#34;支持向量机的学习算法&#34;&gt;支持向量机的学习算法
&lt;/h3&gt;&lt;p&gt;在训练软间隔支持向量机时，我们可以使用如下学习算法（基于&lt;em&gt;拉格朗日对偶性&lt;/em&gt;，推过过程参考李航-《统计学习方法》）：&lt;/p&gt;
&lt;p&gt;输入：训练数据集 $T={(x_1,y_1),(x_2, y_2),\dots,(x_n,y_n)}$，其中 $x_i \in \mathbb{R}^{d}, y_i \in {-1,1}$&lt;br&gt;
输出：决策超平面和决策函数&lt;br&gt;
（1）选择一个正则化超参数 $C &amp;gt; 0$, 求解以下凸二次规划问题（软间隔支持向量机的原始问题的对偶问题）：&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \min_{\alpha} \frac{1}{2} \sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i = 1}^n \alpha_i \\
&amp;amp;\begin{aligned} \text{s.t. }
&amp;amp; \sum_{i = 1}^n \alpha_i y_i = 0 \\
&amp;amp; 0 \le \alpha_i \le C, \quad i = 1,2,\dots,n
\end{aligned}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;求解最优解 $\alpha^* = (\alpha_1^*, \alpha_2^*, \dots, \alpha_n^*)$&lt;br&gt;
（2）计算 $w^* = \sum_{i = 1}^n \alpha_i^* y_i x_i$&lt;br&gt;
选择一个下标 $j$，满足 $0&amp;lt; \alpha_j^* &amp;lt; C$，计算&lt;/p&gt;
&lt;p&gt;$$b^* = y_j - \sum_{i = 1}^n y_i \alpha_i^*(x_i \cdot x_j)$$&lt;/p&gt;
&lt;p&gt;（3）求得决策超球面
$$w^* \cdot x + b^* = 0$$
以及决策函数&lt;/p&gt;
&lt;p&gt;$$f(x) = w^* \cdot x + b^* = \sum_{i = 1}^n \alpha_i^* y_i(x \cdot  x_i) + b^*$$&lt;/p&gt;
&lt;p&gt;在上述算法中，$w^*$ 和 $b^*$ 只依赖于 $\alpha_i^* &amp;gt; 0$ 的样本点 $x_i$，称这些样本点为&lt;em&gt;支持向量&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;可以看出在支持向量机学习算法中，无论优化的目标函数还是决策函数都只涉及两个向量之间的点积，例如 $x_i \cdot x_j$ 和 $x \cdot x_i$。&lt;/p&gt;
&lt;h3 id=&#34;核函数&#34;&gt;核函数
&lt;/h3&gt;&lt;p&gt;我们已经知道，低维空间的非线性决策曲面可以等价为高维空间的线性决策平面。&lt;/p&gt;
&lt;p&gt;但是如果特征的维度 $d$ 非常高，此时使用非线性映射 $\Phi(x)$ 得到的高次多项式特征维度非常大，$\Phi(x_i) \cdot \Phi(x_j)$ 的计算复杂度非常大。&lt;/p&gt;
&lt;p&gt;于是，我们希望找到一个低维空间&lt;em&gt;核函数&lt;/em&gt;，等价于高维空间的点积运算&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设 $\mathcal{X}$ 是一个低维输入空间，$\mathcal{H}$ 是一个高维特征空间，如果存在一个映射
$$\Phi(x): \mathcal{X} \rightarrow \mathcal{H}$$
使得对所有的 $x, z \in \mathcal{X}$，函数 $K(x, z)$ 满足
$$K(x, z) = \Phi(x) \cdot \Phi(z)$$
则称 $K(x, z)$ 为核函数。&lt;/p&gt;
&lt;p&gt;如果我们不显示指定一个非线性映射 $\Phi(x)$，而是使用一个特定的核函数 $K(x, z)$ 替代高维空间的点积 $\Phi(x_i) \cdot \Phi(x_j)$ ，那么支持向量机的优化目标可以写成&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \min_{\alpha} \frac{1}{2} \sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_j y_i y_j K(x_i, x_j) - \sum_{i = 1}^n \alpha_i \\
&amp;amp;\begin{aligned} \text{s.t. }
&amp;amp; \sum_{i = 1}^n \alpha_i y_i = 0 \\
&amp;amp; 0 \le \alpha_i \le C, \quad i = 1,2,\dots,n
\end{aligned}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;此时我们可以在核函数 $K(x, z)$ 对应的高维特征空间 $\mathcal{H}$ 中寻找一个线性决策超平面，但是参数学习是在低维输入空间 $\mathcal{X}$ 进行的，这称为支持向量机的&lt;em&gt;核技巧&lt;/em&gt;。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 03 感知机学习和最大间隔分类器</title>
        <link>https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/</link>
        <pubDate>Mon, 19 May 2025 16:29:23 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 03 感知机学习和最大间隔分类器" /&gt;&lt;h2 id=&#34;感知机算法&#34;&gt;感知机算法
&lt;/h2&gt;&lt;p&gt;在 &lt;a class=&#34;link&#34; href=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/#%e6%84%9f%e7%9f%a5%e6%9c%ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器学习 02 线性分类器和感知机——感知机&lt;/a&gt;中，我们将在 $x-$ 空间寻找分类超平面的问题转换为在 ${} w- {}$ 空间寻找一个最优点 $w$ 的问题。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;$x$-space&lt;/th&gt;
          &lt;th&gt;$w$-space&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;hyperplane: $\{ x : w \cdot x = 0 \}$&lt;/td&gt;
          &lt;td&gt;point: $w$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;point: $x$&lt;/td&gt;
          &lt;td&gt;hyperplane: $\{ z : w \cdot z = 0 \}$&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;在 $x-$ 空间的超平面被转换为在 $w-$ 空间的一个点，这个点是超平面的法线&lt;/li&gt;
&lt;li&gt;在 $x-$ 空间的样本点被转换为在 $w-$ 空间的超平面，这个超平面的法线为样本点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果我们需要找到的 $w$ 满足：当 $x$ 为类别 $C$ 时，$x \cdot w \ge 0$，那么&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 $x-$ 空间，存在一个超平面，分离了不同类别的点。所有属于类别 $C$ 的点与 $w$ 在超平面的同侧；所有不属于类别 $C$ 的点与 $w$ 在超平面的不同侧。&lt;/li&gt;
&lt;li&gt;在 $w-$ 空间，每一个点 ${} X_i$ 对应了 $w-$ 空间一个超平面 $H_i$。如果 ${} X_i \in \text{class C}$，那么 $w$ 与 ${} X_i$ 在 $H_i$ 的同侧；反之，在不同侧。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/53660548c2812635c4ce367acba8da3b.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;在左图中，$x-$ 空间内寻找一个超平面，可以将蓝色的样本点 $C$ （类别为 $C$ 的点）和红色的样本点 $X$ （类别不为 $C$ 的点）正确分类，$w$ 是这个超平面的法线。并且 $w$ 和样本点 $C$ 处于同侧。&lt;/li&gt;
&lt;li&gt;在右图中，每一个样本点在 $w-$ 空间对应一个超平面，$w$ 需要处于红色超平面的反侧，处于蓝色超平面的正侧，即图中阴影区。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;现在我们需要在 $w-$ 空间中找出最优的 $w$，使其最小化 $R(w)$ 。&lt;/p&gt;
&lt;p&gt;优化算法：对风险函数 $R(w)$ &lt;em&gt;梯度下降&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先初始化 $w$ 的值，然后计算 $R(w)$ 关于 $w$ 的梯度（即 $R(w)$ 增长最快的方向），然后在相反的方向移动一步。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;a class=&#34;link&#34; href=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/#%e6%84%9f%e7%9f%a5%e6%9c%ba&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器学习 02 线性分类器和感知机——感知机&lt;/a&gt; 中定义的风险函数&lt;/p&gt;
&lt;p&gt;$$R(w) = \frac{1}{n} \sum_{i = 1}^n L(X_i \cdot w, y_i) = \frac{1}{n} \sum_{i = 1}^n -y_iX_i \cdot w$$&lt;/p&gt;
&lt;p&gt;计算其梯度为&lt;/p&gt;
&lt;p&gt;$$
\nabla R(w) = \begin{bmatrix}
\frac{\partial R}{\partial w_1} \\
\frac{\partial R}{\partial w_2} \\
\vdots \\
\frac{\partial R}{\partial w_d}
\end{bmatrix} = \nabla \sum_{i \in V}-y_i X_i \cdot w = -\sum_{i\in V}y_i X_i
$$&lt;/p&gt;
&lt;p&gt;其中 $V$ 表示所有满足 $y_i X_i \cdot w &amp;lt; 0$ 的下标 $i$ 的集合。&lt;/p&gt;
&lt;p&gt;对 $R(w)$ 的梯度下降可以表示为&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; w \leftarrow \text{任意非零初始化} \\
&amp;amp; \text{while } R(w) &amp;gt; 0 \\
&amp;amp; \quad \quad V \leftarrow \text{所有满足 } y_i X_i \cdot w &amp;lt; 0 \text{ 的下标 } i \text{ 的集合} \\
&amp;amp; \quad \quad w \leftarrow w + \varepsilon \sum_{i \in V} y_i X_i \\
&amp;amp; \text{return } w
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;其中 $\varepsilon$ 是&lt;em&gt;学习率&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;梯度下降的每一步复杂度为 $O(nd)$，我们可以通过&lt;em&gt;随机梯度下降&lt;/em&gt;提升算法性能。&lt;/p&gt;
&lt;p&gt;随机梯度下降的核心思想是在每一步的梯度下降中，随机选择一个满足条件的下标 $i$，并使用损失函数 $L(X_i \cdot w, y_i)$ 的梯度，该方法被称为&lt;em&gt;感知机算法&lt;/em&gt;，其每一步的计算复杂度为 $O(d)$。&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \text{while } \text{找到某一个 } i \text{ 满足 } y_i X_i \cdot w &amp;lt; 0 \\
&amp;amp; \quad \quad w \leftarrow x + \varepsilon y_i X_i \\
&amp;amp; \text{return } w
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;虽然随机梯度下降法非常高效，但是并不是所有梯度下降法可以解决的问题都可以使用随机梯度下降。&lt;/strong&gt; 如果数据线性可分，感知机算法一定可以找到全局最优，随机梯度法一定可行。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;如果我们需要找到不经过原点的超平面，此时只需要额外增加一个维度用于表示 $\alpha$&lt;/p&gt;
&lt;p&gt;$$
f(x) = w \cdot x + \alpha = \begin{bmatrix}
w_1 &amp;amp; w_2 &amp;amp; \alpha
\end{bmatrix} \cdot \begin{bmatrix}
x_1 \\ x_2 \\ 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;此时我们的每个样本数据点存在于实数空间 $\mathbb{R}^{d + 1}$，其中最后一个维度的特征 $x_{d+1} = 1$。仍然可以在 $d + 1$ 维空间中使用感知机算法找到解。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;感知机收敛理论&lt;/em&gt;：如果数据是线性可分的，感知机算法可以在最多 $O(r^2/\gamma^2)$ 的迭代中找到一个可以将所有数据正确分类的分类器。其中 $r=\max{||X_i||}$ 是数据的半径，$\gamma$ 是最大间隔。&lt;/p&gt;
&lt;h2 id=&#34;最大边界分类器&#34;&gt;最大边界分类器
&lt;/h2&gt;&lt;p&gt;一个线性分类器的&lt;em&gt;间隔&lt;/em&gt;是超平面到最近的一个训练样本点的距离。如果我们可以找到一个超平面，它的间隔最大，那么这个分类器最优（决策边界离所有的点距离最远，最不容易分类错误）。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/1fde55281481c2de74c3cd3b96b57416.png&#34; width=&#34;600&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;如图，一个满足上述条件的超平面可以表示为 $w \cdot x + \alpha = 0$，那么平行于这个超平面的、经过最近的样本点的超平面可以表示为 $|w \cdot x + \alpha| = 1$（$1$ 只是为了方便讨论，也可以为 $w \cdot x + \alpha = k$，对 $k$ 归一化后也为 $1$）。&lt;/p&gt;
&lt;p&gt;由于所有的样本点都在这两个最近超平面之外，所有样本点满足
$$w \cdot X_i + \alpha \ge 1 \quad \text{for } i \in [1, n]$$
由于标签 $y_i \in \{1, -1\}$，约束条件可以表示成
$$y_i(w \cdot X_i + \alpha) \ge 1 \quad \text{for } i \in [1, n]$$&lt;/p&gt;
&lt;p&gt;已知，如果 $||w|| = 1$，那么 $X_i$ 到超平面的有符号距离为 $w \cdot X_i + \alpha$。所以我们对 $w$ 归一化，可得
$$\frac{1}{||w||}|w \cdot X_i + \alpha| \ge \frac{1}{||w||}$$
即间隔为 $\frac{1}{||w||}$，为了使间隔最大化，需要最小化 $||w||$。&lt;/p&gt;
&lt;p&gt;于是可以求解优化问题&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
&amp;amp; \text{找到 } w, \alpha \text{ 最小化 } ||w||^2 \\
&amp;amp; \text{同时满足 } y_i (X_i \cdot w + \alpha) \ge 1 \quad \text{for all } i \in [1, n]
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;上述问题被称为在 $d + 1$ 维空间中，有 $n$ 个约束的的二次规划（quadratic program）。如果所有样本点线性可分，它有唯一解，反之无解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最小化 $||w||^2$ 而不是 $||w||$ 的原因：$||w||$ 在 0 处不平滑。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上述方法可以得到一个&lt;em&gt;最大间隔分类器&lt;/em&gt;，即&lt;em&gt;硬间隔支持向量机&lt;/em&gt;（hard-margin support vector machine， SVM）。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_03_perceptron_learning_and_maximum_margin_classifiers/eb9976478a95b52b81156b59409096c8.png&#34; width=&#34;800&#34;&gt;
&lt;/figure&gt;

&lt;p&gt;上图是 $(w_1, w_2, \alpha)$ 的三维空间。类别 $C$ 的样本点为法线的超平面为绿色，非类别 $C$ 的样本点为法线的超平面为红色。图中为 3 个样本点的情况。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了满足约束条件 $y_i (X_i \cdot w + \alpha) \ge 1$，$w, \alpha$ 存在于绿色超平面之上，红色超平面之下&lt;/li&gt;
&lt;li&gt;为了满足 $||w||^2$ 最小，$w, \alpha$ 应该最接近 $\alpha$ 轴&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;和感知机算法类似，只有所有数据点线性可分时，硬边界 SVM 可行。&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 02 线性分类器和感知机</title>
        <link>https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/</link>
        <pubDate>Sun, 18 May 2025 16:32:02 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 02 线性分类器和感知机" /&gt;&lt;h2 id=&#34;分类器&#34;&gt;分类器
&lt;/h2&gt;&lt;p&gt;假设我们有 $n$ 个观测样例（bofservations），每个观测样例有 $d$ 个特征（features），有些样例属于类别 C，有些不是&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/decision_boudaries.webp&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;不同的决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;决策边界&lt;/em&gt;：分类器选择的可以将样例分成不同类别的边界&lt;/p&gt;
&lt;p&gt;&lt;em&gt;过拟合&lt;/em&gt;：决策边界拟合得太好，以至于无法用于未来样本的分类&lt;/p&gt;
&lt;p&gt;&lt;em&gt;决策函数&lt;/em&gt;：将一个点映射到一个值，例如&lt;/p&gt;
&lt;p&gt;$$
\begin{matrix}
f(x) &amp;gt; 0 &amp;amp; &amp;amp; \text{if } x \in \text{class } C; \\
f(x) \le 0 &amp;amp; &amp;amp; \text{if } x \notin \text{class } C;
\end{matrix}
$$&lt;/p&gt;
&lt;p&gt;则分类边界为 $\{x\in \mathbb{R}^{d}: f(x)=0\}$&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/dc_2.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;分类边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;linear-classifier-math&#34;&gt;Linear Classifier Math
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;内积&lt;/em&gt;：$x\cdot y = x_1y_1+x_2y_2+\dots + x_dy_d$，也可以写成 $x^Ty$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;欧拉范数&lt;/em&gt;：$||x||=\sqrt{x\cdot x}=\sqrt{x_1^2 + x_2^2 + \dots + x_d^2}$。可以表示向量 $x$ 的欧拉长度 (Euclidean length)。&lt;/p&gt;
&lt;p&gt;归一化向量 $x$： ${x}/{||x||}$&lt;/p&gt;
&lt;p&gt;给定一个线性决策函数 $f(x) = w \cdot x + \alpha$，决策边界为 $H = \{x: w \cdot x = -\alpha\}$。集合 $H$ 被称为&lt;em&gt;超平面&lt;/em&gt;（hyperplane）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超平面是 $d-1$ 维，并且分割了一个 $d$ 维空间&lt;/li&gt;
&lt;li&gt;它是无限且平展的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$w$ 和超平面 $H$ 上的任何直线正交。所以 $w$ 被称为 $H$ 的&lt;em&gt;法线&lt;/em&gt;（normal vector）。如果 $w$ 是一个单位向量，那么 $f(x) = w \cdot x + \alpha$ 是 $x$ 到 $H$ 的有符号距离（singed distance）。此外，$H$ 到原点的距离为 $\alpha$。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/normal_vector.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;平面的法线&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;如果存在一个超平面可以将所有的训练样本点分类，则它们&lt;em&gt;线性可分&lt;/em&gt;。&lt;/p&gt;
&lt;h2 id=&#34;一个简单的分类器&#34;&gt;一个简单的分类器
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;质心法&lt;/em&gt;（Centroid method）：首先分别计算属于类别 $C$ 的所有点的平均 $\mu_C$，以及所有不属于 $C$ 的平均 $\mu_X$。则决策函数可以表示为
$$f(x) = (\mu_C - \mu_X) \cdot x - (\mu_C - \mu_X) \cdot \frac{\mu_C + \mu_X}{2}$$&lt;/p&gt;
&lt;p&gt;质心法非常简洁，但是也只能处理简单的情况。例如下图中的数据点无法使用质心法正确分类。&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_02_linear_classifiers_and_perceptrons/centorid_method.webp&#34; width=&#34;400&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;质心法失效的情况&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;感知机&#34;&gt;感知机
&lt;/h2&gt;&lt;p&gt;假设有 $n$ 个采样点数据 $X_1, X_2,\dots,X_n$。对于每个采样点，对应的&lt;em&gt;标签&lt;/em&gt;为&lt;/p&gt;
&lt;p&gt;$$
y_i = \begin{cases}
\begin{matrix}
1 &amp;amp; \text{if } X_i \in \text{class C} \\
-1 &amp;amp; \text{if } X_i \notin \text{class C}
\end{matrix}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;为了简化讨论，这里只考虑经过原点的决策边界。于是我们寻找决策边界的目标可以转换成寻找一个权重 $w$，使其满足&lt;/p&gt;
&lt;p&gt;$$
\begin{matrix}
X_i \cdot w \ge 0 &amp;amp; &amp;amp; \text{if } y_i = 1 \\
X_i \cdot w \le 0 &amp;amp; &amp;amp; \text{if } y_i = -1
\end{matrix}
$$&lt;/p&gt;
&lt;p&gt;等价为
$$y_iX_i\cdot w \ge 0$$
上式即为&lt;em&gt;约束&lt;/em&gt;（constraint）。&lt;/p&gt;
&lt;p&gt;为了找到最优的 $w$，我们定义一个&lt;em&gt;风险函数&lt;/em&gt;$R$，如果约束被违背，则风险函数为正值，然后通过最小化 $R$ 选择最优 $w$。&lt;/p&gt;
&lt;p&gt;首先定义&lt;em&gt;损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$
L(\hat{y}, y_i) = \begin{cases}
\begin{matrix}
0 &amp;amp; &amp;amp; \text{if } y_i\hat{y} \ge 0 \\
-y_i \hat{y} &amp;amp; &amp;amp; \text{otherwise}
\end{matrix}
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;其中 $\hat{y}$ 是分类器的预测值。&lt;/p&gt;
&lt;p&gt;于是，我们可以定义风险函数（即优化问题的目标函数）为
$$R(w) = \frac{1}{n} \sum_{i = 1}^n L(X_i \cdot w, y_i) = \frac{1}{n} \sum_{i = 1}^n -y_iX_i \cdot w$$
所以寻找超平面的问题变为一个最优化问题：寻找 $w$ ，使得 $R(w)$ 最小。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习 01 引言</title>
        <link>https://zhiim.github.io/p/ml_01_introduction/</link>
        <pubDate>Sun, 18 May 2025 15:40:22 +0800</pubDate>
        
        <guid>https://zhiim.github.io/p/ml_01_introduction/</guid>
        <description>&lt;img src="https://zhiim.github.io/p/ml_01_introduction/ml_header.webp" alt="Featured image of post 机器学习 01 引言" /&gt;&lt;p&gt;本人 &lt;a class=&#34;link&#34; href=&#34;https://people.eecs.berkeley.edu/~jrs/189/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CS 189/289A Introduction to Machine Learning&lt;/a&gt; Spring 2025 的学习笔记，仅供参考。&lt;/p&gt;
&lt;h2 id=&#34;分类问题&#34;&gt;分类问题
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;收集具有标签的训练样本&lt;/li&gt;
&lt;li&gt;预测新样本的类别&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_01_introduction/classifier.webp&#34; width=&#34;800&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;两种不同分类器的决策边界&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;训练验证和测试&#34;&gt;训练、验证和测试
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;我们拥有一些设置好标签的数据&lt;/li&gt;
&lt;li&gt;从数据中取出一部分作为&lt;em&gt;验证集&lt;/em&gt;（通常 20），其余作为&lt;em&gt;训练集&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;训练一个或者多个分类器：训练多种不同算法，或者具有不同超参数的同一算法&lt;/li&gt;
&lt;li&gt;使用验证集选出具有最低验证误差的分类器&lt;/li&gt;
&lt;li&gt;使用新的数据测试算法的效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;训练误差&lt;/em&gt;：训练集中分类错误的比例&lt;br&gt;
&lt;em&gt;验证误差&lt;/em&gt;：验证集中分类错误的比例&lt;br&gt;
&lt;em&gt;测试误差&lt;/em&gt;：测试集中分类错误的比例&lt;/p&gt;
&lt;p&gt;大多数机器学习算法的一些超参数改变会造成欠拟合或者过拟合&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://zhiim.github.io/p/ml_01_introduction/overfitting.webp&#34; width=&#34;600&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;[k邻接算法中超参数k的改变带来的影响]&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;过拟合&lt;/em&gt;：当验证/测试误差恶化时，原因是分类器对异常值或其他虚假模式变得过于敏感。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;欠拟合&lt;/em&gt;：当验证/测试误差恶化时，原因是分类器不够灵活，无法拟合模式。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;异常值&lt;/em&gt;：具有非典型标签的点（例如，富裕的借款人仍然违约）。增加过拟合的风险。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
